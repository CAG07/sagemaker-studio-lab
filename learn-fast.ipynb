{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc25f35-28f0-40e6-aebb-a8ca9c7c9d10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (1.0.0)\n",
      "Requirement already satisfied: openai in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.28.0)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: aiohttp in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai) (3.8.5)\n",
      "Requirement already satisfied: requests>=2.20 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from openai) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->openai) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: youtube_dl in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2021.12.17)\n",
      "Requirement already satisfied: youtube_transcript_api in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.6.1)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from youtube_transcript_api) (2.31.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->youtube_transcript_api) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->youtube_transcript_api) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->youtube_transcript_api) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->youtube_transcript_api) (3.4)\n",
      "Requirement already satisfied: torchaudio in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (2.0.2)\n",
      "Requirement already satisfied: torch==2.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torchaudio) (2.0.1)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.7.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.7.101)\n",
      "Requirement already satisfied: typing-extensions in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (4.7.1)\n",
      "Requirement already satisfied: sympy in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (1.12)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.10.3.66)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (10.9.0.58)\n",
      "Requirement already satisfied: networkx in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (3.1)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (3.12.4)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (8.5.0.96)\n",
      "Requirement already satisfied: jinja2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from torch==2.0.1->torchaudio) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio) (68.0.0)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->torchaudio) (0.41.1)\n",
      "Requirement already satisfied: lit in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchaudio) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.1->torchaudio) (3.27.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from jinja2->torch==2.0.1->torchaudio) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sympy->torch==2.0.1->torchaudio) (1.3.0)\n",
      "Requirement already satisfied: sentencepiece in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.1.99)\n",
      "Requirement already satisfied: sacremoses in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacremoses) (2023.8.8)\n",
      "Requirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacremoses) (1.3.2)\n",
      "Requirement already satisfied: tqdm in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacremoses) (4.66.1)\n",
      "Requirement already satisfied: click in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from sacremoses) (8.1.7)\n",
      "Requirement already satisfied: transformers in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (4.33.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: filelock in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (0.17.1)\n",
      "Requirement already satisfied: requests in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: fsspec in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "#installing libraries\n",
    "\n",
    "!pip install python-dotenv\n",
    "!pip install openai\n",
    "!pip install youtube_dl\n",
    "!pip install youtube_transcript_api\n",
    "!pip install torchaudio\n",
    "!pip install sentencepiece\n",
    "!pip install sacremoses\n",
    "!pip install transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa277843-0bc3-4179-853b-a5b473ad9653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#importing dependencies\n",
    "\n",
    "import re\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import torch\n",
    "import torchaudio\n",
    "import openai\n",
    "import textwrap\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9918dfc7-91e8-4ff7-bd9e-58f674275f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Music] foreign hello there my name is Ryan Lewis and I'm happy to welcome you to this course AWS developer designing and developing if you've come to learn the ins and outs of AWS then you're in the right place this course has a few objectives but the most important is that your understanding and proficiency with developing an AWS will have leveled up by the end this course is intended as a follow-up to my previous course AWS developer getting started I will expect familiarity with many of the most common AWS services so if you haven't watched that course yet then I definitely suggest taking the time to work through it with that being said this course does cover many services already covered in the previous course the main difference is the approach and details that we'll be covering the getting started course's goal was to really get you familiar with the multitude of aws's offerings this course takes that familiarity and aims to make you moderately proficient at developing an AWS by adding even more detail and information additionally the focus for this course will be on the development side of AWS and not cover infrastructure related services like VPC or iam for this course we're going to focus on taking a web application and integrating it into AWS in the process we'll create and modify services using multiple different methods such as the CLI and SDK I'll take special care to include specific details about each service we're interacting with that should help you understand how to design your AWS architecture for your own applications developing an application in the cloud is not always quite the same as typical non-cloud development as companies migrate to the cloud more and more developers are getting their first taste of operations and infrastructure as they're directed to do their own cloud deployments in your past experience you may have been used to just building your application and then handing it over to another team or engineer to do the actual deployment of that application because of that process you may have thought of your application as only the code you wrote and other parts as outside of your domain but as with everything in Tech the world is changing and operations concerns are consistently falling into developers laps I believe this is a good thing and will ultimately improve your career as a developer it also does something else though and that's a sort of Mind expansion once you can utilize everything that AWS offers it changes how your applications work and how they're designed which really changes the way you think about your applications I've experienced this personally over the past few years when I began study AWS development and it's only improved the quality of my work I like to describe Cloud development like this basically when you begin thinking about a new application you'll want to start with the infrastructure there are certain decisions you'll need to make such as Computing persistence routing and AWS gives you multiple options for each decision you make fundamentally shapes how your application is structured and this approach is what I like to call Cloud development with each service that we cover in this course you'll add another tool to your application development tool belt and when you approach a new feature or application you'll be able to consider how certain AWS Services might best be used eventually your application will be a distributed and scalable combination of code and services this adds resiliency improves maintainability and ultimately results in a better application the service is in AWS are grouped into domains in the AWS console you can see how they've grouped them by the application concern each deals with some examples are compute storage and database because there are quite a few Services it helps to group them conceptually and provide some context to each in this clip we're going to talk through each AWS domain to help you understand what each one contains the first and sort of the most important domain is compute this domain is for services that assist in executing custom code that you provide some examples are ec2 instances or Lambda even though there are several different services in this domain all of them ultimately run on ec2 instances under the hood the main differentiator for each is the level of abstraction and automation the service provides storage is another important domain that focuses on storing different types of data for retrieval at a later time specifically this is going to be file system like storage we're not talking about databases or Cache here S3 is the perfect example service for this domain which is used to store just about anything these days the next domain is database and it's pretty self-explanatory this domain includes managed SQL databases with relational database service and nosql data stores with dynamodb elasticash is also included in this domain even though it may be debatable whether it can be classified as a true Database Network and content delivery is a hugely important domain that is often hidden under the covers the networking concerns of this domain manage how external users access and interact with your application as well as how internal AWS resources interact with each other cloudfront rounds out the content delivery aspect of this domain providing your applications with a high quality CDN to get your content to users faster than you may ever need to the developer tools domain is full of services dedicated to the development process such as Source control and continuous integration these Services aren't for users to interact with but really are focused on the developer experience side of AWS using these are optional though so don't feel like you need to use any of these developer tools Services if you're developing on AWS services that focus on the service management side are in the management and governance domain these Services focus on the creation and management of other services in AWS a lot of these services are used more by devops Engineers but we will be using cloudwatch in this course the security identity and compliance domain contains services that deal with managing secure access to your AWS account and Individual Services IAM is the main service here as it manages and provides the policies that affect nearly every service in AWS the analytics domain focuses on services that allow you to consume and process data in different ways from services like elastic mapreduce which lets you process big data to the Kinesis service which provides streams that facilitate data transfer any service focusing on dealing with internal data can be found here the last domain we'll talk about is application integration which is for services that provide Integrations between other services in AWS for instance services like simple notification service and simple queue service provides ways for applications and services to communicate with each other inside AWS simple workflow and step functions give developers ways to orchestrate applications inside AWS and those are all the main AWS domains there are a few more but they're relevance to this course and likely your job are minor enough that I'm leaving them out here now that you understand the AWS domains keep them in mind when you're looking for a solution in your application there may be a cool new service and a domain that you're concerned with and it might just be the perfect solution for your problem Amazon provides a few different ways to interact with their Cloud the web console is one of the first you'll interact with and the CLI is probably the second as you configure your local credentials but the AWS SDK software development kit is likely what you'll use most of the time in your role as a developer this is because the AWS SDK provides the simplest interface between your code and the cloud AWS distributes and maintains these sdks in multiple languages so you can develop in the cloud in whatever language your application is built with there's some other open source sdks for languages that aren't supported already by AWS these are typically very usable but it's important to know when an SDK is officially supported so that you can understand the level of support it'll receive let's take a look at the official sdks available from Amazon first of all AWS provides sdks for the current most popular server-side development languages in no particular order these are java.net C plus plus python Ruby go and node on the client side you can use the same JavaScript SDK that is used with node on mobile you have some interesting options there are official sdks for Android iPhone the xamarin platform and unity with all of these offerings it's very likely that your language of choice has a fully supported SDK from Amazon in this course we're going to be working with an application built on node so we will be using the JavaScript SDK extensively if you want to get a head start with it here's the URL for the documentation site for the JavaScript AWS SDK this course is designed to not only teach you how to be an amazing AWS developer but also to help you complete the AWS developer associate exam the exam tests your knowledge of AWS and the many different Services options and problems that you encounter in the process of developing on the platform the content of the exam is updated every few years so while I can't make any assurances that this course will contain everything you'll need for the exam whenever you take it I am covering each topic as if you'll be tested on it the current AWS developer associate exam contains all multiple choice questions typically each question will focus on a single service and ask about topics service specifics or service limits there are also questions about debugging certain problems and giving architecture recommendations based on use cases there's no real way to know which services will be on the exam but the core Services of AWS like ec2 S3 and dynamodb are almost guaranteed to be on the exam I've also seen services like sqs Route 53 and elastic Beanstalk to name a few bottom line the more you work with and learn about AWS the easier the test will be for you I recommend watching all of the courses in the AWS certified developer path here on pluralsight and then continue building more projects if there's a service you aren't very familiar with it wouldn't be a bad idea to build something just to give it a shot in addition I've created a course called demystifying the AWS certified developer associate exam which serves as a great intro to the exam the course covers the exam format how to study which resources to use and what the day of the exam will look like I recommend watching it after you've completed the AWS certified developer path to put you on the best footing for the exam in order for us to utilize a bunch of AWS Services together we need an application to work with for this course I provided a demo web application project that will modify to make the most of AWS here's the demo app called hamster ball fantasy league this site features a make-believe League of hamsters racing each other in hamster balls the site is a single page application built with react and Redux the backend application is built with node and the happy web application framework with hbfl you can look at individual hamster details to see how they've been faring look at race details to see what place each hamster came in and also look at the leaderboards to see how each user is ranked based on their favorite hamsters if you log into the application you can get access to your own data and can pick and choose which hamsters you want to represent you in the leaderboards you can only favor it three at a time there are some behind the scenes controls in this configuration panel that let you reset the race results and start them again these are all make believe randomly generated results so each time you run the simulation you'll get a different outcome see if you can make it to first place this application is going to utilize a multitude of different services to provide some robust capabilities for users starting in the next module we'll get this application running in ec2 and see how it feels when it's served from the cloud as we begin this course I'll assume that you already have an AWS account set up if you don't pause the video and go sign up for one now new accounts receive access to the free tier for one year so you might just want to create a new account anyways for the purposes of this course now one of the first things I like to do on a new account before I create any resources in AWS is to set up a billing alarm or an AWS budget this lets you know once you've accrued a certain amount of charges on your account it's a nice Peace of Mind in case you spin up some resources without being aware of the costs I can say it has saved my bacon a few times billing alarms through cloudwatch used to be the main way that you could achieve this billing monitoring but AWS introduced the budget service to provide more features and the ability to fine-tune the monitoring based on an incredible number of factors we'll use it in a very simple way but the functionality is there to use it in a more robust manner let's set one up first log into the AWS Management console with the root user this would be the email address that you first started the account with then navigate to the AWS budgets dashboard from this front page click the create a budget button if this is a new account you might see an alert like this here that wants you to enable cost Explorer on your account to open up new types of budgets you can do this later it won't hinder what we're doing since we're going to set up a cost budget which is already selected here so click next we'll start by looking at the set budget amount section we want to look at a monthly period since that's how we're billed by AWS then we'll have this budget be recurring so it just continues to check every single month the start month drop down should already be populated with the current month so you can leave that as it is now for the budget amount I always set this to a small enough amount that lets me know if I did something wrong for instance I left some resource running also I set it low enough that I won't mind if I accidentally get charged for that amount for those reasons I pretty much always set this amount to five dollars the budget scoping section is where you can put filters on the budget such as which regions services or even tags to include your screen may look like mine spinning if you haven't enabled the cost Explorer functionality yet continue on and give your budget a name such as AWS budget then you can click next now we need to add an alert this is when and how you'll be notified if the budget is exceeded click the add an alert threshold button first we'll Define the when of the notification the threshold amount refers to the dollar amount we specified in the previous screen I'm going to enter 100 here so it'll let me know when the full amount will be exceeded and the trigger drop down you can either choose actual which means it'll be triggered when you actually have billable charges over that amount which means you'll have to pay it or forecasted which means it'll be triggered when you are forecasted to have charges over that amount by the end of the month this means that you can terminate those resources before they actually hit your budget amount hopefully saving you some money so choose forecasted next add an email address to receive an email if the budget threshold is exceeded then you can click next here you can add additional actions for the budget to trigger such as terminating ec2 instances but we're not going to do that so just click next now we can review the budget configuration and finally click create budget and that's it for creating your budget now you'll be notified if any AWS charges sneak up on you before we start working with AWS there are some prerequisites you'll need to ensure that you have completed the first is to have the AWS CLI downloaded installed and configured with a user the user should have permissions to do almost anything in AWS if you have to be more restrictive than that I recommend taking a look at the services in this course and just giving yourself read write permissions only for those we will be working a lot with the SDK and it requires you to have your CLI level configuration already set up next you should have a good code editor installed I'll be using vs code which is a free editor from Microsoft if you want to use something different my only suggestion is to just make sure it has some syntax highlighting and auto indent they're pretty essential for JavaScript development you'll also need to have node and npm installed you can download these from the node website at nodejs.org this is the version we'll be using in this course so it's best to download a matching major version finally make sure to download the demo project code you can get this code in the exercise files section of this course on pluralsight but I'm also making it available on GitHub if you prefer to get it from there I would say the benefit of using the exercise files from pluralsight is that any changes in the course are present in that code from each module there's a before and after section of the code it can be really beneficial to have that available just in case you make a typo during a lesson and we will be writing a lot of code in this course and that's all you need to get started now let's dive deeper into the world of Amazon web services welcome back to AWS developer designing and developing in this module we're going to learn Advanced methods for creating and managing elastic Cloud compute or ec2 instances in the previous getting started course we saw how you can easily create instances using the AWS console but in this course we're going to interact mostly with AWS through the SDK and AWS CLI so we'll be utilizing different methods to do some operations you may have learned in the past we'll start the module by covering some of the fundamental concepts of ec2 Ami and the AWS Marketplace then we'll discuss the ec2 rest API and look at the details that go into each request I'll spare you the pain of manually making one of these requests because it is really inconvenient next we'll create an ec2 instance with the SDK and install the demo application with our instance create aided we'll look at different ways to manage your ec2 instances with the SDK then we'll use an existing product from the AWS Marketplace to launch an instance that will have some pre-installed software on it with our instance created we'll create an Ami from that existing instance to get a good reproducible image finally we'll look at the limits of the ec2 service and Amis these limits are important to understand when designing your AWS Solutions and should help you from painting yourself into a corner in the future the ec2 service is one of the most fundamental services offered by AWS it underpins many of the other services in AWS we've already covered most of the basics of this service and the getting started course so we're going to go into a bit more detail into ec2 in this course the basic unit in ec2 is the instance this is a virtual not physical self-contained Computing unit each instance needs software installed on it to run and this is available in AWS as an Amazon machine image or Ami Amis can be a basic Linux or Windows operating system installation or include other software like a vendors application or some open source application to give an example later in this course we'll use an Ami that comes from the AWS marketplace with Linux and node.js pre-installed you can create your own Amis with your custom software installed or launch one from the AWS Marketplace you can also create Amis iteratively by launching an instance with an Amazon Linux Ami then adding your own software and creating an Ami from that Amis are very flexible and versatile in that respect AWS provides a separation between the instances execution environment such as the CPUs and memory available to the instance and the instances file storage in module 4 we'll cover this in more detail but basically ec2 gives you three options for how your ec2 instance file system will be managed the first is instant store volumes these are physically connected to the ec2 instance and act similar to basic hard drives they can't be reused for other ec2 instances the second option is elastic block storage shortened to EBS the main difference between EBS and instant store volumes is that EBS volumes can be connected to other instances and also can live on even if the instance is terminated the third option is elastic file system shortened to EFS EFS differs from EBS in that it is scalable in size other details such as a separate life cycle from the ec2 instance and being able to connect to different instances is similar when creating an Ami you have the option to Define an instance volume backed Ami or an EBS backed Ami choosing one or the other restricts what states are available to the created instance an EBS backed instance can be stopped for an arbitrary period of time the instance data will be persisted on the EBS volume contrast this to an instance volume backed instance which can only be terminated or restarted can't be stopped it's a small distinction but it can afford quite a bit of flexibility with using EBS volumes a larger distinction between the two types of instances is the boot up time and EVS backed instance created from an Ami is significantly faster to boot than an instance-backed volume the main reason that EBS is faster is that the Ami data for an instance-backed volume must be transferred from S3 on boot up which is different from EBS EBS was introduced by AWS after instant store volumes so there are many good reasons to use EBS one more important detail of Amis is the visibility of the image basically who has permissions to use it when creating the Ami you will need to specify the launch permissions your options are public which allows anyone to launch an instance from your Ami explicit which lets you give permission to certain AWS accounts to create instances from your Ami and finally implicit which only allows you to create instances from the Ami you can also publish Amis to the AWS Marketplace but we won't be covering that in this course there are a few high-level instance attributes that you have to decide on before starting an ec2 instance one of them is instance type which I covered in the getting started course another attribute is the instance class these classes are spot instances on-demand instances or reserved instances you may have seen these terms but I'll Define them here and give them a little more explanation in this course and the getting started course we're only using on-demand instances but based on the descriptions consider giving the other classes a try instance class doesn't have anything to do with what the instance can do it really defines the conditions in which the instance lives and how much you'll pay for it on-demand instances are the usual pay only for what you use no commitment instances that we spin up and spend down like nothing these instant instance types are good for applications that need to be scaled up and down it's also the best kind of instance when you're learning AWS like we are because you can kill the instance when you're done playing with it reserved instances are just like they sound where you make a commitment to AWS and reserve a set number of instances for a certain period of time there are different links to the commitment time typically between one and three years but the discounts can be massive some up to 75 percent off if you're a stable company that will reliably need certain instances running for that period of time reserved instances might be a great way to save some money the final class of instances is called spot instances these are instances that you bid on and they utilize spare ec2 computing power think of it like the unused hotel rooms or Airline seats that are sold at rock bottom prices at the last minute just because it's better to get some money than no money discounts can be even deeper than with preserved instances up to 90 percent off but there's no real guarantee on availability it's suggested that only extremely flexible compute jobs utilize spot instances a web application is not a good use case for spot instances but a research job or data processing job might be a good fit AWS provides a rest API to perform operations on services in your account just like any typical rest API lets you perform normal crud operations such as creating ec2 instances or listing all available Amis the API seems very convenient before you discover the signing procedure required for each request when sending any API request to AWS you must first sign the entire request with your AWS credentials and add the generated value as a query parameter this is actually common with secure apis but can make it difficult to work with manually AWS understands this can be difficult which is why they develop and maintain the sdks inside the code of the sdks AWS is signing each request Made For You indeed most of the SDK is just a thin layer of code over the rest test apis making a request to this rest API especially signing that request is a convoluted process I wouldn't wish on anyone it's a process that requires multiple levels of hashing creating hmax and then actually making the request but let's go ahead and take a look at the process involved so we can appreciate what the SDK does for us first of all you need to decide which version of signature you want to produce version 2 is the older version supported by many of the original services but it's largely being deprecated in most places version 4 is the preferred signature version certain newer AWS regions actually only support version 4. as do many newer Services each signature version has a different process when signing requests version 4 is the more secure and therefore more complicated process where the version 4 signature you'll start by creating a canonical request this is a combination of the HTTP method the URI path query parameters any headers and finally the hashed contents of the request payload the next step is to create a string that will be signed for your request this string contains the hashing algorithm used the date the region the service and finally the previously derived canonical request that has been hashed with sha-256 and we're not done yet once we have this string we need to actually sign it to do this we'll need to First calculate a signature by making several passes through an hmac and then finally signing our string with the resulting signing string that process gives a signature which we will add as a query parameter to the actual request when we make it this process is pretty difficult to get right as I myself experienced trying to manually sign a request the errors when you get it wrong are unhelpful and the documentation is not the clearest luckily for you me and every other developer in the world you can instead just use the AWS SDK to make these requests you won't have to sign anything at all it does it all for you the SDK uses your local credentials to make the request and then return the response to you due to the complexities involved I'm sure you'll be grateful that we'll be using the SDK as our sole interface to the AWS rest apis for this course in order to run your application in AWS we need to have one or more ec2 instances to execute that code so the first step is to create an ec2 instance there are many different ways to do this into getting started course we mostly use the AWS Management console to create resources and another great creation method is using the AWS CLI which can be useful if you're doing automation but we are going to use the AWS SDK to create an instance with code and before we can create the instance we need to create a few other resources in this clip we're going to create a security group for the instance then in the next clip we'll create a key pair and then finally we'll use both and create the instance the first change I want to make will have impacts across your entire project whenever we do things with the AWS SDK we need to tell the code which region we want to use you need to do this every time you perform an operation in your code so I want to make that process simpler where you don't need to type your region over and over and over open the dot EnV file in the demo code there's a string here which will be saved as an environment variable when you run your code modify the string to have the value of the region ID that you're using then save the file and your region is set to go for the rest of the course now a lot of this course will be using code to perform operations through AWS so I've created a scripts folder in the course exercise files this scripts folder is organized by module so navigate to the module 0 3 folder I've created a file we'll modify in this clip called create ec2 instance.js this file provides a blueprint for the changes we need to make each modification has a to do comment so it's clear where you'll be making your modifications I'll walk you through the file to start with at the top are dependency Imports then we've got two to Do's where we need to create a new ec2 client with our region and send a command this is a convenience function the send command function that we'll use in the operations functions below it so that we don't have to write the same code multiple times keeping our code dry next are some local variables being declared then we've got the code that calls several functions to create a security group create a key pair and then create an ec2 instance below that are three functions that we'll need to implement the first one is to create that Security Group the second is to create a keeper and the last is to create an ec2 instance using the security group and the key pair that we just created as a side note an ec2 instance can be created through the SDK without an explicit security group or key pair if neither are given the instance is assigned the default Security Group in your account and no key pair is assigned after the fact you can reassign security groups so getting the default one isn't much of an issue but the key pair is more serious because if you're using an Ami that isn't pre-configured to allow a different type of access method such as a password then you won't be able to SSH into your instance if you don't start it with a key pair with the vanilla AWS Linux instance you must attach a key pair to access it like this and let's cover one more thing version three of the JavaScript AWS SDK which we will be using in this course is pretty different from version two I want to cover those differences real quick the first is how you import things in version two you import the entire AWS SDK dependency version 3 is more modular and it allows you to only import the code that you want to use these are bundled into clients such as the ec2 client and the dependency as shown here in addition when we import things from a client we'll de-structure the result and only import the Constructor functions that we'll be using these will usually be one client and several commands this follows into the next big difference in version three how you make something happen in AWS there's a new pattern that you'll be following first you instantiate a client then you instant a command then you use the client to send the command to AWS this pattern happens over and over with the new AWS SDK we'll be implementing a send command function in most of our files to keep our code dried but just know that this client command send command pattern is the way things happen in the new version of the JavaScript AWS SDK this new version doesn't apply to other AWS sdks with other languages now let's get started with the changes in this file after we've made all modifications and run this file we will have a running ec2 instance with a security group allowing us access and a key pair that has been saved locally that we can use to access that instance in addition the ec2 instance will already have node installed the demo project downloaded dependencies installed and the demo running on Port 3000 it's a lot of things first scroll to the top of the file at line two we need to import the ec2 client Constructor to start with replace the comment with a const declaration and curly braces in the curly braces enter ec2 client entered as pascalcased then add an equals after the curly brace and require the at AWS SDK slash client ec2 package with our client we can now implement the send command function as I mentioned earlier this will help us keep from repeating code the function will take a command as an argument create a new ec2 client and then send the command with that client so start by creating a new const variable called client assigned to it a new Constructor call of the ec2 client variable as in an object literal with the property region and the value of process.env.aws underscore region with AWS underscore region all being uppercased this is how you'll Define the region for the AWS SDK to use which will use the region value that we modified in that EnV file in the next line we'll return a call to client.send passing in the command variable this is the pattern that I mentioned a bit ago all right now we can implement the create Security Group function the first step is to create the input for our Command create a new const variable called SG params assigned to an object literal in these params objects which we will write many of all the property keys will be Pascal case starting with a capital letter so keep that in mind as you add them add one property called description the value will be the SG name parameter then add another property called group name with the value as the same SG name parameter now we can instantiate our first command to do this we need to import it from the AWS SDK scroll to the top of this file and add a comma after the ec2 client import inside those destructuring curly braces now import the create Security Group command object here then scroll back to the code that we were writing add a const named create command assigned to it a new Constructor call of create Security Group command pass into that the SG prams object we created above now the command will have all the right instructions and details we want now we just need to use the client to send it which is what our helper function above will do in the next line add a new const called Data then after an equal sign add the await keyword and then call send command passing in the create command variable the send command function needs await here because it is an async function that returns a promise now once the command is sent it'll create the security group and return the data about that security group we will use that in our next step which will be to set the rules for the security group we want to set up two Ingress rules first we'll want to enable Port 22 so that we can SSH into the instance once it's been created and then Port 3000 so we can access the demo app in our browser so start by creating a new const called rules params and assign an empty object literal to it there are a few properties required to create these Security Group rules that should sound familiar to you if you've set one up with the Management console before the first is the group ID to apply the rule to we can get that from the data variable populated by the creation command response for the value here enter data dot group ID the next property is ippermissions which is an array this array enables us to configure multiple rules in one request create an object in this array and add the property IP protocol the possible values for this are protocols like TCP or UDP enter TCP here all lowercase the next two properties from Port and to Port Define the port range you're enabling the rule for in the first case we'll enter the number 22 in both from Port and to Port since we're allowing SSH here finally add the property IP ranges with the value as an empty array here is where we can Define who this rule applies to add an empty object with the property cider IP we'll enable access to anyone so give this property the value of 0.0.0.0 now we need to also add the rule for Port 3000 so copy this rule object add a comma after it and then paste the only differences will be the front port and two Port Properties and you can set both to three thousand awesome now we're ready to send one more command first let's import it at the top of our file after ec2 client enter authorize Security Group Ingress command it's a long one and then a comma I like to organize these Imports as the client on top and then the commands that are in alphabetical order below it that's why I put the authorize command before the create command here now we can use the command create a new const called auth command and then assign to it a new Constructor call of authorized Security Group Ingress command passing in the rules params variable then we just need to send it and the next line add the return keyword and then call send command passing in auth command and with that code complete we've successfully written the code to create and assign rules to a security group in the next video we'll create the key pair in this video we're going to continue our work from the last clip and create a key pair for an ec2 instance using the AWS SDK this create key pair function is what we'll need to implement so let's get started first create a new const variable named params and assigned to it an empty object literal a key pair only really needs one property to be created which makes it really easy for us the property identifier is keyname and you can give it the value of the keyname argument that was passed into the function now we need to create the command to do that we need to import a new command Constructor at the top of the file make a new line after the authorize command and add create key pair command and a comma Now back in the function implementation create a new const variable named command and assign to it a new Constructor call for create key pair command passing in the params variable now we just need to send that command and the next line add the return keyword and then call send command passing in the command variable returning this function call here is important because when the key pair is created in AWS the function Returns the contents of that key pair's private key if you don't save this locally the key pair is essentially useless I provided a helper function that will actually take this key pair data and save it locally as a file if you don't return the send command response here it'll basically throw away the created key pair which you don't want now the code to create the key pair is complete if you scroll up you'll see the key pair persistence being called after the key pair is created this is where it will save the key pair locally in your SSH folder and the next clip will take the work that we've done creating the security group and the key pair and we'll create an ec2 instance using both we've created a security group and a key pair so now we're ready to create an ec2 instance that uses both of those resources we've only got one unimplemented function left in our file create instance this function takes the arguments SG name and keyname which were used to create the other resources inside this function we'll start by defining a params const variable and assigning an empty object to it as you can probably imagine this params object will have many more properties than the other ones that we've used the first property to add is image ID this is the Ami ID that will be used to create the instance and to get this we'll need to make a little side trip to the AWS Management console see there are a couple of interesting things about Amis first there is a command in the ec2 client in the AWS SDK called describe images command that can be used to essentially search for Amis that you can create instances from the problem is searching with no filters takes a super long time and it's just easier to go to the AWS Management console directly if you know what you're looking for the second thing to know about Amis is that they are region specific for instance an Amazon Linux 2 Ami will have different IDs depending on which region they're in and you can't just reference the ID from one region to create an instance in a different region the image IDs are unique for each region even if the image contents are the same so for that reason I'm going to show you how to find the Ami ID that you'll use in your code go to the ec2 dashboard in the AWS Management console click on the launch instance button and then launch instance and you'll be presented with the Ami selection screen at the top there's the Amazon Linux 2 Ami at the end of that title is the ID the ID you see probably won't match this one since you may be using a different region or they might have published an update there will also be two different IDs one for x86 and one for arm these are Amis for different types of ec2 instance architecture we'll be using a T2 micro instance which is based on x86 architecture so copy the Ami ID for x86 now switch back to your code and paste that in a string as the value for the image ID property the next property in params is instance type this is a type of instance such as T2 small or M4 extra large and defines the properties of the instance we've talked before about what these mean and we can use one of the smallest here T2 micro next we'll Define the key pair name with the property keyname and enter the value of keyname from the function arguments next is the max count property which defines how many instances to create enter one then we have the Min count property and you can enter one again here now add the property security groups and give it an array as the value this is where we can add any security groups to the instance enter the SG name argument into the array you can add more than one Security Group here if you want to there's also a security groups ID property that you could use instead and reference these security groups by their IDs rather than by their names but since we already have the name argument it's just easier to do that the last property in our prams object is user data user data has a couple of different uses with ec2 instances but we will use it to run shell commands once the instance starts if you open the file in the O3 folder called ec2 startup.sh you'll see several shell script commands these commands will install node and git clone the demo project from my GitHub install the dependencies and then run the demo project you can't just put these shell scripts directly into the user data field it needs to be base64 encoded so I've gone ahead and encoded the commands these commands for you in this comment below so copy that line without the preceding hash symbol then go back to the create ec2 instance file and paste it into a string as the user data value and with that we have all the params in place next we need to build our Command scroll to the top of the file so we can import The Constructor after the create Security Group command add a comma and then add run instances command then go back to your function and create a new const variable called command assigned to it a new Constructor call of run instances command and then pass in the params variable the last step is to send the command so return a call to send command passing in the command variable and with that we've completed our script now for the fun part running it but before that how about a PSA this code is going to assume two things about the region that you're deploying to first it's assuming that you already have a default VPC for that region for some regions AWS will configure this by default and for others it won't if you look at the ec2 dashboard and the Management console AWS has the default VPC listed if you don't have an ID here then the default VPC isn't configured for this region in your account on this page AWS has information and the steps for creating that default VPC manually so make sure to follow that those steps before running the script if you don't have a default VPC already configured for your region the other thing this code assumes is that the subnets in your default VPC are configured to Auto assign public ipv4 addresses this means that when you create a new instance in these subnets they will give the instance a public IP address now most default vpcs and subnets already have this configuration set up but you might want to check by looking at this configuration detail in the default subnets to make sure it's already configured correctly now move over to your command line and navigate to the demo project we haven't installed dependencies yet and we'll need them for our script so go ahead and run npm install at the demo project root once that completes navigate down to where these scripts resides now you can run node create ec2 instance.js to execute that script no other arguments after a few seconds of processing it should print out the location of the key that was written locally and the details of the instance that was just created or maybe an error if something went wrong the actual boot up of the instance can take a few minutes and it's done completely on the instance you don't see any output here I'm going to go ahead and fast forward the video and then I'm going to refresh the ec2 instance dashboard in the console here's the instance that was created and because it's running and because at this point the applic the demo application should have already started working if you copy the public IP address which is found here open it in a new tab and add colon 3000 you should see hamster ball fantasy league running completely in AWS in the next clip we're going to look at what we can do with this instance now that it's already been created once your instance has been created there are a few different operations that you can perform on the instance as you manage its life cycle the SDK command that you'll want to use to modify the configuration of a running instance is modify instance attribute command some examples of instance attributes this command allows you to modify are granular ones like kernel and ramdisk and higher level attributes like instance types and block device mapping I really didn't think there was enough for us to dig into with this command so I thought we'd actually try out two different commands to manage existing instances in this video just wanted to mention this modify instance attribute command in the beginning here the describe instances command is useful because it will list all the instances you currently have running in a given region you can apply different filters to further filter the query and limit what types of instances are returned I wanted us to give this command a try and see the type of response we get in your code editor open the file manage ec2 instance.js we can start by importing the ec2 client and the describe instances command at the top of the file replace the to do with a const variable with curly braces inside the curly braces first enter ec2 client and then under that add the describe instances command then add an equals after the curly brace and require the at AWS SDK slash client ec2 package now let's implement the list instances function the describe instances command only has a few possible parameters in the command input the main one is the filters to apply you can filter based on things like instance type VPC ID or tag for our purposes we're not going to use a filter but I just want you to know that that functionality exists so we can go ahead and declare a new const variable named command then assigned to that a new Constructor call of describe instances command even though we aren't providing any properties to this command it still requires an object as input so just pass in curly braces to the Constructor call then in the next line just return a call to the send command function passing in the command variable if you scroll to the bottom of the file the list instances function is called and the result logged to the console We'll add a little more to the list instances function but I wanted us to go ahead and run it and see what kind of output we got on a command line make sure you're in the right folder and then execute node manage ec2 instance.js once it's finished running it should print out a bunch of Json to your console this is a structure of the data object that is returned once you've sent the describe instances command the property that we care about here from this Json is called reservations a reservation in this context is essentially a request from some user to AWS to launch an ec2 instance the request could have come from the Management console the SDK the command line or even something automated like an auto scaling group if you have more than one instance on your account there is a very high possibility that the array that is signed to the reservations property will have more than one object in it in my case there's only one because I'm on a new account each of the objects in this reservations array has an instances array this is the information that we actually want the actual instance info is hidden here just because the Json printout doesn't go that deep so let's go back to the code to reduce the Json that's coming back we're going to use a little javascripting to join together all the instance arrays from each reservation object so just stick with me here first replace the return statement at the end of list instances with a new const variable named data then add the await keyword before the send command function call and the next line will manipulate the data object so enter the return keyword then enter data dot reservations and then chain a call to reduce inside this function call you'll create a new Arrow function with two arguments i and r is the aggregated instances array and R is the current reservation object inside that function return a call to I dot concat passing in r dot instances with a capital i then after the closing curly braces add a comma and then an empty array which is the seed that our reduced function requires now if we save that and go back to the command line and execute node manage ec2 instance.js again we will see all the instance information output you've got a ton of information here like the instance type the key name the public IP address and if you've got more than one instance you should see them all listed here we'll use a piece of the information here with our next management function so let's go ahead and grab it find the instance you just launched from the create ec2 instance script the easiest way to identify that would likely be the key name that should show hamster underscore key copy the instance ID property which will be like a lowercase i hyphen and then a bunch of letters and numbers switch over to your code editor and paste this instance ID into the parentheses of the terminate instance function call at the end of the file ensure that there are quotes around the ID in case you didn't copy them now let's Implement terminate instance this function is going to take an instance ID as an argument and tell AWS to terminate that instance for the record you can also stop an instance if you want to the key difference between stopping and terminating an instance is that a stopped instance can be restarted but a terminated instance can't we will need a params object to pass into our Command so first declare a param's const object it will only take one property instance IDs which is an array you can terminate multiple ec2 instances with a single command which is why this is an array inside this array just enter the instance ID argument variable now we'll import the command we need so at the top of the file add a comma after describe instances command and add terminate instances command then back in our function create a new const called command and assign to it a new Constructor call of terminate instances command passing in the params object we won't modify the response from the command at all so in the next line just return a call to send command passing in the command variable I've already written the code to execute this function at the bottom of the file but as you saw earlier it's currently commented out so comment the line with the list instances function call and then uncomment the line below it calling terminate instances save the file and then switch over to your command line we're going to terminate our hamster instance by running node manage ec2 instance.js when the script is complete it'll output the response from the terminate command which should contain that same instance ID that was passed in and that's it for terminating instances if you don't trust your command line you can go check the AWS Management console and see that the instance in question is currently shutting down in the next clip we're going to launch an instance with an Ami from the AWS Marketplace running your own code on a vanilla Linux or Windows instance is one of the most common use cases for ec2 but occasionally there is some vendor provided software that you want to run as well you could install this yourself but you'll often find that there is an existing Ami with the software pre-installed available on the AWS Marketplace let's take a look at the marketplace by navigating to the ec2 dashboard in the AWS Management console click on the launch instance button and then launch instance and you will be on the Ami selection page the software that are available from the AWS Marketplace are just Amazon machine images with an operating system and some custom software pre-installed that's basically it on the left hand menu click AWS Marketplace and here you'll be able to see what's on offer there are some featured and popular Amis and then a bunch of categories you can see there's quite a few categories available thousands of Amis in them in fact for this course I wanted us to launch an instance with an Ami from the marketplace specifically one that has node already installed on it in the search box type node.js and hit enter there should be quite a few results but look for the result titled node packaged by bitnami and it should have a fairly recent updated by date this number here is the version of node that is installed on the instance there shouldn't be any cost information here if an Ami from the marketplace does cost money it will have the per hour cost listed next to the image information you may see some other entries with a cost associated with it node is free and open source and the bitnami Ami is free as well you'll still be charged for any instance cost but the Ami has no additional charges now click more info and then click on the link in ending in product detail page on AWS Marketplace see we're not going to use the console to launch this image but instead grab the Ami ID and then use our scripts again this next page has a lot more information about the image if you want to read it for our current case click this continue to subscribe button on the right hand side now we're in the subscription page we have to subscribe to this image meaning that we must accept the terms and conditions if this image had a cost associated with it we would also be agreeing to pay whatever amount the image costs once you've looked at the terms and conditions click the accept terms button and then wait a while while they process the subscription I actually don't know why this takes any time at all but it does a couple minutes at least so I'm going to fast forward and then once the subscription has gone through your page should look like mine with a continue to configuration button in the top right so click that now now we can finally get the Ami ID you can adjust some parameters here such as the architecture version of node and region if you remember I mentioned before each Ami is region specific this is why picking the correct region here matters it's technically the same code on the Ami for each region the same image but the IDS are different based on the region so even though everything should be automatically populated for you based on the region in your console just make sure that your region is matching where you want to deploy it to and then also make sure that the version of node matches what we're using in the demo project so far you can see in the drop down that there are many different versions major versions of node that you can choose from now once you've selected everything correctly copy the Ami ID that is displayed right here and then switch back over to your code editor and open the create ec2 instance.js file we're going to launch an instance using this Ami instead of the vanilla Amazon Linux Ami that we did before so scroll down to the create instance function and paste over the image ID value with the Ami ID that you copied now we could run this just as it is now but there are two more changes that we need to make the user data we have here installs node and git and then it clones the demo down and it runs it we don't need to install node anymore because this bit Nami Ami comes with it so let's change this user data open up the file titled Marketplace ec2 startup.sh this is basically the same as before minus the node installation steps and the Linux package manager updated to apt-get since the bitnami image is Debian based copy the base64 encoded text minus the starting hash at the bottom and paste that into the user data string value in the create ec2 instance file the next modification is to the key pair and security group names on line 17 and 18. if we were to run this file again it would try to create new Resources with those names and then error out because they already exist let's give them new unique names by just tacking on a 2 to the end of each string now save the file then switch over to your command line and run the file with node create ec2 instance.js you should see the same type of message as when you ran the file previously this time however you've launched the instance with a whole different Ami I'm going to skip the video ahead in time a bit and then confirm that the instance creation worked correctly I'll copy the public IP from that newly created instance into my browser and add the port 3000 and there's hamster ball fantasy league it's running on a new Marketplace Ami that has node pre-installed in the next clip we're going to create our own Ami from this instance Amazon machine images are sort of like the primer for every painting that you want to create with an ec2 instance it's the starting point in the base software that you build off of but they do have another use which is to help you easily reproduce an instance with your given application loaded on it there are a million other uses for Amis but bottom line is that they are ubiquitous and super important throughout AWS in this module we're going to use the SDK to create an Ami from the instance we started up in the last clip to do this move over to your code editor and open the file createami.js you can probably tell just from looking at the file but there will be significantly less code than the last few files we worked on we can start by importing the ec2 client and the create image command at the top of the file replace the to do with a const variable with curly braces inside the curly braces first enter ec2 client and then after that add create image command then add an equals after the curly brace and require the at AWS SDK slash client ec2 package now on line 12 we're calling our create image function passing in an instance ID and a name for the image once it's done it just prints out complete nothing special we'll go get the real instance ID to enter here later let's implement the create image function we'll start by creating our params const object first there are only a few parameters that the image creation operation needs first is the instance ID property which you can assign the function argument seed instance ID every Ami needs an instance to be based off of this instance ID is what should be copied to a new image next is the name property to which you can assign the image name argument this name is basically just an identifier for the Ami and that's it for the parameters now we can use this params object to send a create image command first add a new const with the identifier command and assign to it a new Constructor call of create image command passing in the params object then in the next line add the return keyword and call send command passing in the command variable and that's it for implementing create image one note on the create image function it will create an image that is private meaning only your account can launch instances with this image to change the visibility of this image you would use the modify image attribute command after the image has been created or you can modify it in the console or with the CLI the last step is to get the instance ID that we want to create our image from we can use the manage ec2 instance file to do this open the file and make sure the list instances operation is uncommented and not terminate instances we don't need to terminate anything then save that file and then move over to your command line and run node manage ec2 instance.js you should have some instances output in case that you are just barreling through this course you may see both the instance we just created from the AWS Marketplace and the one that we had previously terminated with the Amazon Linux on it if so look for this state property one of the instances should show as running while the other should show as terminated copy the instance ID for the instance that shows that it's running now move back to your code editor and open up that create ami.js file on line 12 where it says instance ID paste over that with the copied instance ID from the command line and finally save the file now we're ready to create our Ami move back over to the command line and run node create ami.js it should process for a second or so and then print out complete meaning that it's done you can go to the ec2 dashboard in your browser and then to Amis in the menu to see the image that you just created one thing I want to mention before we finish this clip is that even though you've created this image the user data startup script that we were using doesn't actually attach to that image this means that even if you start a new instance with the image that we just created the demo project won't be started when the instance is created the code for the demo project will be present on the instance it just won't run reproducible user data would exist in a launch template and that's what we're going to be talking about in the next module all services and resources in AWS have limits it's important to be aware of them so that they don't trip you up when designing AWS architecture and that's the worst thing about limits you end up hitting them at the most inopportune times so let's take a look at some of the limits involved with ec2 and Amis ec2 has a limit on the max number of running instances per region this actually gets a little complicated though so I'll show you where to view these limits go to the ec2 dashboard in the AWS console on the left hand menu there is a limits entry and click on it you'll see there are a lot of limits here and they get very specific they're also instance type specifics so you can really do a lot of damage if you get creative with the image types you're using scrolling down you can see there are a few other types of limits listed here Beyond running instance limits one important thing to notice about all of these limits though is that you can select any of these limits and use the request limit increase button this means that these aren't hard limits you can ask AWS to raise them if you ever need to if you're using AWS with a corporate account or for your company then raising these limits is rarely an issue this page largely contains all the ec2 limits as for AMI limits we've already discussed that Amis are region specific so definitely keep that in mind there doesn't appear to be a limit to the number of Amis specifically that you can have but since each Ami is stored using an EBS snapshot and there is a limit of 10 000 EBS snapshots I would assume the max number of Amis you can have is 10 000. also keep in mind that you do have to pay per EBS snapshot so having 10 000 Amis is not going to be cheap we've taken a good tour of what ec2 has to offer so you should have a good understanding of how Computing Works in AWS let's recap what we covered in this module we started by discussing some specifics of ec2 instances instant storage and Amis then we talked about the different classes of ec2 like spot instances reserved instances and on-demand instances which is what we're using after that I walked you through how the signing process happens when making a request to the AWS rest API and luckily we don't need to manually do that since we're using the SDK then we started writing code as we created a security group a key pair and finally an ec2 instance once we had that instance we looked at other SDK functions like listing instances and terminating instances then we took a trip to the AWS Marketplace to pick up an Ami pre-loaded with node and finally we created an Ami from an instance that we can use when creating other instances hello there and welcome back to AWS developer designing and developing this module is all about scaling Computing and we're going to look at two of the major tools to automatically scale your instances scaling is about meeting the demands on your resources without you having to stay awake all night in order to achieve this magic we'll first take a look at what scalability and elasticity mean within AWS with this knowledge we'll then take the first step by creating a launch template that describes the application instance we want to run then we'll create a load balancer that will handle and distribute incoming HTTP requests both the launch template and load balancer are needed for us to create the main scaling resource which is an auto scaling group and the last coding step we'll need is to configure a scaling policy on that auto scaling group with those four pieces in place our scaling configuration will be complete we'll close out the module by looking at limits that exist with auto scaling and elastic load balancing now let the scaling begin in the world of the cloud and AWS terms such as scalability and elasticity are thrown around like they're the most common things in the world the terms are often used interchangeably something which I'm guilty of as well I found however that the nuances of these terms often aren't completely understood and can result in some confusion in how each impacts your Cloud architecture with AWS in this clip we're going to talk about each one in detail scalability in terms of AWS refers to the ability for your resources to increase or decrease in size or quantity there's a lot of infrastructure involved to make something like this happen so it's no easy task many of the services in AWS are scalable by default which is one of the reasons that AWS is so successful scalability is pretty simple to Define which is why some of the aspects of elasticity are often attributed to it elasticity is the ability for your resources to scale in response to stated criteria often cloudwatch rules this is what happens when a load balancer adds instances whenever a web application gets a lot of traffic scalability is required for elasticity but not the other way around not all AWS surfaces support elasticity and even those that do often need to be configured in a certain way it might seem redundant but the services in AWS that start with elastic often support elasticity scalability gives you the ability to increase or decrease your resources and elasticity lets those operations happen automatically according to configured rules together they let you sleep soundly at night with the assurance that your applications are healthy no matter the situation now in the next clip we're going to start our path towards elasticity by creating a launch configuration in the last module I mentioned launch templates and how they are the place that you'll persist your user data it's true a launch template is the blueprint for automatically creating new instances it contains all the key attributes for how to create an instance including the Ami to use the instance type instance security groups and the startup user data although they do stand alone and you can actually manually create new instances just directly from a launch template they're also a key component in an auto scaling group before AWS created launch templates Auto scaling groups used launch configurations to launch instances you can still use launch configurations for auto scaling groups but launch templates give you one huge key advantage and that's the ability to create new versions of the same template launch configurations weren't versionable so anytime you made a really small change a completely new launch configuration had to be created with launch templates you can create a new version if you need to update any of the configuration detail and then you can just update the version through using tags on the launch template or you can manually specify a version in your auto scaling group it is a much cleaner and more modern way to define your instance configuration let's create a launch template for our demo project go to your code editor and open the zero four folder in the scripts directory we'll be modifying the create launch template JS file the script is pretty simple with only one function that we need to implement also since we've already gotten our feet wet with the AWS SDK I'm actually utilizing a lot more helper functions here so you can write less code and focus more on the important stuff for instance I've already imported the create launch template command here from the ec2 client also you may have noticed that there is no ec2 client Constructor being imported here that's because I've already done that and implemented a send command function in the helpers.js file in the execute function you can see that we're also using a create IAM role function from that same helpers file another one that I've already implemented for you that's actually one thing that we need for the launch template that we haven't really talked about yet an IAM role to give the ec2 instances permission to talk to other services in AWS here we are creating a role for the hamster instances an ec2 profile is also created with this function and the role is then attached to that ec2 profile and then the Arn for the profile is returned from this function the profile Arn will be added to the configuration for the launch template so that any instances that are created with it will use that profile be aware this role that the profile is using gives the instance administrator access to your entire AWS account just for simplicity's sake for the demo when you're doing this thing for real make sure that you just you never give any instance admin access in a production account restrict the permissions and the role to only the operations and resources the Apple application on the instance actually needs now we can implement the create launch template function so let's do that we'll start by declaring a params const object the first property is launch template name give it the value LT name next is launch template data which has the value of an object all the rest of the configuration for this params object will be in this launch template data object the first property here will be IAM instance profile with the value of an object inside this object there's only one property needed which is Arn and the value for this will be the profile Arn variable then that's it for that IAM instance profile object the next property is image ID which is the Ami that we want to launch from we'll use the Ami that we created in the last module but we do need to go get the ID for it so to do that switch over to the ec2 dashboard in the AWS Management console and click on the Amis menu option on the left find the image with the name hamster image and copy the Ami ID bring that back to your code and paste it as a string as the value to image ID the next property is instance type which is the ec2 instance type I'll just enter t2.micro here since it's cheap and what we've been using so far any instance type could be entered here next add the property keynain which will be the name of the key pair the instance will be launched with and give it the value of the keyname variable now add a security groups property and set the value to an array these security groups Define the security groups that will be added to each instance that is created in this case we'll use the security group that we created in the last module which is already declared here in the SG name variable so add that to the array the last property is user data since we're launching from an Ami built off the bitnami Ami open up the marketplace ec2 startup.sh file in the scripts O3 folder and copy the base64 string at the end paste that is the string as a value to user data and that's it for the launch template configuration now let's create the command to create the launch template and the next line create a new const called command assigned to it a new Constructor call of create launch template command passing in the params variable as an argument then in the next line we can send the command add the return keyword and then call helpers dot send command passing in the command and that completes the modifications needed for this file we can now run this file to create the launch template even though the other pieces of our scaling architecture aren't ready yet and your command line make sure you're in the scripts slash o4 folder and run node create launchtemplate.js after a bit the launch template details should be output to your command line and that's it in the next clip we're going to create a load balancer as the next piece of our scaling architecture a load balancer is an essential point for any elastic web application establishing a single entry point for web requests that can handle any size load let's create our load balancer by modifying the file create load balancer.js in our scripts folder just like before I've already implemented some of the support code so that you can focus on the actual load balancer creation as we modify this file I'm going to explain what I've already done so that you can kind of understand it at a high level at the top I've already imported two commands for creating Target groups and creating listeners this time the client that we're using is actually different from the ec2 one this client is the elastic load balancing V2 client both of these commands are used in functions that I've already implemented below we will be using another command for creating the load balancer so let's go ahead and import that now under the create listener command add create load balancer command and don't forget the comma after it now in the variables section you can see there are two variables that we need to modify the first is the VPC ID for your default VPC since the load balancer will be created in a VPC we need to give it that information to use move to the AWS Management console and go to the VPC dashboard we'll get the VPC ID first by clicking on your vpcs in the left hand menu you'll likely only have one VPC so copy the ID for it and your code editor paste that value in the string after the variable named VPC ID now go back to the Management console and click on the subnets menu entry we need at least two subnets for the load balancer and they need to be in the VPC that we just pasted I don't have any additional subnets but you can use this VPC column to make sure that you're choosing the right ones I'm going to use the subnet Setter in Us East 2A and Us East 2B which I can identify using this availability Zone configuration detail when we're setting up the auto scaling group I'll use those two availability zones which is why I want to choose the load balancer subnets to match that now copy each one of the subnet IDs and paste them into the subnet array as strings great just like the other script files this execute function calls the operation functions that we are implementing it starts by creating a new security group for the load balancer that is open on Port 80. this is so that users can access the load balancer over HTTP from their web browsers then a Target group is created and a load balancer I'll explain the target group in just a second the responses from each of those calls are stored in variables so that we can then get the Arns for the Target group and load balancer then a listener is created with those values which connects the load balancer and the target group I want to explain the relationship of all these different resources quickly a load balancer is in charge of responding to http requests and sending them somewhere a Target group is the destination defined where a load balancer will send those requests this is going to be connected to our Auto scaling group and will basically end up being a group of ec2 instances with our demo project running on them The Listener defines a relationship between the two in our case the load balancer will be listing on Port 80 which is for HTTP and our Target group will be listening on Port 3000 since that's what our demo application is using the listener will facilitate sending those requests between the two ports now let's implement the create load balancer function start by creating a params const object the first property will be name which you can give the value lb name then add the security groups property which will Define these security groups in the load balancer itself set the value as an array with just one value the variable sgid next add a property named subnets set this property to the variable subnets which we populated earlier then add a property named type and give it the value of application as a string the default value for this property is actually application but I like to make it explicit so that you can clearly differentiate it from other types of load balancers now we can create our Command on the next line add a new const called command assigned to it a new Constructor call of create load balancer command passing in params as the argument then in the next line add the return keyword and then call helpers.send elb command passing in command this send function is different from the one that we used before because it is going to be using a different client to send the command the elastic load balancing V2 client I've already implemented the send function with that client it this function just has a different name and that's all we need to modify below this function are functions to create the target group and The Listener and no modifications are needed for them so now we can run this file move over to your command line and execute the command node create load balancer.js there should be some output specifically I want to draw your attention to the line that says Target group name Arn basically because we will need this in the next clip so you can either leave your command prompt output alone don't delete it or you can copy this Arn and save it somewhere like I'm going to do and with this we have a load balancer ready to distribute web requests to instances and the next clip we're going to create an auto scaling group that will create those instances Auto scaling groups are responsible for creating and removing ec2 instances from a group according to configured rules they use a launch template when creating the instance and they can be attached to a Target group so that each instance can be used with the load balancer let's create our Auto scaling group open the file create autoscaling.js just like the other files you'll see that I've already imported some commands for you this time from the auto scaling client I've also implemented a special send command function for the auto scaling commands which I'm importing from the helpers file here now replace the value of the string for tgrn with the target group Arn that we copied from the command line in the last clip you do still have that saved to your clipboard right well if not you can easily get it from the Management console right here now we can move on to implementing the create auto scaling group function start by declaring our params const object the first property is auto scaling group name which identifies this Auto scaling group in the Management console enter the argument ASG name as the value here next add the property availability zones and set the value as an array these are the availability zones where the auto scaling group will launch the instances at least two need to be defined these availability zones should match the subnets that we used in the load balancer so that the auto scaling group doesn't create an instance that can't attach to the load balancer I'll enter uses 2A and USC's 2B the next property is launch template give it the value of an object that object just has one property launch template name which can be assigned the value of the ltname argument this is how the launch template that we created is going to be connected to the auto scaling group when the auto scaling group needs to create a new instance it'll use the configuration that's saved in its launch template to create the instance and remember that launch templates are versioned if we don't explicitly Define a version here then the auto scaling group will use the version that's tagged default on the launch template you can update which version which number version is tag default on the launch template by just modifying the launch template currently version one is tag default on our launch template because that's the only version that exists next add the property Max size which defines the maximum number of instances to create I'll enter two then add the property Min size which defines the minimum number of instances to have in the group I'll enter one here the next property will be Target group Arns with the value of an array enter the tgarn variable which we configured earlier this property ties the auto scaling group to the Target group now we can create the command on the next line create a new const variable named command and assigned to it a new Constructor call of create auto scaling group command passing in params in the next line add the return keyword and call send Auto scaling command passing in the command variable and that's all that's needed to create an auto scaling group now we're done with the auto scaling group creation code but we're not actually going to run this file until we've completed the attachment of the auto scaling policy code and we're going to do that in the next clip so see you there an auto scaling policy defines when an auto scaling group should scale up or down and there are a few different types of Auto scaling policies simple scaling policies are the original way that auto scaling groups worked these used a cloudwatch alarm to monitor certain attributes and then perform an action once the alarm actually occurs only one action could be configured per alarm therefore if you wanted true elasticity you needed a policy for scaling up and a policy for scaling down step scaling policies were added as a way to define multiple actions when a single alarm fires this allows you to modify the behavior of a policy based on the severity of the alarm that's being fired the policy will keep scaling even after it's been initiated which differs from simple scaling policies the newest scaling policy and my favorite is Target tracking policy it's what we're going to be using Target tracking policies allow you to just Define a target for a metric attribute and the policy the single policy will take care of scaling up or down to stay pretty much right at that threshold it's the easiest policy it's kind of like a set it and forget it type operation and it is what Amazon suggests you to use now for auto scaling groups in our code editor back in that create auto scaling JS file we're going to implement the create ASG policy function to start with we need a params const object so go ahead and create that we're going to be putting all of our policy details in here the first property is adjustment type and this defines what the policy should do enter the string change in capacity here the next property is auto scaling group name and you can set the value to the ASG name function argument next add the property policy name which is just the unique identifier for the policy you can set the value to the policy name function argument the next property is policy type enter the string Target tracking scaling other options for this are simple or step which I explained earlier next add the property Target tracking configuration and set the value to an object these are the details about what metric to use the first property for this object is Target value we're going to be watching CPU so if you enter the number five then the auto scaling group will scale up and add instances if it's above five percent CPU utilization or it'll scale down and remove instances if the average Falls below so enter the number five here the next property is predefined metric specification and given an object as a value this object has one property predefined metric type with the value of a string with the text ASG average CPU utilization because CPU utilization is a predefined metric and it's well understood by Auto scaling groups this is how we can use it and it's going to average the CPU utilization across all of the instance instances in the auto scaling group after the params object create a new const called command assigned to it a new Constructor call of put scaling policy command passing in the params variable on the next line just return a call to send Auto scaling command passing in the command and that's it to set the policy on our Auto scaling group so now we have the group being created and the policy being attached so let's run this file and watch it work in your command line execute node create autoscaling.js after some time processing it should output the result of the auto scaling group creation I'm going to skip this video and time a bit as I usually do and we can see that the auto scaling group was created successfully and an instance has also been created for the group if you go to the load balancers menu option and copy this DNS name into your browser from the load balancer that was created you'll see the demo project pull up this is a result of each one of those pieces that we put together over the last few videos the load balancer is routing our request to an instance that was created by an auto scaling group using a launch template pretty awesome in the next clip let's look at limits that you may run into when using Auto scaling and load balancers limits for auto scaling largely Define how many of each resource that you can have the ec2 limits page that we looked at earlier also contains limits for both Auto scaling and load balancing while these aren't all the limits that exist for auto scaling in elastic load balancing they are the ones that you can increase like the soft limits but let's take a look at some of the most important limits in my opinion with auto scaling the main limits are on the number of groups and launch configurations that you can have I think these are sufficiently large enough that you wouldn't hit them unless you have an entire company working on one account and again it's something that you can raise as far as I can tell there are no limits on how many launch templates that you can have so go wild with that load balancing limits are also defined around the number of resources if you do a lot of internal load balancing the application load balancer limit may not be enough so just keep an eye on that another load balancer related limit limit is the number of certificates that you can attach to a load balancer which is set at one this is important when you're creating a web application because you'll need individual public internet facing load balancers for each TLS endpoint that has a unique certificate you'll actually attach the SSL certificate to a listener but you can only have one listener on Port 443 for each load balancer so that limits the number of certificates for https I've never actually had a problem with this because I've never actually needed more than one SSL certificate on a load balancer but if you do that's how you would solve it by having multiple load balancers in addition you can only have one load balancer configured for each Target group but you can have multiple Target groups for one load balancer and that's about it for the major limits for auto scaling and load balancing scaling is the lifeblood of the cloud and scaling Computing resources is one of the most common operations done in AWS this module walks you through everything you needed to be able to configure scaling efficiently we started by defining the differences between scalability and elasticity essentially elasticity scales based on configured rules then we started implementing our scalable architecture by creating a launch template creating the load balancer was next followed by the all-important auto scaling group and an auto scaling policy in addition the rules that tell an auto scaling group when to add or remove instances finally we discussed the resource limits you'll work within when using Auto scaling or load balancers in the next module we're going to look at different types of storage in AWS specifically elastic Block store for ec2 instances and S3 storage for all-purpose object storage we'll work from the resources that we created in this module so don't tear anything down yet see you next time hi there and welcome back to AWS developer designing and developing file storage is one of computing's oldest concerns and something every developer has to deal with there are several different solutions in AWS for storing your files and each Target a specific use case in this module we're going to start by discussing options for ec2 instance file storage then we'll try our hands at reusing an EBS volume between two ec2 instances then we'll switch over to S3 to create an S3 bucket and then upload objects into that bucket finally we'll talk about some important limits with EBS volumes and S3 without further Ado let's get to storing ec2 provides you three different storage options elastic Block store or EBS elastic file system or EFS and instant store volumes each were introduced at different times showing a sort of evolution and process for AWS ec2 storage instant store was the first storage type for use with ec2 instances these volumes are physically connected to the server where your instance is running they used to be the default storage option but there are a few reasons why they're not best for General use anymore ec2 data on an instant store volume is completely wiped if the connected instance stops or terminates data stored on an instant store volume is basically stuck you can't create a snapshot from an instant store volume you could move the data off the volume but it's completely up to the user and it's a manual process EBS is the preferred way to store data with an ec2 instance these days EBS volumes live independently from the ec2 instance and can be detached for reuse backed up with snapshots and allows the instance to be stopped there are also lots of options in regards to the types of volumes you can connect this table shows you the options you can choose when creating a new EBS volume EBS volumes are also replicated within the same availability Zone to ensure that there's no loss of data EFS is an Evolution Past EBS but also serves a very specific need EFS is built to connect to multiple instances at once and scale the file system capacity as needed the product documentation advertises capacity growing to petabyte scale but I haven't been able to test that EFS is good for big data type operations and is more of a dedicated file system which isn't optimized for a root disk for an ec2 instance EFS provides data durability by replicating volumes across multiple availability zones in the next clip we'll play with an EBS volume to see how it can be reused there are a lot of different things you can do with EBS volumes like creating snapshots creating new EBS volumes from snapshots and attaching multiple volumes to an instance just to name a few for this course I wanted to give you just a taste of some of these EBS operations in this clip we're going to detach an EBS volume from one instance and then attach that volume to a different instance in the scripts o5 directory we'll start with the stop ec2 instance.js file this file does more than just stopping an instance it will first stop an instance the one that will be removing the EBS volume from but then it will launch a new ec2 instance so that we can attach that volume to that instance that's going to all happen in this one file after this we'll modify a different file that will manipulate the EBS volume but let's go ahead and modify and run this file first there are only three small pieces we need to change in this file first we need to add in the instance ID for the instance that we want to remove the EBS volume from this will be the instance that we created manually not one of the auto scaled ones let's go to the ec2 dashboard to find this click on the instances menu option on the left you should probably have two to three instances now one that you created manually and then one or two that were Auto scaled one of them will be the instance that we launched with the Ami from the AWS Marketplace you can tell by selecting the different instances and looking for the one that has the Ami name of a long string that starts with bitnami node when you find it note which availability Zone that it's in this is going to be important so that we can launch our new instance in the same availability Zone copy the instance ID and then move to your code editor and paste it as a string for the value of instance ID next in the params object in the create instance function we have a placement object which describes which availability Zone the incident should be launched in this should be the same availability Zone as the instance that we are removing the EBS instance from the one we just stopped so enter the availability Zone that you remembered from the AWS Management console this is all necessary because EBS volumes can only be connected to an instance in the same availability Zone that the volume is in since the volume was launched and the same availability zone is the instance that was launched previously we want them to match this is a practical consideration since the latency when connecting a volume in a different Data Center and a different availability Zone even though it's super low because it's staying within the AWS Network it could be prohibitive to actually doing anything with CPU it needs a much closer connection with that storage volume now at the top of this params object we need to add the image ID to use to boot our new instance the instance doesn't need a special Ami so just Amazon Linux will do so go back to the AWS Management console and you can click this launch instance button then copy the Ami ID from the very first Amazon Linux 2 entry for the x86 architecture then paste that as the value for image ID now you can go to your command line and make sure you're in the scripps05 folder and run node stop ec2 instance.js this will stop the instance we're going to remove the EBS volume from and create a fresh brand new instance to attach the volume to this is a great time for a quick note why are we actually stopping the instance here well to detach a root volume from an instance that instance Must Be Stopped the instance in question only has one attached EBS volume and that is its root volume to detach it we need to stop it first if it had multiple volumes and we were detaching a non-root volume well we could detach that volume while the instance was still running all right now open up the ReUse EBS volume.js file this script does two things it detaches a volume from one instance and then attaches it to another pretty simple so let's make this script functional the first step is to find some IDs we will need the volume ID of the EBS volume that will be detaching and the instance ID of the ec2 instance that we just created for the second value you can get that from the command line in the output to the command we just executed copy the instance ID and paste it as a string for the value of instance ID for the volume ID variable we need to go to the ec2 dashboard again in the left hand side menu click on volumes you should have a number of volumes here equal to the number of running instances which volume ID do we want well we need the volume attached to the instance that was stopped so look at the instance ID that you had copied and select the volume that is attached to that instance then copy the volume ID and paste it as a value for volume ID now with the correct values in place let's implement the detach volume function start by declaring a params const object the only property that AWS needs to know when detaching a volume is the volume ID so add a volume ID parameter and set the value to the volume ID argument that we passed in then on the line after the params object add a new command const variable and then assign to it a new Constructor call of detach volume command passing in the params variable then on the line after that you can add the return keyword and then call send command passing in command now once the volume is detached it'll be available to be re-attached to another instance so let's Implement attach volume now first create a params const object the first property to add is instance ID which designates the instance to attach to set the value to the instance ID function argument the second property is volume ID which is the volume to attach set the value to volume ID and then the last property We'll add is device this defines the name that will be used to mount the volume on your instance these values are usually things like slash Dev slash sda1 or Dev slash xvda these are all kind of like Linux mounting things we'll call ours something way more creative than those enter slash Dev slash SDF alright so now on the line after the params object create a new const called command assigned to it a new Constructor call of attach volume command passing in the params variable then after that add the return keyword and call send command passing in the command now when we run this it'll swap the volume in question to the new instance there's just one more thing to do though we need to stop the newly created instance before we can attach this EBS volume to it more stopping why do we need to do that well any volume launched from an Ami with a thing called a product code cannot be attached to a running instance I believe this restriction is in place because when an instance starts up any product codes on attached volumes are copied to the instance and then they're used to make sure the correct subscriptions and charges are applied to that running instance so it's sort of a compliance billing kind of concern unfortunately AWS has pretty much hidden any possible way to find out if a volume has a product code anymore they've removed it from the Management console there's no CLI operation that'll show it to you for volumes that you've launched from a custom Ami that you own in your account you can view the product code that is still shown on the snapshot for that Ami but otherwise you just have to try to attach it and see if you get an error I know we'll get an error so we're just going to go ahead and do this but regardless it's probably actually a better idea to stop an instance before attaching a volume if it's possible so let's go shut down the instance move to the ec2 dashboard in the AWS Management console click on the instances menu option and find the instance we just created you can tell by the Ami ID that's shown on the instance it should say Amazon Linux 2. select it and then click the instant State drop down and then stop instance yes we could have done this with our file as well but you know this is just a little bit quicker I'm sure you don't want to write any more code that is unnecessary uh you will need to keep an eye on this instance to see when it has finished stopping but I'm going to skip in time a little bit here so you don't have to wait with me in your command line run the command node reuse EBS volume.js once you get some output it should have detached and reattached the volume to the new instance now we can check our work by going to the ec2 instances in the AWS Management console refresh it and select the target instance that was attached to in the instance details click on the storage Tab and there's a section for blocked devices and you should see the volume that we just attached it should have the device name Dev slash SDF now you could start the instance again if you wanted to because it's currently stopped and if you did that you'd then have full access to all the data that's on that volume there's plenty more that you can do with EBS but I hope that this gave you just an idea of the versatility of the service in the next clip we're going to get started with S3 buckets S3 is a monster in AWS often rivaling ec2 and its ubiquity across the cloud provider the reasons for its popularity are the reliability speed and ease of use that it has but there are some automated aspects of the service that make it even more useful for applications two of those features are versioning and life cycle events versioning allows you to store multiple iterations of the same object in case you need to access a previous revision this can be configured at the Bucket Level and applies to creating deleting or modifying when a file is created it becomes the current and only version of that file if you upload a new file with that same key the new upload becomes the current version and the original is retained with a unique version ID if you then delete the file it shows as deleted but you can always get the older versions of that file as a simple bit that can be flipped on a bucket versioning is tremendously useful the only downside is the cost of keeping multiple versions for each file each version of a file is charged the exact same as the file itself so depending on how often a file is replaced or deleted this can multiply your S3 costs quite a bit one way of managing those S3 costs not only for versioning but also for normal usage is by configuring life cycle events for your bucket a lifecycle event allows you to set certain rules for what to do with an object in your bucket one says certain number of days has passed one life cycle event you can trigger is to move an object to a lower cost type of storage for instance you can say that once a file has been in S3 for five days move it to either S3 infrequent access storage or Glacier storage both offer much cheaper costs for files that don't need to be accessible quickly you can use this option for previous versions of files as well this means you can move them to Glacier and offset the cost of having versioning on your S3 buckets the other convenient life cycle event is expiration which allows you to delete files once they've existed for a given number of days as with moving objects you can apply rules to only affect previous versions permanently cleaning out older versions once they've been retained for a certain amount of time using both of these features can add to the robustness and usefulness of S3 as the ultimate storage utility in the next clip let's get started by creating our first S3 bucket the most fundamental construct in S3 is the bucket and it's required before you actually store anything in the service so let's create a bucket so that we can store the assets for our demo project in the scripps05 folder open the create s3bucket.js file this script is super simple it just creates an S3 bucket with a name and some configured permissions that's it we'll start by creating the name for our bucket now S3 bucket names do have to be globally unique which means that you'll need to add something to the end of this bucket name to make yours different from mine and everyone else's so replace the comment after hamster slash bucket with I don't know whatever you want also make sure your bucket name does not contain an underscore so maybe not whatever you want just whatever you want as long as it's not an underscore we will be using the bucket name in a domain name later and underscores aren't allowed in URLs for some reason now let's implement the create bucket function start by creating a trusty old params const object creating this bucket doesn't require a lot of properties the first property is bucket and you can give it the value of the bucket name function argument the next property is ACL all uppercase and it stands for Access Control list this basically defines the permissions of the bucket and its objects the value here is one of the several canned ACLS that AWS provides which are just predefined sets of permissions enter the string public Dash read as the value here and with our params object complete we can now create the bucket on the next line create a new const command and assign to it a new Constructor call of createbucket command passing in the params variable then add the keyword return on the next line and call send S3 command passing in the command variable this is a command sending function that has been imported from the helpers file up above that I wrote previously it uses an S3 client to send the command and with that our script is done you can go ahead and switch over to your command line and execute the command node create s3bucket.js you'll see some output once it's complete and your bucket will be created if you really want to you can double check in the Management console as well if you want to ensure that the bucket creation was successful in the next clip we're going to upload some of our demo project static assets to this bucket S3 buckets are great for storing all types of objects pictures videos text files you name it it's particularly nice for static assets for websites and that's what we'll be using it for we'll be using the file upload S3 objects.js so open that in your code editor just like with other files I've already written a lot of code to help you out here specifically the code that's going to be loading files from your file system to upload the modifications to this script will only target the put object to S3 operation so start by replacing the string value in the bucket name variable with whatever you set it to in the last video the execute function is where the code actually runs a helper function is called that gets all the files in the public folder of the demo project each one of those files is iterated through with a for Loop the upload S3 object function is called on each file and then the e-tag confirmation is logged to the console that's it so let's implement the upload S3 object function start by declaring the params const object the first property is bucket which you can set to the bucket name function argument obviously this is where we want to upload the file to the next property is ACL which is pretty much the same thing as the property that we used when we created the bucket but this ACL property is specific to the file that's being uploaded and since all the files should be exposed to end users enter the string public Dash read here the next property is body and this is the actual file contents which can be accessed by setting the value of the property to file dot contents the next property is key which is the object key identifier that should be used in S3 we can use the file name for that so set the value to file.name the next property will be different for each file and the property name is content type this should match whatever type of file is being accessed so I've created a helper function called helpers.getcontent type call that here and as the value and then pass in file.name the helper function will return the correct content type based on the file extension from that file name and that's it for our params object on the next line create a new const called command assigned to it a new Constructor call of put object command passing in the params variable after that enter the return keyword and call helpers.send S3 command passing in the command variable and that completes our modifications for this file move over to your command line we do need to build the project once so that the JavaScript and CSS files are generated we haven't done that yet for this demo project run the command npm run build this will run webpack to compile all the assets and it can take a while it doesn't have any output it's just going to sit here on this blank screen but once it's complete it'll send you back to the prompt then we can upload the files execute the command node upload S3 objects.js this operation may take a little longer depending on your upload speed from your computer but once it's complete you'll see the output for each file that was uploaded our files are now an S3 yay but we aren't using them yet oh we need to update our asset references in the demo project source code so switch over to the AWS Management console and go to the S3 dashboard click into the bucket that we created and select just any one of the files here just click on the file name I'm going to use application.min.js this object URL value is what we want but we really only need part of the URL for our purposes which is everything in the link up until the file name so select and copy that and then we'll use that with the asset references and our demo project so move over to your code editor we only have two places in our application where we need to modify the path for the assets and the first is in the handlers slash template directory the file is index.html on line 5 we're importing a style sheet and right now it's referencing that style sheet from the public directory so remove the public part of that path and paste in what you got from the S3 object make sure there's not a double slash there before a style sheet your pass should look something like this the main difference is going to be your bucket name on line 7 do the same thing with the reference to the favicon icon then on line 12 we're bringing in the JavaScript file so again remove public and paste over that path the other place that we need to modify the path is in the util folder and the file is assets.js at the very top of the file there is a base const being created and it should have public right now you can remove that completely and paste the S3 URL that you have just make sure that there is not a slash at the end so that the final path for those images is created correctly in the next clip we're going to look at some limits that you may run into with EBS and S3 both EBS and S3 have limits that you might actually hit a little bit more than some of the other services that you've seen with EBS I've already mentioned the limits around attaching and detaching but I'll reiterate them here an instance must be stopped in order to detach a root volume if the volume isn't a root volume then it can be detached at any time without the instance having to be stopped an EBS volume can be attached to a running instance at any time as long as it does not have an AWS Marketplace product code if that exists then the attaching instance Must Be Stopped one important limit about EBS is that volumes can only be attached to instances that are in the same availability Zone while EBS snapshots are stored across availability zones instances and volumes must be in the same availability Zone to keep latency low with S3 there's a soft limit on the number of buckets allowed and it's set to 100 at default within a bucket there's no limit to the number of objects aside from the size of your wallet as mentioned before bucket names must be globally unique not just unique in your account and that's most of the limits you may hit in your daily usage with either service there's no limit to the number of uses for file storage and we were only able to cover a small amount here regardless this module has given you a good overview of EBS and S3 and hopefully sparked your imagination for what you might want to use them for we started by talking about the trade-offs between file system types for ec2 instances with EBS EFS and instance store ultimately EBS was the best for our purposes then we went through the practice of detaching and then reattaching an EBS volume between two different instances next we got into S3 storage and created our first bucket but an empty bucket is worse than useless so then we uploaded all of our static assets from the demo project to the bucket finally we talked through some common limits found in EBS and S3 in the next module we're going to look at another type of persistence in AWS as we look into dynamodb and RDS and using redis caches with elasticache make sure you stick around and I'll see you in the next module hello there and welcome back to AWS developer designing and developing along with Computing and storage databases and persistence are the third pillar upon which stands the halls of AWS there are plenty of different persistent Solutions in AWS and they continue to upgrade and add new ones every year the key Solutions are dynamodb for nosql and quick document data storage RDS for traditional relational databases and elastocache for short-term caching in this module we're going to start by looking at how provision throughput and secondary indexes work in dynamodb then we'll create a dynamodb table to store our hamsters and look at a few ways to query and get data out of that table next we'll learn how to create an RDS database with the SDK and connect our code to that service then we'll work with elasticache with the SDK K by creating a cluster and then using that to Cache our user sessions and finally we'll look at the all important limits we may hit when using dynamodb RDS or elastic cache throughput capacity in dynamodb is the key measure for how your tables will perform there aren't really any other attributes in the table that you need to configure but throughput isn't exactly that simple at its most basic it can be defined as the number of Records you're allowed to read or write per second up to four kilobytes for reading and one kilobyte for writing but there are so many caveats and details to that description that it sometimes needs a full video to explain which is why we're here I want to start by going over how to calculate throughput and then go over each type of capacity mode for dynamodb tables AWS has released many new Innovations with these over the past few years so there may be some surprises here for you so let's talk through some scenarios to better understand how this provisioned throughput thing works we'll start with a simple table of hamsters like the one we'll set up later in this module we'll configure this table with five units of provision throughput for reading and writing now let's say we have six hamster records locally that we're going to populate the table with if we do a batch write call it'll essentially try and write all six records at once there are a couple of considerations here how large are the records in our case they are only a few properties with not much text and come in at under a hundred bytes each Dynamo will round up each right to one kilobyte if it's under so each right will consume one unit of provision throughput but wait that means we'll be writing six units at once which exceeds our five provisioned units for writing well in this case you will have one of two things happen and you won't be able to really guarantee which will happen either the sixth right will return a provision throughput exceeded exception or burst capacity will be consumed and the right will go through AWS documentation says that burst capacity will be used when it can but there's no guarantee this basically means if you occasionally exceed your provision throughput AWS may let the requests go through because we're using the SDK it actually already has built-in code that retries if it gets a provision throughput exceeded exception so you'll likely rarely see that error unless it's an extreme case now let's look at reading the records once they've been populated let's say you want to get all six records at once doing a scan there will be six reads that will happen within a second and even though the records are well below the 4 kilobytes reserved for each unit each will consume a full unit it'll round up except they won't because by default the SDK does an eventually consistent read if you're doing eventually consistent reads you get double the reads per unit so each unit will give you two reads instead of one read which if you were doing a strongly consistent read you would only get one read out of that unit the consistency here relates to AWS replicating your data across availability zones in a region eventual consistency will get you the record but it may not have very recent changes to it strong consistency ensures that you get the newest version of a record with any changes no matter how recent so back to our hamsters our scan of six items with eventual consistency will actually only consume three Reed units which is quite convenient now we'll just run through another quick example let's assume we've got a table with some binary items they're about 20 kilobytes each how many units would it consume to read one item well if we're doing eventually consistent reads it would take three read units each unit in this consistency will hold up to eight kilobytes a 20 kilobyte item will technically use 2.5 units but AWS rounds up so we've got three what about if we wanted to read it with strong consistency now it would take exactly five because each unit would only account for four kilobytes let's look at writing then how many right units would it consume unfortunately there's no consistency discount for writing so a 20 kilobyte item would consume 20 right units that's a lot more than reading for sure now let's talk about the capacity modes in dynamodb there are two modes provision capacity which is the original mode and on-demand mode the new mode that is more flexible let's look at provision capacity mode first with provision capacity you specify the number of reads and writes per second for the table or index any requests above those limits could be handled by burst capacity or could be rejected AWS has released Auto scaling for provision capacity mode so you could configure rules that would scale the provision capacity up or down based on an increase in requests with the configured reads and writes for your table you do pay regardless of whether you actually are using that capacity or not and then there's on demand capacity mode this was released in the last couple of years and is a model that charges you per read and write requests unlike provision capacity you don't pay for anything that you don't use however the read write requests do cost more than if they were configured with provision capacity you gain flexibility but it may be more expensive depending on actual use on-demand capacity also scales as needed you don't need to configure anything it's a true set it and forget it capacity mode and that's about it hopefully those examples have given you an idea of how throughput is calculated in AWS and the capacity mode review has given you some ideas for configuring your tables in the next clip we're going to talk about Dynamo keys and indexes in order for dynamodb to achieve high performance it spreads your data across many different partitions the attribute it uses on each item to decide which partition to use is called the partition key also sometimes it's called the hash attribute this key is passed to an internal hash function and the output is used to assign the item to a partition this partition key must be unique in your table kind of like a traditional primary key there's another type of key schema you can use instead that lets your partition key be non-unique and that's by adding a sort key also sometimes called arrange attribute in the case that you define both of those keys Dynamo will use the partition key as input to the hash function to find which partition to use and then it'll use the sort key to sort all the items found at that partition location both of these key schema types allow you to be very flexible when defining how your table should work but sometimes a single set of keys isn't enough to properly query your data in this case you can use a secondary index to provide alternate query methods there are two types of secondary indexes Global secondary indexes and local secondary indexes let's start by discussing Global secondary indexes with a global secondary index you define a new key schema to organize your items and query on you could have a partition key schema or partition key plus sort key schema in addition you'll Define which attribute you want to include in the index if you query on the index only selected attributes will be accessible in some ways Dynamo is just replicating the data and organizing it in a different way another nice thing is that when you create a global secondary index you define new provision throughput capacity exclusively for the index when querying on that secondary index only the throughput configured for that secondary index is used a local secondary index is very different from a global one with a local secondary index you define an additional sort key to query data at the partition level differently the original tables partition key is used in conjunction with the local secondary indexes sort key to query the data Additionally you don't create provision throughput capacity or Define a set of attributes all of the base tables properties are carried over in many ways a local secondary index just enhances the original data where a global one creates an exclusive new set and with both indexes Dynamo maintains them completely when adding or deleting items updating the indexes and you don't have to do anything in the next clip we're going to put some of this information into practice as we create a dynamodb table the first step we need to accomplish is to create a table in dynamodb that we can use to store data in our case we'll be creating two tables to store the data for hamsters and races in the scripps06 folder open the create dynamotetable.js file you can see in the execute function we'll be using the same create table function twice to create a hamsters table and a racist table in dynamodb both tables will use the same key schema which is how we're able to do both operations so simply like this so let's go ahead and implement the create table function start by creating a params const object the properties needed to create a Dynamo table will take into consideration the schema and keys of the table as well as the performance attributes of that table we'll start with the first property called table name and give it the value of the table name function argument a quick note here table names do need to be unique in each region at the account level if you use different regions you can reuse a table name the next property is attribute definitions with the value of an array these attribute definitions aren't used to define the whole table schema or anything they really only Define the attributes we'll be using as our key inside the array create an object the first property here is attribute name which will give the value of a string with the value ID the next property is attribute type here we've got three options s for string n for number or B for binary in our case our IDs will be numbers so enter a string with the value of an uppercase in here that's the only attribute we need to Define after attribute definitions add the property key schema this will also be given the value of an array here is where we Define our table Keys based on the attributes we defined inside the array add an object the first property here will be attribute name which will be the string ID this needs to match the attribute name defined in our attribute definitions the next property is key type which will enter as hash in all uppercase now after the key schema property We'll add the last property provisioned throughput which is an object add the property read capacity units with the value of 5. then the property right capacity units also with the value 5. and those are all the properties needed to create our table now on the next line create a new const called command and then assign to it a new Constructor call of create table command passing in params on the line after that you can add the return keyword and then call send dynamodb command passing in command now our script is complete so switch over to your command line and make sure you're in the scripts 06 directory then execute the command node create dynamotable.js when it's complete you should get some output on the second table that was created we just threw away the output from the first table creation the tables will take a few minutes to be created but then we'll be able to use them in the next clip we're going to populate those tables with some data that our demo project needs to use an empty Dynamo table is worse than an empty S3 bucket okay maybe not but still we want to populate our Dynamo table with some data in your scripts folder open the file populate dynamotable.js let's go over what's happening in this script I've created some helper functions to get the initial hamster and race data then after it's retrieved the data it will populate the respective table with it that's pretty much it I want to bring another thing to your attention if you look at the command import at the top of the file you will notice that it is not importing from client dynamodb but instead from lib dynamodb this is a higher level Library than the Bare Bones client the client dynamodb it makes working with dynamodb items way easier by handling the serializing and deserializing of dynamodb data types if you've used the dynamodb document client from version 2 of the AWS SDK then you already know what this Library will do because it's functionally the same code when we send this command we will use a dynamodb document client instead of the regular client to send the command now let's implement the populate table function the input to populate the table will be dynamically created from the data function argument and then we'll use the batch write command to write all the items at once so we'll start by declaring our params const object this one's a little weird because it has a single root property request items give that property an empty object for the value then inside our property key will be the name of the table that we are inserting to to get this from the table name argument use square brackets and put the table name argument inside we'll build the value for this dynamically so give it the value of a call to data.map and pass in an arrow function with one argument called I inside will return an object with the property put request as a note you can include other types of requests in this object such as delete request and the batch write function processes both which is pretty convenient now for the value create another object and give it the property item with the value of the I argument again because we're using the dynamodb document client it'll take care of converting the item to a dynamodb compatible object and with this the items for the put request will be built out dynamically one item at a time on the next line create a new const called command assigned to it a new Constructor call of batch write command passing in the params variable then on the line after that add the return keyword and call send Dynamo item command passing in command and that's it for this script so let's go ahead and try it out in our command line the tables that we created in the last video should be finished creating by now so go ahead and run the command node populate dynamotable.js when it's done processing move over to the AWS Management console and go to the dynamodb dashboard click on the items menu option on the left and then select either one of the tables you should see the items which we pushed so everything is complete in the next clip we're going to connect our demo project up to this table to consume the data that we just put there in this clip we're going to look at two of the main ways to get data out of a dynamodb table the easiest way to get data out of a table is to just use the primary key of the item that you want to get by utilizing this method combined with secondary indexes Dynamo becomes a very flexible and Powerful tool however there are some other methods for retrieving data that we'll look at in this clip and those are scanning and querying we'll get a chance to use both as we get the hamster and race data that we need for this we'll be modifying several files in our demo application to start with go into the lib folder then data and then lib again very creative with my directory names we'll be modifying the dynamo.js file here this file abstracts some common Dynamo operations that are called often the functions can be used with any table and query you'll see with the Imports that we are using things from both the dynamodb client and the library we're importing the regular dynamodb client and then the dynamodb document client and some commands this send command function is like the one I previously hid in a helper's file you can see a Bare Bones client is created then that is used to create a document client we're using this document client because we'll be working with Dynamo items and the document client greatly simplifies these operations let's start by implementing the get all function this function will use one of the first Dynamo data retrieval methods scanning a scan will take a table and an optional start key and attempt to get all the items from that table scans return one megabyte chunks of data this means if your table has more items than that you'll need to do multiple scans to get all the data a single scan will only return the number of items that can fit into that one megabyte and then it'll return a start key to use with your next scan for our purposes we will only have to perform one scan since our data set is small start by creating a params const object for the scan we only need one property table name give it the value of the table name function argument then in the next line create a new const called command assigned to it a new Constructor call of scan command passing in the params then after that create a new const called response we need to modify the result from the scan to only return the items from that result after an equal sign add the await keyword and call send command passing in command then in the next line add the return keyword and then response dot items with an uppercase i this is how we can access the items from the response and the document client will make those items native JavaScript objects and that's all our get all function needs just this simple scan command the get function will be a little more complicated you can see by the function signature that we're passing in the table name and an ID it's true with these arguments we could just do a call to get and immediately receive the relevant item but this is also a great place to use Query instead so let's do that in query by the ID start by declaring a params const object add the table name property first and give it the value of the table name function argument the next property actually defines the query we're using Create a property called key condition expression the value will be a query on the key so enter a string with the value ID equals colon H key this query will find any records or the ID of the record equals whatever we set as H key the colon before H key indicates that it is a variable We'll add the variable with the next property expression attribute values this will have the value of an object and the property will be colon H key in quotes the value will be ID but add a plus in front of it so that it will be coerced into a number in case it's passed as a string and with this the ID argument is added to the params and configured correctly to work with the key query on the line after the params object create a new const called command and assign to it a new Constructor call of query command passing in the params variable just like get all we need to modify the command response before we return it so on the next line create a new const called response after an equal sign add a weight and then call send command passing in the command then on the next line add return and then response dot items when you use a query response is an array for this get function we just want to return one item which would be the first item in the array so then reference the first item with bracket notation and the index of zero great our Dynamo functions are complete so let's use them move up One Directory and open up hamsters.js we want to change the sources of our hamsters and races data from the current mock to the Dynamo file that we just modified on line two we'll change the value of the data source import by removing the right hand side of the assignment and replacing it with the require function call passing in the string dot slash lib slash Dynamo now save that file and open up races.js and make that same change to line one to import that Dynamo file that we edited and that's it for our Dynamo changes in the next clip we are going to move back to relational SQL land if Dynamo doesn't work for your needs you can always use a trusty relational database the relational database service in AWS is great because it takes care of the maintenance tasks that all databases need performed including automated replicas and backups the data we're working with in the demo project is simple so it could really live in either Dynamo or RDS but I split it up so that you can have an example of working with each in the scripts 06 folder open the create rdsdb.js file in this file we're going to do exactly as it suggests and create an RDS database we'll use it to store the user and user favorites data from the demo app all the modifications for this script are in the create database function so let's Implement that start by creating a params const object for our database creation properties the first property will be allocated storage which lets us configure the storage size of the database the number is in gigabytes and there are some upper and lower limits depending on the database engine we can enter 5 here to represent 5 gigabytes since that is the minimum size allowed for a MySQL instance in RDS the next property will be DB instance class it's kind of funny because they alternate between calling this value in an instance class or an instance type but ultimately it's just the same as an instance type like with ec2 we can use the smallest instance type of a available here db.t2.micro the next property is DB instance identifier you can set the value to the DB name argument then we have the property engine which is the database engine that you want to use you know I really wanted to use aws's own Aurora database engine for this course but it's not eligible for the free tier and it can only be created in clusters not individual instances so it's a little more complicated instead let's just set this to the string MySQL next add a property named DB name and give it the value DB name the next property is vpc's Security Group IDs there are actually two types of security groups applicable to an RDS instance DB security groups and VPC security groups VPC security groups are more versatile which is why we're going to be using them here DB security groups are old and they're actually only applicable when you have an ec2 classic configuration which you probably don't and you probably don't want to figure out how to set one of those up it's kind of an old way of doing things for the value of this property create an array and add one item the sgid argument the last two properties are the database credentials add the property Master username and set it to the string admin then add the property Master user password and set it to the string my password all lowercase we are setting them explicitly here because that's what's already written in our code now in the next line create a new const called command and then assign to it a new Constructor call of create DB instance command passing in the prams variable then in the next line return a call to send RDS command passing in the command and that's it for this function let's run this file to create our database instance switch over to your command line and execute the command node create rdsdb.js when it's complete it should spit out the details of the instance that was just created getting an RDS instance booted up can actually take some time but we do need to modify some code in our demo application to utilize this database so let's just do that back in your code editor open the file lib slash data slash users.js instead of writing queries to the database manually the demo app uses the sqlize orm library to do all the database interaction it provides an abstraction that lets you treat querying the database like you're accessing functions on lines one and two we import the user and user favorites objects from sqlize these are currently pointed to a mock so let's update the reference for user change it to require and then in the parentheses do dot slash lib slash models slash user and then for user favorites change it to require and then dot slash lib slash models slash user favorites the next change is to modify the actual connection to the database so open the file lib slash data slash lib slash mysql.client.js the only change necessary here is to change the value of host to the actual hostname of the RDS database to get this value let's head over to the AWS Management console and the RDS dashboard on the left hand menu select databases and then select the database and since we just created now because it takes a while it's possible that this instance will still be in the process of creating but as usual I've skipped my video in time a few minutes here and you can see that it's already finished you may have to pause the video and come back when it's finished creating scroll down to the endpoint value and copy it now back in your code editor paste this value inside the string next to host and that's it for the changes required for creating and using an RDS database in the next clip we're going to configure a redis cluster in elastic cache elasticache provides managed caching services that are particularly useful to web applications there are two open source cache engines redis and memcache D that can be used with elasticache but over time it really seems that redis has come out as the clear better solution here we're going to create a redis cache cluster and then use it to Cache the user sessions in the demo app ensuring that our application can scale to a whole lot of users without impacting application performance in the scripts 06 folder open the create rediscluster.js file the only function we need to implement here is the create redis cluster function the only arguments it takes is the name for the cluster and an ID of a security group for the cluster we'll start by creating a params const object the first property is cache cluster ID which is basically the unique identifier for the cluster we'll use the cluster name argument as the value here for the next property add cache node type this is like the ec2 instance type we'll use the smallest available for our purposes add the string cache.t2.micro next add the property engine and give it the value of redis as a string this just tells elasticash what type of cash that we want to create the next property is num cache nodes which says how many nodes or instances that you want to give the cluster for redis the value must be one so just enter that here then add the properties Security Group IDs and set the value to an array with one item named sgid and the next line create a new const called command assigned to it a new Constructor call of create cache cluster command passing in the params in the next line use the return keyword and call send elasticache command passing in the command now we finish the script so we can run it by switching to your command line and then executing the command node create rediscluster.js once it's complete your redis cluster will be booting up and just like RDS this can take some time once it's finished we'll need to get the host of the redis cache to be able to add things to our code but there's some things that we can go ahead and do I've already written most of the code to use the redis cache in the application so really you just need to make a few minor changes in index.js at the root of the project there are a couple of sections that we need to uncomment this section at the top inside of options is the first so uncomment all of that this will configure our happy web server to use a redis plugin and make a connection to it to use as a cache then scroll down to where we Define this cache const and uncomment the line with the property cache this will set the redis cache as the default cache to use for user sessions before it was just using memory now go back to that top section of code that we uncommented and here we need to add the host of the redis cash cluster that we created so because the cache creation can take a while I'm going to skip my video ahead a few minutes here and the cache is now available so in your AWS Management console go to the elasticash dashboard and click on the redis menu on the left hand side menu expand the cluster that we created and copy the primary endpoint field excluding the colon and the port at the end now back in your code editor paste the value inside the quotes for that host property and the options object now you can save that file and the demo app is all ready to use now it would be really great to be able to just try out all of this stuff locally in fact the dynamodb and RDS stuff can be run locally but unfortunately we can't easily access a redis cluster running in elastic cache outside of the VPC we could configure a jump box or a Bastion host to access it but it's just a little overkill for this course instead we're just going to update the code deploy what we've got locally to our instance in AWS and see it work remotely in your command line navigate to the root of the project I've created a deploy script that will push your code out to your instance and get it running all you need to provide is an IP so go to the ec2 dashboard and the auto scaling group screen select our Auto scaling group and then the instances tab you should see just one instance so select it now grab the public IP we're going to use this in our command line so switch back to it run the command npm run deploy Dash Dash and then the IP if you've followed the video exactly up to this point you should be able to just run this command and not have any problems if you made any modifications you may need to update some of the values and deploy.js now on running this command it will zip up all your code upload it to the instance extract the zip file install the dependencies terminate any node applications that are running and then restart the application you should be able to try it out then in your browser using the load balancer DNS name in the next clip we're going to look at limits you may run into in working with any of these persistent services expect dynamodb RDS and elasticache have some more significant limits than the other services persistence is a much more touchy subject and requires both heightened security and a more strict environment in order to achieve high performance let's take a look at some of the limits and restrictions Dynamo has several restrictions around secondary indexes you can only create up to five Global secondary indexes and five local secondary indexes per table those local secondary indexes must be created at the same time as the table and cannot be deleted after global secondary indexes don't have this restriction finally when creating a table tables with secondary indexes must be created consecutively more than one cannot be created at the same time RDS has limits around numbers of databases and size of the databases which is in the terabytes but nothing you wouldn't expect there are some restrictions about database and usernaming but they differ with each database engine so you'd need to research those yourself elastic cache similar to RDS has a limit around the number of clusters and nodes these are soft limits and can be raised with a request to AWS the one restriction to remember is that you can't directly access elastic cache from outside of the VPC it's created in beyond that there's no other restrictions that need mentioning with so many different options for persistence in AWS hopefully this module has been enlightening and will help you to make the best choice for your use case let's recap what we covered in this module we started out by going over some fundamental dynamodb topics around provision throughput and secondary indexes then we created a dynamodb table with the SDK and populated it with hamsters and races next we learn how to retrieve data from the table with queries and scans then we moved on to RDS and created a mySQL database for our user related data next we created a redis cluster in elasticache and used it for our user session State and finally we took a look at the limits and restrictions around each of these services in the next module we're going to look at how to configure different types of external routing using AWS services like Route 53 and API Gateway hi there and welcome back to AWS developer designing and developing we've been doing all sorts of cool things with AWS in this course but what good would any of it be if users couldn't access our hard work that's where Route 53 and API Gateway come in both provide external internet access to your internal AWS resources Route 53 is direct DNS routing whereas API Gateway provides an abstraction over routing and is based around rest API design we'll use both to Route our demo project in different ways so let's look at some of the topics we'll cover in this module we'll start by discussing the types of routing policies available in route 53. then we'll see how to configure a hosted Zone and record set in Route 53 to support a domain name next I'll explain what API Gateway is and how it works then we'll go about creating configuring and deploying an API Gateway finally we'll end the module by discussing some of the many limits with Route 53 and API Gateway let's get started while Route 53 is often used as simple DNS rules to route to AWS resources there's more advanced functionality available in regards to how the routing takes place routing policies allow you to define the behavior a routing rule follows once a request has come in there are five types of routing policies that we'll discuss here simple weighted latency geolocation and failover a simple rounding policy is when you want to map a single DNS record to a single AWS resource for instance sending the domain name hbfl.online to a single load balancer a weighted routing policy lets you configure multiple resources for a single DNS record and gives each of the resources a weight Route 53 will route requests to each resource based on its percentage of the total weight for instance if you configure two Resources with a weighted routing policy one with a weight of one and the other with a weight of two it would route requests to the First Resource a third of the time and Route requests to the other resource the other two-thirds of the time a latency routing policy is really useful to reduce the latency for your users with your application it allows you to configure multiple resources and Define regions for each you'll configure the region and the record set to match the region of the resource then when a user requests a domain Route 53 will determine which request will have the lowest latency based on the available regions that you configured and the IP of the request this is done automatically so it's a pretty convenient way of dramatically reducing latency a geolocation routing policy is similar to latency routing policies where latency automatically routes based on lowest latency geolocation lets you configure which requests will go to which resources based on continent country or subdivision which is a state or a province this gives you tremendous control over who goes to what resource and is likely very useful for legal or compliance reasons finally there's failover a failover routing policy allows you to configure two resources one as primary and the other as secondary if the health of the primary resource fails then requests will be routed to the secondary resource this provides some stability for domain records when a load balancer won't work between all of these routing policies you should be able to find one for whatever need you have in the next clip we're going to configure Route 53 for a domain name Route 53 has two key Concepts to know when setting it up to support a domain name the first is the hosted Zone this is basically the domain name itself without subdomains for instance example.com or hbfl.online the other concept is the record set this is basically a DNS rule which configures an AWS Resource as the destination of a request each hosted Zone will have many record sets in it in our case we're going to create a hosted zone for our demo project and then configure a route set to send requests to our load balancer one note before we start unless you purchase a domain name from a registrar and add the name servers from your hosted Zone you won't be able to actually test the changes we'll do in this video so if you do want to follow and test it purchase a domain name and use that instead of the placeholder here now switch over to your code editor and open the file in the scripts 07 directory named create Route 53 hostedzone.js this file is simple with just one method that we need to implement for this script we'll be using the Route 53 client and as usual I've already implemented the send command method in the helpers file let's implement the create hosted Zone method start by declaring a const named params there are only two properties that will be needed the first is the name property which you can give the value of the HZ name argument above we're passing this in as hbfl.online if you wanted to use your own domain name just update the string for the HZ name variable the next property is caller reference this is a unique value that differentiates calls to create hosted zones it's required must be a string and the easiest thing to do is to just set it as a timestamp for the value here create a string template with back ticks then evaluate a call to date dot now by surrounding it in the dollar sign curly brace syntax that's just a quick way to get a time stamp as a string and again this is to avoid duplicate hosted zones being created and that's all we'll need for properties and the next line create a new const called command assigned to it a new Constructor call of create hosted Zone command passing in the params variable then after that add the return keyword and call send command passing in command now save the file and switch over to your command line and navigate to these scripts 07 directory execute the command node create Route 53 hosted Zone dot Js once it's complete you should see the Json output for the requests we need to copy the ID field under hosted Zone because we'll need it for our next clip where we'll create a record set for this hosted zone so go ahead and select that and copy it now that we have a hosted Zone created we can add a record set and your code editor open the file create Route 53 recordset.js this is another fairly simple file with only one command to send but the properties for the record set itself will be quite numerous so let's start by replacing the string contents of the hzid variable with the hosted Zone ID that you copied from the command line output in the previous video it's important that this value is correct since the route set has to be applied to the right hosted zone for it to work now we can implement the create record set function create a new const named params and assign to it an object make sure not to delete this comment because we're going to need that later to look up a region code the first property for the params object will be hosted Zone ID which you can give the value of the hzid e argument the next property is change batch the command we'll be using is actually a change command which can be used to create modify or delete record sets for a hosted Zone it's a different pattern than we've used before and Route 53 is pretty unique in this way give the property the value of an object the object will have one property called changes which you can give the value of an array with a single request you could create or modify multiple record sets by adding different objects to this array for our case we're only going to need a single object so add that here the first property for this change object is action which we will assign the value of a string with create in all caps the next property is resource record set which has the value of an object now finally this is the object where we will get to the nitty-gritty details of this change the first property is name and this is the DNS name for the record you could enter a subdomain here something like demo.hbfl.online or www.hbfl.online but I'm just going to enter the string hbfl.online so I can create this record for the root domain the next property is type and this is the type of DNS record to configure we're going to do an a record so you can enter the uppercase letter A here in quotes Route 53 allows you to do basic DNS routing to IPS or more advanced Alias routing which is what we want Alias routing lets you just point to an AWS resource and Route 53 will figure out all the IP and routing stuff add the property Alias Target and give it the value of an object the first property is DNS name and this is the internal AWS DNS name in our case we're going to send it to the load balancer that we've created so let's go to the ec2 dashboard select the load balancers menu option and then the load balancer that you created copy the DNS name from the details panel now paste that DNS name as a string as a value to the DNS name property the next property is evaluate Target Health which Route 53 can do to determine whether to Route requests or not we don't really need that so we can just set that property to false the last property is the hosted Zone ID although they are called the same thing don't get this confused with the hosted Zone ID at the root of our params object this is actually the hosted Zone ID of the elastic load balancer and we can use the link that I put in the comments to find that copy the URL I put in the comments and go to it in your browser this page has all the hosted zones for any region in AWS for load balancers look for the region that you're deploying to and then look in the column that has application load balancers in it they have created multiple hosted zones for different types of load balancers now so just make sure you're picking the one with application load balancers then copy the ID that's in that cell back in your code editor paste that value as a string as a value for hosted Zone ID and that is going to be it for the params object and the next line you can create a new const called command and assigned to it a new Constructor call of change resource record set command passing in the params object I think that might be the longest command name we've used so far then on the line after that you can just return a call to send command passing in command now switch over to your command line and we'll send this record set creation request by executing node create Route 53 record set dot Js once it's complete you should get some Json output like this showing the status as pending a quick note here because these are actual DNS changes they can take some time to propagate it's not instant like some of the other changes we've done if we switch over to the AWS Management console and go to the Route 53 dashboard you'll see that we have our hosted Zone here and clicking into it the record set we just created is also here even though it shows here it doesn't mean that the change has propagated live so even if you're using a real domain name this a record may have not propagated out into the public internet yet and that's all for Route 53 and the rest of this module we're going to look at a different way of routing users to your application with API Gateway API Gateway is one of the newer AWS services and it reflects an evolution of routing with Cloud resources while there is API directly in the name it is by no means only for use with apis it actually really works well for all types of routing so what exactly is API Gateway it's essentially a routing layer that allows you to configure many rules on the same domain and connect it to many different resources the language of API Gateway is couched in classical rest terms each part of a URI path is called a resource and on those resources you configure methods each method can be configured to have a certain behavior and send the request to some AWS service you can transform the request before sending it on and also transform the response as well each method on a resource is configured separately so you could have different operations on a path going to different AWS resources it's really flexible and Powerful in this module we're going to set our demo project up to respond from an API Gateway resource I'm using this example because you could then see how you could route other paths on that same domain name to other applications now let's get started with creating our API in the next clip there are quite a few parts to actually creating and configuring an API and API Gateway each part serves its purpose but it can be a little complicated when you're just getting started so I'm going to walk you through it in our script files let's get started by opening the createapigateway.js file you may notice right off the bat that the execute function in the script is really large this is where the API is created and configured I'll explain each step in just a second an API and API Gateway is structured like this you have your API the API has different resources which are organized sort of in a tree structure with parents and children then each resource can have one or many methods a method has two main parts to it the integration request and integration response where you can manipulate the data going either way keep all these pieces in mind because those are what we'll be configuring with this file so let's look back at our code here we're creating the root rest API we save the response of that creation to a variable so we can use the data from the root API later in the code anytime you want to create a resource and an API you have to give it a parent resource to Branch from in this case we're using the top level resource the root API to Branch from then we create a resource called hbfl this will be what our demo app responds on we have to then create the method and integration for that resource then we create one more resource which is actually a proxy resource it's basically a catch-all for any request Beyond hbfl it'll be used for things like API calls to our demo project once that proxy resource is created we configure a method and integration for it as well we're going to need to implement all the functions called in this execute function so we'll be working in this file over the course of the next few videos let's start by implementing the create rest API function first declare a params const object there's only one property needed here with the key of name one thing to note with all of these API Gateway commands is that the property keys are in camel case not Pascal case like every other AWS service I'm not sure why they made an exception for this service but in a different version 2 of the API Gateway client they've reverted to Pascal case so it seems like this might have just been a whoopsie give that property the value of the API name argument then after the params object create a new const called command assigned to it a new Constructor call of create rest API command passing in params then on the next line return a call to send command passing in command and that's it for create rest API the next function we'll Implement is get root resource which will get the root resource ID from the API we just created the create rest API command does return the ID of the API but not the ID of the root resource which is needed to add other resources to that's why we need this function start by creating a params const object this is another simple object with only one property with the key rest API ID the value will be the API dot ID property from the API function argument in the line after the params object create a new const called command assigned to it a new Constructor call of get resources command passing in the params variable then after that we want to manipulate the response before returning from this function so create another const named response and then assign to it an awaiting call to send command passing in the command in the next line create a new const called root resource we will go through the resources returned from our Command call to find the resource that only has a slash as the path this will identify that Resource as the root resource after an equal sign call the function response.items.find which will look for an item matching some criteria from response.items inside the parentheses add an arrow function with one argument named r on the right side of the arrow enter r dot path triple equals single quote slash single quote this will return the single root resource then in the next line add the return keyword and return root resource dot ID now that we have that root resource ID we can create a new resource off of it by implementing the create resource function start by creating the usual params const object this one will have a few more properties than the other ones the first property will be parent ID give it the value of the parent resource ID function argument the next property is path part and you can give it the value of the resource path function argument this is the path that the resource will be configured to respond to on the final API URL scheme the last property will be rest API ID which you can give the value of API dot ID and that's it for the params object on the next line add a new cons named command assigned to it a new Constructor call of create resource command passing in the params object here's another response we'll need to manipulate so on the next line create a new const named response after an equal sign add an awaiting call to send command passing in command then in the next line add return and then response dot ID we just need to return that ID property from the response and that's everything needed for the create resource function we've implemented the functions to create an API retrieve a root resource and create a resource but there's still more to do in the next clip we're going to create a method an integration for a resource methods in API Gateway are where you define how a user will actually interact with your endpoint each method corresponds with a single HTTP method like get or post and different Integrations are added to transform the request or response let's get started by implementing the create resource method function in our createapigateway.js file start by declaring a params const object on the first line the first property is authorization type with API Gateway methods you're able to create authorizations by using IAM or even your own function we won't be securing our endpoint we want it open to the public so enter the string none in all caps the next property is HTTP method which will give the value of the method function argument then add the resource ID property which you can give the value of the resource ID function argument this is how to tie this method to a resource the last property is rest API ID which you can give the value of API dot ID and that's it for the params object in the next line create a new const named command assigned to it a new Constructor call of put method command passing in params then we can just return a call to send command passing in command we won't actually use the response from the command but there's no harm in returning it and that's it for the create resource method function as you can see there's not much to creating a method either the interesting stuff with API Gateway comes into play with the Integrations let's go ahead and move into implementing the create method integration function start by creating a params const object this one will have the most properties in this file add the first property named HTTP method to which you can give the value of the method function argument the next property is resource ID which you can give the value of the resource ID function argument both HTTP method and resource ID together tell this function which resource method to attach the integration to the next property is rest API ID which you give the value API dot ID then add the property integration HTTP method which is the method used to call the back end AWS resource you can give it the same method function argument next is the type property there are several different types of Integrations like sending it to a Lambda function or using it as a mock or sending it to an AWS resource we will use the HTTP proxy type when sending requests over to our load balancer so add that as the value in all caps with an underscore between the words and the final property is URI which we will give the URL to our load balanced demo application so switch over to the ec2 dashboard and go to the load balancers menu option and get the DNS name from our load balancer in the value for URI the URL does need to include a protocol so in a string add HTTP colon slash slash and then you can paste the load balancer DNS name and those are all the properties needed and the next line create a new command const assigned to it a new Constructor call of put integration command passing in params then in the next slide just return a call to send command passing in command now our basic method and integration creation functions are complete which will let us create a basic resource we do however want to also create a pure proxy resource that will handle any requests coming on the hbfl resource beyond the root there will be some additional code for this and that's what we're going to add in the next clip a normal API Gateway integration to an HTTP endpoint strips away all of the original headers and only sends special ones from the integration sometimes this might be what you want but other times you want to preserve the initial request to API Gateway and just pass it completely over to your HTTP resource on the other side this operation is called a proxy in our case we have several endpoints on our app that serve as API type endpoints like getting the leaderboards or posting a change in user favorites we don't want to have to configure a new API Gateway endpoint for every single one of these so we'll create a catch-all proxy resource with the parent being the hbfl resource so if we call something like hbfl leaderboards it will be handled by this proxy method in API Gateway and sent on to the load balancer if we look at our code and createapigateway.js where the proxy resource is declared we first create the proxy resource by using our create resource function but passing in this very specific path part this string which contains proxy plus and curly braces tells API Gateway to execute the resource with the rest of the path whether it's nested or not whatever it is then we use the same method and integration functions that we used before but we add an additional proxy string as the last argument you'll also notice the HTTP method we're using is any which isn't actually an HTTP method but it tells API Gateway to handle all possible HTTP methods so because we're using the same functions that we've already implemented we'll just add some functionality to the create resource method and create method integration functions let's start with create resource method after the params const object declaration add an if and check for the existence of the path parameter and then create a code block after that if the path argument exists we want to configure it as a request parameter on our params object so add params.request parameters equals and assign an object the object will have one property but it's going to be dynamic so insert square brackets and then back text to create a string template inside enter method dot request dot path dot dollar sign open curly brace path close curly brace to the value of this property add true now what this is doing is telling our resource method that we're going to have a parameter on the path of the URI the value of which is whatever is passed into the function in the path argument for instance in our case it's going to be proxy this is basically how you define parameters on your path for a resource method and this is similar to how you would add query parameters if you were going to use them now we can add the new Behavior to the create method integration function prams const object declaration check if path is truthy and then create a block just like we did in create resource method the first thing we'll modify is adding the path parameter to the URI property on params so enter params.uri then plus equals to add and assign to it then back ticks to create a string template inside the back ticks enter slash open curly brace dollar sign open curly brace path close curly brace close curly brace mind the multiple curly braces there this will take our path function argument and add it with curly braces around it to the end of our URI on the next line enter params.request parameters assigning to it an object this is where we tie our integration request parameter which will be going to the load balancer endpoint to the method request parameter similar to the resource method change we made we'll have a Dynamic Property key here add square brackets and then a string template enter integration dot request dot path dot dollar sign open curly brace path close curly brace then give that property the value of a string template and inside it add method dot request dot path dot dollar sign open curly brace path close curly brace and that will configure the path parameters for proxing and I'm sure you're done listening to me read out codes like that now we've got this entire file complete and boy does it do a lot go ahead and save it and switch over to command line we're going to be able to run it now which will be awesome run the command node create API gateway.js it won't take very long before it's complete and it'll output API creation complete if it succeeds with no errors now since everything is created like this we can deploy the rest API in the next clip and then we can actually try it out in our browser deployment and API Gateway is kind of unique there are two key Concepts stages and deployments API Gateway lets you create stages for your apis think of stages like development integration staging and production you can deploy your API to different stages to have a healthy development life cycle each time you deploy to a stage a deployment is created these are saved in the history of each stage so you can move between them easily especially in cases where you might need to roll back to make our rest API accessible we need to deploy it so open up your code editor and open the file deploy API gateway.js first we need to paste the ID for your API in this API ID variable to get this go to the AWS Management console and then the API Gateway dashboard you should see your API titled hamster API in this table there's an ID column with the API ID so just copy that and then back in your code paste it inside the string for the value of API ID deploying our API is as easy as just creating a deployment so let's implement the create deployment function create a new params const object We'll add the first property which is rest API ID which you can give the value of the API ID function argument next is the stage name property which you can give the value of the stage name function argument this is where you could differentiate between stages like prod staging or Dev or however you like to differentiate your environments in the next line after the params object create a new const called command assigned to it a new Constructor call of create deployment command passing in params then in the line after that return a call to send command passing in the command now even though this is a create function you can actually call it every time you make a change to your API every time you need to deploy it each time the function is called API Gateway will take the current state of your API resource and deploy it to that stage simple as that now let's move over to our command line and deploy our API by running the command node deploy API gateway.js after it finishes it'll output some response data to get the URL for the deployment let's go back to our web browser and select that hamster API option in the API Gateway table under hamster API on the left hand side menu now click on stages and you should see a prod stage expand that and you'll see the resources and methods that are deployed you can see the any method for our proxy was actually turned into all the different HTTP methods if you click on the get under the hbfl resource you'll see a prominently displayed link here which is where the API Gateway was deployed we need to manually modify this link just a bit so copy the link and then paste it in a new browser window our demo app needs to be accessed with a trailing slash so add that on the end of the URL and then you can hit enter now we're seeing the demo project routing from API Gateway to the load balancer to an ec2 instance in the next clip we'll look at limits and restrictions you might find when working with Route 53 and API Gateway Route 53 and API Gateway are both constrained by restrictions and limits but they come from different concerns Route 53 has many restrictions due to the nature of DNS changes whereas API Gateway has many restrictions due to it being a new service in AWS let's look at a few of both in Route 53 you'll notice quickly that there is a restriction on how fast record set changes are propagated to the internet and this is based on DNS rules usually I found that if you change your record set the change can be observed almost instantly if you make an error for some reason though and need to change that same record set again the change will be delayed by around five minutes I've encountered this several times so make sure to be careful when changing record sets aside from that there are the normal resource limits but they can all be raised by AWS API Gateway has limits around the number of requests you can make to create modify or delete something in your account these are hard limits and say for instance that you can only send two create API requests per minute all the rest are blocked and then you just have to wait it's kind of inconvenient there are also many known issues with API Gateway that restrict certain types of operations AWS maintains a page with these found here and it's a good idea to be aware of some of the issues it currently faces I ran into some in the past around the way the API Gateway resolves requests and spent a few months figuring it out beyond that there are the resource limits as usual and those can be raised in this module we looked at two different ways that you can get your web application to your users Route 53 follows the classic DNS model but has some tricks up its sleeves an API Gateway represents a new way of abstracting routing behind a URI unless you get creative with how a domain routes requests let's recap what we covered in this module we started by discussing the different types of Route 53 routing policies that lets you control where requests Route 2 based on waiting lowest latency geolocation of requests and more then we created a Route 53 hosting Zone and record set to route to our load balancer then I introduced API Gateway and explained how it was structured after that we created all the pieces to make a rest API and API Gateway and then we deployed it finally we looked at limits and restrictions with Route 53 and API Gateway in the next module we are going to take a look at other ways to get content to our users by looking at caching static assets and cloud formation and serving up static sites with S3 hello and welcome back to AWS developer designing and developing in this module we're going to look at two different ways that AWS empowers you to deliver your content to users the first is cloudfront a global content delivery Network that is essential for providing fast delivery of static files the second is a feature in S3 and that's S3 static websites let's take a look at what we'll be covering in this module we'll start by discussing the specifics of invalidations with cloudfront then we'll create a cloudfront distribution using our existing S3 content next I'll explain how static sites work in S3 and then we'll configure one based on our existing S3 bucket and finally we'll look at the limits and restrictions with cloudfront and S3 static websites content coming from cloudfront is typically best when being served from the cloudfront cache and not from your origin getting this right will dramatically reduce latency to your user but when configuring cache you'll set a Max time to live before the cache retrieves a new version of the content from the origin set the value too high and when you release new code then users are still getting the cached version set it to low and you're not taking advantage of what cloudfront can offer there are two real solutions to make the most out of a high time to live value the first is to invalidate content when you've got a new version available this can be done by sending an invalidation to cloudfront defining which files to replace with new versions a single invalidation can have multiple files to invalidate even referencing entire directories but there are a couple of downsides to using invalidations exclusively for handling new content the first is that there's a hard limit on the number of simultaneous invalidations you can process while the number is high at 3 000 objects this essentially places an upper bound on the growth of your application in addition you're only allowed 1 000 free invalidations per month anything above that has a price associated with it and to top all this off invalidations can take some time they aren't instant while I've often experienced it takes only a few minutes to process an invalidation that was with only a few files I can imagine it would take longer with more files the second solution which would work better for many applications using cloudfront is to treat your assets as immutable and never change them this way they can be cached the maximum time and never need a manual invalidation the easiest way to achieve this is to generate a unique name for the object any time it changes this might be something as simple as adding a hash to the file name or a date time stamp then when you push a new version of the application it only references the new files the first time they're requested cloudfront retrieves them from the origin and then caches them for the maximum time to live this has been My Chosen solution when working with cloudfront since it has the side benefit of solving client caching issues as well in the next clip we're going to actually create a cloudfront distribution for our static assets we've already changed our code to directly access Assets in S3 so let's create a cloudfront distribution based off that bucket so that we can get even better performance from it cloudfront SDK commands might hold the award for most properties in a params object I seriously made us a completely separate file just because there were so many navigate to the scripts 08 folder and open the cloudfront parameters.js file I moved the two biggest sections of params into this file so that the actual create file will be much easier to read we've got two sections here the origin and default cache Behavior properties let's get started with adding the origin properties this Origins object can hold one or many Origins so there is a quantity property We'll add first this tells AWS how many Origins will be in the items array we're only creating one here so enter the number one the next property is items and it has the value of an array this is where the origin details will go create one object here the first property for the object is domain name this will be the domain for your S3 bucket we'll build it dynamically so create a string template then create a variable and add the bucket name function argument after that continue with DOT S3 dot Amazon aws.com and with that we'll build the domain name dynamically based on whatever your bucket name is the next property is ID this is going to be a unique value that will be referenced in the other params chunk default cache Behavior for this value create another string template and again reference a variable with the bucket name function argument after add an underscore in the word origin this will make an ID based on your bucket name so it should be unique enough for our purposes the next property is S3 origin config which has the value of an object this property is unused by us but it's required by cloudfront for an S3 origin to work give it one property origin access identity with the value of an empty string like I said it's unused now that's everything we need to configure for our origin let's configure the default cache Behavior below to use that origin these default cache Behavior properties do exactly as they say they configure the default behavior that our created distribution will follow the first property is forwarded values which has the value of an object this will tell our distribution to forward on cookies or query parameters to the origin in our case we don't want any of this Behavior add the property cookies with the value of an object inside give it one property called forward and the value of none as a string after the cookies property add a property named query string and give it the value of false now after the forwarded values object add a new property called Min TTL and set it to zero this is the minimum time to keep an object in cash before forwarding the request back to the origin the next property is Target origin ID which needs to match the origin ID we defined above the easiest thing is to just copy the value from above and paste it here that way you won't get any typos next add the property trusted signers which has the value of an object this will let you create signed URLs for cloudfront content and only allow users to access content that way we won't be doing that but this is another required field add the property quantity with the number zero then the property enabled with the value false after trusted signers add a property named viewer protocol policy this is where you can Define https only redirect HTTP to https or both I like to redirect HTTP calls to https so add the string redirect Dash 2 Dash https next add the property allowed methods with the value of an object here we'll Define what HTTP methods can be used with this distribution we'll have two so add a property named quantity with the value 2. then add a property called items with an array as the value we'll add two methods here get in all caps and head in all caps after the items property add another property called cached methods with the value as an object this defines the HTTP methods that cloudfront will cache add the property quantity with the value 2. then add the property items with the value of an array the items here will copy what's above with get and head entered here and with that our long properties are complete now we can open create cloudfront distribution.js because we're setting up the cloudfront distribution in front of our S3 bucket you need to populate the value of bucket name with the unique S3 bucket name that you created previously in this course now let's implement the create distribution function start by creating our params const object even though I've offloaded many properties to that other file we've still got quite a few to add here so start by adding a distribution config property and giving it the value of an object the first property inside this will be caller reference which you may remember from the Route 53 hosted Zone this is a unique identifier for the SDK command that we're making add a string template with a variable and the expression date dot now to get a timestamp for this value the next property is comment which is sort of like a description or name for your distribution it'll be visible on the cloudfront dashboard to help you differentiate it from other distributions so name it something that will help you distinguish the distribution here I'm going to put hbfl distribution the next property is default cache Behavior this is what we set up in the other file so for the value called default cache behavior and pass in the bucket name function argument the next property is Origins which is the other set of properties that we worked on so set the value to a call to Origins and pass in the bucket name as well the next property is HTTP version which I like to set to the string HTTP 2 to make sure that it's future facing the next property is price class basically the pricing is set according to how many Edge locations that you choose add the value price class underscore 100 which is the cheapest and it only uses Edge locations in the US Canada and Europe the next property is is IPv6 enabled which I also set to true to be future facing we don't really need it right now but it's good to have and finally the last property is the most important it's enabled and you want it to be set to true if you want to be able to use this distribution at all if it's set to disabled it just won't work and that's it for the params object on the next line create a new const called command and assigned to it a new Constructor call of create distribution command passing in the params on the line after that return a call to send command passing in command and that's it for this function and file now we can create this script to create our distribution so switch over to your command line and navigate to the scripts 08 folder execute the command node create cloudfront distribution.js and then wait for it to finish cloudfront distributions can take some time to set up after we get this response back from the command line so in the meantime we can go and we can modify our demo project first we need to get the new domain name for our cloudfront distribution which is available even though the distribution is still deploying so go ahead and open the AWS Management console and go to the cloudfront dashboard find the distribution by either the comment or the origin and then copy the domain name from the table now just like with S3 there are only a few places we need to update static references in our code the first is in util slash assets.js so go to that file and your code editor copy the value into the base const on line one you will need to add the https protocol to The Domain also so make sure it looks like this now one more place go to handlers slash template slash index.html and then on line 5 update the URL and take out any bucket that's on the path just leave the file name at the end do the same on line 7 with the favicon and then also on line 12 with the JavaScript import now to fully test this out you need to follow all of these steps because this video is already long enough I'm not going to be walking you through them you'll just need to follow them on your own so you'll need to rebuild the project locally deploy the code out to your ec2 instance upload the files to S3 again repopulate the Dynamo table and finally wait for the cloudfront distribution to finish getting set up you can then test out the changes with the load balancer URL or you could use the fancy API Gateway URL we did in the last module a lot of steps but our application is becoming really robust it's using a ton of AWS services and the speed of the application is getting really fast so for the rest of the module we're going to take a look at how static websites work with S3 static website hosting with S3 is one of aws's oldest Solutions launching shortly after S3 itself and while AWS has since released other services that provide similar functionality S3 static hosting still continues to be one of the easiest to configure and work with let's take a look at how it works first let's look at the difference between a website and an S3 bucket with an S3 bucket you use keys to get back objects every object must be referenced directly and only that object is returned it's just like getting static assets a website is different because you can reference a path and some HTML will be returned to you you don't have to reference the file where the HTML is explicitly in many ways that's what S3 static hosting is really providing it's letting you configure some static Behavior to occur when a directory is referenced in the URL with S3 static hosting you configure what the name of your index file will be and what the name of your error file will be the defaults are index.html and error HTML then you can place these files in any directory in your S3 bucket and they will be returned when that path is navigated to in addition S3 static website hosting gives you a unique URL to use to get to the site route there isn't too much complexity involved but it provides a simple solution when you need a simple site in the next clip we're going to get a static site set up using our existing S3 bucket in order for us to get to try out a static S3 site I've made a simple splash page for hamster ball fantasy league the files are already in your S3 bucket with the index.html file and a style sheet in the static folder we'll configure our existing S3 bucket to serve a static website and then that content will get served in your code editor open the file configure s3website.js the first change is to update the name of your S3 bucket for the value of the bucket name variable now we can implement the configure S3 site function by first creating a params const object the first property is bucket which will give the value of our bucket name function argument and then the next property is website configuration which will need the value of an object this is where we're going to tell S3 how we want the site to operate there are really only two options for properties that you could use here index document or error document we could add error document and provide an error.html file but I didn't bother making one so we'll just leave that out for now go ahead and add the property index document and give it an object as a value this tells the site what file to use as the index in the object add the property suffix and give it the value of index.html as a string you could put something else here if you wanted say a file like static.html to load as the index of the site we just already have index.html in there now index.html is actually the default for this property so we didn't necessarily have to add the property I just wanted to make it explicit how this was working you know kind of a learning experience for you so that's it for the prams object and the next line create a new const called command and assign to it a new Constructor call of put bucket website command passing in the params variable then on the line after that just return a call to send command passing in command and that's it it's pretty simple to configure an S3 site now switch over to your command line and execute node configure S3 website.js once it finishes loading unfortunately it doesn't actually give us any useful output such as the domain name that we would want to use to see the site so switch over to the AWS Management console and go to the S3 dashboard click into your hamster bucket and then go to the properties tab at the bottom of this page is a section for static website hosting this hyperlink at the bottom is clickable and if you visit it you'll be taken to our static site being hosted with S3 static website hosting again this isn't the full hbfl site it's just a static file to demonstrate these S3 static sites in the next clip we're going to look at some limits and restrictions with cloudfront and S3 static website hosting While most AWS Services have limits to be concerned with S3 static websites have none the only limits they have is in relation to normal S3 limits cloudfront however does have a few limits to be aware of most of them are just soft limits on the number of resources but there are some limits on how you configure your distributions you can only have a single SSL certificate on a distribution so you'll have to expect multiple distributions in your design if you require multiple certs there is no limit on the number of objects that cloudfront can serve and only a soft limit on the number of distributions there are limits within validations that I mentioned earlier and those are not soft limits so do be very cautious with that but other than those few caveats cloudfront can likely handle whatever you throw at it in this module we looked at a few different ways to deliver content to your users cloudfront Finds Its way into most Web projects but Estuary static hosting is a little more of a special use tool these days let's recap what we covered in this module we started by discussing cloudfront invalidations and why you might not want to use them then we created a cloudfront distribution with all of those parameters next we moved on to S3 static websites and set one up based on our existing S3 bucket and finally we looked at the few limits concerning cloudfront in the next module we're going to look at inter-service communication inside AWS both simple queue service and Kinesis streams provide ways for services to decouple and communicate in an asynchronous manner hello there and welcome back to AWS developer designing and developing in this module we're going to look at two different ways to communicate between services or applications simple queue service or sqs is a way to send messages via a q system individual messages can be taken off the queue and processed by consumers Kinesis streams are a real-time stream based messaging service that works similar to sqs but with subtle differences that change the ideal use case we'll be looking at both in this module and configuring them to work with our application let's go over what we'll be covering in this module we'll start by looking at some of the specifics of polling and sqs and how messages work then we'll create an sqsq to send race results and look at how to consume and process those messages next I'll introduce you to Kinesis streams and we'll set one up for our application to send race results to the races table and finally we'll cap it off by looking at some of the restrictions and limits with both services sqs works on a process of clients sending messages to the queue and then consumers polling for messages on the Queue this is sort of like the life cycle of a message in sqs with sending messages the process is simple enough the messages are just sent and the client continues on but polling is a different story what happens when there are multiple consumers pulling a single queue what happens if a consumer pulls for messages but then can't process them sqs has solutions for these scenarios which is what we're going to talk about here when a message comes into the queue it may or may not be visible to Consumers immediately depending on the delay seconds attribute this delay seconds attribute will delay the visibility of a message in the queue for a given number of seconds when the message initially arrives in the queue once the message is visible and a consumer reads it another attribute comes into play the visibility timeout once a message is read it is considered in flight this means the message is still in the queue but it's not visible to Consumers until the visibility timeout runs its course the message will stay in the queue and be invisible if the visibility timeout runs out then the message becomes visible to Consumers again and they can then read it which starts the visibility timer all over again for that message because of this a consumer should delete a message once they finish processing the message the visibility timeout gives consumers time to process messages and then delete them here's a diagram showing the life of a message with both delay seconds and visibility timeout in this case the message isn't deleted and so it becomes visible again for consumers but in this next diagram the message is deleted In Time by its consumer this is the ideal life cycle for a message in sqs both the visibility timeout and delay seconds can have default values set on the cube but each message added to the queue can override those values and provide specific delay seconds when sending a message or specific visibility timeout seconds when consuming messages another default value you can set on your queue is how long consumers should wait when they're polling for messages this can be set up to 20 seconds if any messages are available or come available while the consumer is waiting the consumer will immediately return with the new messages this is called long polling and it can be useful because you are charged for sqs by each request made requests are either a request to read a message or to write a message so by using a balance of long polling and how frequently you read messages you should try to strike a good balance that makes it economically feasible to use sqs for messaging and also highly performant in the next clip we're going to create our sqsq for sending race results the first step in using a Q and sqs for communication between services and AWS is to actually create the queue let's start by opening the file in the scripts 09 directory named create sqsq.js since there's quite a bit of code required to work with cues I've actually done most of the work and will just be filling in certain parts of the code mostly properties in this file you'll only need to create the params object for the create queue function so go ahead and start by creating the params const object the first property is qname which we can give the value of the qname function argument the next property is attributes which has the value of an object most of the properties We'll add here are ones that I actually mentioned in the last clip the first property here is going to be delay seconds which is the default delay seconds for each message we can just set it to the value 0. remember an incoming message can override this if they want to the next property is message retention period which is how long the messages will stay in the queue if they're not read and deleted we'll set this to the number 345 600 which is the second equivalent to four days if you ever need that number next add the property visibility timeout which is how long messages will be invisible after being read we can set it to the value 30 for 30 seconds then the last property is receive message wait time seconds which will set the long polling time of a consumer we can set this to the value 0 and let consumers decide whether they want to Long Pole or not now the create script is ready so we can save that file and switch over to your command line and then navigate to the scripts 09 folder then you can execute node create sqsq.js once it's complete it'll output the URL of the queue which is very nice but we don't actually need to keep it for now now in the next clip we're going to configure our demo project to send messages to this queue our demo project has the ability to run a sort of simulation for the hamsters and their races a fantasy simulation since this is sort of a real-time operation I thought it was a good use of sqs normally you'd want to have another service on the other end consuming messages one to send one to consume but for our demo we'll have the same application send and receive messages let's start with the ascending and the lib data lib folder there is an sqs.js file this is a file that will send the results of each race simulation pushing a message to a queue requires you to have the full Q URL and I've abstracted this push function so that it only requires a queue name and then it will go and retrieve the queue URL before it starts pushing to it so the first step in this function is to get the Q URL so inside the push function create a new const and we'll name it Q params not regular params you can assign an object to that it'll only need one property queue name which you can give the value of the queue name function argument then after that create a new const named Q command assigned to it a new Constructor call of get queue URL command passing in the Q params object then in the next line create a new const called Q response and assigned to it an awaiting call of client.send passing in the queue command this is going to retrieve the URL for our queue based on the name next we will create a new const named synprms and assigned to an object this is going to store the properties for our send message command the first property is going to be message body which is the message that we want to actually send and put on the queue we will need to turn the message function argument into a string so call json.stringify and then pass in MSG message the next property is Q URL and you can give it the value of Q response dot Q URL then after the send params create a new const named send command and assign to it a new Constructor call of send message command passing in the send params variable then on the line after that return a call to client.send passing in send command and that's all we need to do to send a message in the params for our send message command you can optionally include things like delay seconds and visibility timeout if you want it to be different then the defaults that are on the cube and with that we're done making our changes for sending messages and the next clip will Implement cue polling for the demo project now that we're pushing race results messages up to sqs we need to pull for those messages and then update the data sources with those records we'll make our first changes in the lib data lib folder with the sqs.listener.js file this file will pull for sqs messages every five seconds if messages are received if there's something there then it will modify the master records in dynamodb if you remember the explanation of long polling this interval of five seconds when polling is likely too short for production applications since it will incur a fairly high cost with one instance it'll make around 500 000 requests a month so definitely do use a better value if working with real world applications this init function is what starts the interval timer for polling and it calls the poll function to actually pull the cube let's add the properties necessary by completing all the to Do's in this file the first step is to get the Q URL so let's add the single property for cube params the key will be queue name and you can give it the value of the global variable race underscore Q all in caps that's done so look further down for the receive params object and this is used to pull and receive messages from the queue the first argument is Q URL which you can set to the queue data dot Q URL value the next argument is Max number of messages this is a value between 1 and 10 and is basically the maximum number of messages that will be retrieved the default is one but I like to set it to 10 to get as many as possible in one request the last property is visibility timeout which I'm going to set to 15. this is just an example of overwriting the queue default there's not any reason to do it here I just wanted to give you an example of that now that's all the changes necessary for the poll function once we receive and process a message then we need to delete it so scroll down to the delete messages function we need to add properties to send to the delete message command the first property is Q URL which you can give the value of the Q URL function argument the next property is receipt handle this is basically a unique value provided each time a message is retrieved this is how the queue knows which actual message to delete we can give it the value of result dot receipt handle to reference the message that was received you may notice this function is called in a for Loop so each retrieved message will have this delete message command called on it there's also a delete message batch command in the SDK that could also have worked well here since it deletes many messages so you can take your pick with that change our listener is ready and we just need to enable all the functionality in the index.js file at our project root at the bottom of the file on line 87 there is a commented line that calls the Q init function this is going to turn the listener on and have it start pulling the queue so go ahead and just uncomment this line and save the file now our changes are done and you could try running them now deploying them up to ec2 but it'll probably be easier to wait until we finish the rest of the work on our Kinesis streams which we're going to do with the rest of this module sometimes sqs isn't good enough for your application use case you could want to avoid the polling process or you find that you want a more real-time solution there might be many reasons why sqs won't work but Kinesis streams might be perfect for you Kinesis streams as a service for real-time data transfer it scales easily and can have multiple producers and consumers it doesn't have fine-grained control over how messages are handled like with sqs but that Simplicity might make it better in some situations let's talk about one of the key Concepts in Kinesis streams and that is sharding sharding is the capacity unit used with Kinesis streams one Shard is equivalent to your Kinesis stream having one megabyte per second of input and 2 megabytes per second of output if you need more input or output you would add shards to your stream we're going to use a Kinesis stream to push our hamster race results similar to The Q but the consumer of the Kinesis stream will update the races instead of the hamsters in our case we're going to use a Lambda as the consumer one of the main reasons is that because a stream is real time it requires a dedicated consumer and our web application is not really the right fit lambdas are actually a great fit as Kinesis consumers so this will give you a taste of how that can work now in the next clip we're going to create our first Kinesis stream now that we understand what a Kinesis stream is and how it works let's go ahead and create our first stream open the create kinesastream.js file in the scripts 09 folder this is going to be another one of those very very simple files we just need to implement one function so let's Implement create Kinesis stream you can start by declaring a params const object and then the first property will be Shard count which is the number of shards that will be created in the Stream you can modify this value later on if the stream needs to increase its capacity but for now we'll just enter the value 1 here the next property is stream name and we can give that the value of the stream name function argument and and that's it for the prams object on the next line you can declare a new const called command and assigned to it a new Constructor call of create stream command and pass in the params variable then after that just return a call to send command and pass in that command variable and that's it for this script you're definitely getting the hang of this now let's create the stream Now by switching over to your command line execute the command node create kinesastream.js when it's complete the process will finish and it has no useful output for whatever reason in the next clip we're going to start sending race results to this Kinesis stream now that we've created a Kinesis stream we need to start sending records to it let's start by just taking a look at a file go to the lib simulation folder haven't been here before and open the resolution.js file this file is used once the simulation is complete and the records need to be sent to sqs and kinesis on lines eight and nine we Define variables for the stream name and partition key the partition key is a value that Kinesis hashes and uses to segment the data record to a specific chart even with A Single Shard which is what we have this is still a required field when pushing data to Kinesis these values will be used when we call the send function so let's Implement that function by opening the kinesis.js file in the lib data lib folder this is another very simple file that only performs one operation the only thing we need to do is to add properties to the params object in the send function as you can tell as time goes on I'm doing more and more code for you I'm sure you're getting tired of me repeating the same things over and over so the first property we're going to put here will be data which is what will actually be sending to the Kinesis stream this needs to be a buffer object so first call Buffer Dot from and then inside the parentheses for that we need to stringify the message so called json.stringify passing in the message function argument that will provide the Kinesis stream with the message in the format that it wants the next property is partition key which we can give the value of our partition function argument and then the final property is stream name which will have the value stream name it's pretty simple just point the data at a stream and put it up there we use the put record command to actually send that message and that's all there is to it now in the next clip we are going to set up our stream consumer for this course we're going to use a Lambda function as a convenient Kinesis stream consumer what's easy about this is that you can configure a Lambda function to be triggered directly from a stream meaning that you don't have to create some long running application to consume all of this Kinesis stream data Lambda functions are event driven so the function will be invoked anytime data comes into the Stream so we'll start by creating our Lambda function in the script so 9 folder open up the create kinesisconsumer.js file now to configure a Lambda function to connect to a Kinesis stream you will need the Arn for the Stream So to get this we're going to move over to the AWS Management console and go to the Kinesis dashboard click on the number in the data streams section and then click on the hamster race results option and then you can copy the Arn which is found here now switch back to your code editor and paste the Arn for the value of the Kinesis Arn variable now the rest of this file creates a role for your Lambda function with permissions to read from a Kinesis stream and then it zips up the code that will run on the Lambda function it creates the Lambda function and then it creates the trigger that connects the Lambda function and the Kinesis stream if you scroll to the bottom of the file you'll see the create trigger function this is the only other part of this file that you'll need to modify we just need to add the properties to the prams object that will be passed to the create Event Source mapping command the first property will be Event Source Arn which will give the value of the Kinesis Arn function argument this is what we needed that Arn from the streams dashboard for the next property is function name which is the name of the Lambda function give it the value of the Lambda name function argument the next property is starting position which tells the Lambda function at which record it should start processing from the stream you can just enter latest in all caps as a string since this connection will be set up before there are even any records in the Kinesis Stream So latest is fine finally add the property batch size and give it the value 100. this defines how many records will be retrieved at one time from the Kinesis stream and 100 is the max value here and that's it for code that we need to modify I do want to show you the code that will be running on the Lambda function so look in the Lambda Kinesis consumer folder and open the index.js file this file doesn't actually have any code that's interacting with Kinesis the Lambda will be invoked whenever the Kinesis stream has records but that data will come in as an array in the event.records property on line 12 you can see how to get the actual data from the Kinesis record that's there you don't have to go to Kinesis in any other way it will be a base64 encoded buffer so it does need to be processed correctly before you can really do anything with it and then the rest of the Lambda function just puts the Kinesis record data into the dynamodb races table now we can switch over to our command line and run the command node create kinesisconsumer.js this will take some time to run because the role creation isn't immediate so I'm speeding up the video here a bit and then you'll see the output for the trigger Creation in the console once created you don't need to do anything else with the Lambda function since it's live and it's connected as soon as it's created here's a good point where you can actually try everything out to test it you'll just have to redeploy your app to the ec2 instance use the same deployment script that we used earlier just make sure the IP address is still relevant Civil Right instance then when the code has been redeployed to run the simulation you'll log into hamsterball fantasy league using the username Ryan and the password pass and then you'll click the gear next to the username and then click the Run simulation button if you've already run the simulation you can also click reset results if you want to start fresh and that's it for sqs and Kinesis streams in the next video we're going to look at some of the limits that exist with both of those sqs and Kinesis streams are highly Performance Services targeted at specific use cases understanding how the systems work and the restrictions around them are essential for designing a robust web application in the previous discussions we've covered most of the limits around sqs and Kinesis streams and of course they both have the soft resource limits that can be raised but there are a few Kinesis stream limits I wanted to mention here the first limit with Kinesis streams is the limit to the size of the data blobs you can send through a Kinesis stream they are static at one megabyte this is basically constrained by the size of the shards making up your stream if you have data larger than that you'd either need to break it into pieces or send records of S3 keys where the data is actually stored in relation to sharding there is a limit around the number of put requests operations you can perform each second and that is one thousand per Shard if you find yourself hitting that limit you'll need to scale your stream up by adding shards to it and those are all the major limits with sqs and Kinesis streams between sqs and Kinesis streams you should be able to find a solution for any messaging need you have sqs provides a more distributed and decoupled form of sending messages and Kinesis provides real-time and high throughput streams let's recap what we covered in this module we started by discussing some of the details of sqs like delay seconds and visibility Timeout on messages then we created a cue and changed our app to send race results through a queue to update the hamster's table next I introduced Kinesis streams and then we again sent race results into the stream while the Lambda function consumed them on the other end and updated the races table finally we covered the limits regarding sqs and Kinesis streams in the next module we're going to look at some communication Tools in AWS like the simple notification service and cloudwatch alarms hello and welcome back to AWS developer designing and developing this is the last module for this course and we're going out with a bang by covering two services that will increase the real-time nature of your applications simple notification service or SNS is used to send notifications to users Via SMS email HTTP and many other ways a usual trigger of SNS is cloudwatch alarms and those can be configured to make sure that your application and services stay up and healthy let's take a look at what we'll be covering in this module we'll start by creating an SNS topic and subscribing to it with your mobile phone or email then we'll modify our demo project to publish race results to the topic and give it a whirl next we'll create a cloudwatch alarm to monitor our application health and send notifications if something happens we'll end out the module discussing some limits around SNS and cloudwatch alarms and then we'll have a special clip on what resources to delete as you're cleaning up your AWS account after this course topics in SNS are single elements where consumers can subscribe when any message is published to the topic all of the subscribers are sent that message over whatever protocol they have configured this is essentially how SNS simple notification service works in this module we're going to set up a topic and subscription so that we can send notifications whenever races are complete we'll start in this clip by creating the topic that will have messages published to it open up the file in the scripts 10 folder named create SNS topic.js we only need to implement the create topic function in this file so let's start by creating a const named params and assigned to it an object the first and only property for this object is going to be name which you can give the value of the topic name function argument topics are super simple as this is actually the only required property for the create topic command now in the next line create a new const called command and assign to it a new Constructor call of create topic command passing in params then in the line after that return a call to send command passing in command this file is now complete pretty easy so we can switch over to our command line to run it navigate to the scripts 10 folder and execute the command node create SNS topic.js you should get an output with this topic Arn something that we're going to need later so don't get rid of this output this will signify that your topic has been created and now that we've got a topic we need someone to subscribe to it which we'll do in the next clip one of my favorite things about SNS is the different types of notification options that are available you're not only able to send emails but also SMS hit HTTP endpoints trigger Lambda functions or even send the messages to an sqsq we'll keep it real low-key for this course and just set up an SMS notification we do want to know what happens with the hamster races after all don't we open up the create SMS subscription.js file we'll start by adding a few values we need a mobile number to send notifications to so add your mobile number including the country code and no hyphens just the numbers as the value for the end point here if you've got an American phone number your country code is one you're welcome then on the next line we need to enter the Arn of the topic we just created you can get that from the command line output from when you created the topic copy this topic Arn value right here and then paste it as the value for topic Arn now we're ready to implement the create subscription function go ahead and start by creating a params const object at the top of the function the first property is protocol which we are passing in as the function argument type the next property is topic Arn and you can give that the value of the topic Arn function argument the last property is endpoint and you can give it the value of the endpoint function argument these are the only three properties you need to subscribe to a topic on the next line create a new const called command assigned to it a new Constructor call of subscribe command passing in params and then after that you can return a call to send command passing in command this will make our file complete so let's switch over to the command line to run it in your command line execute node create SNS subscription.js it'll output the subscription details and you should be up and running in the next clip we're going to implement a library and our demo application so that it can send notifications to SNS now that we have a topic with one subscription any message published to that topic will be sent to the subscriber I've prepared some of what you need to do this we just need to implement a publish function in your code editor navigate to the lib data lib folder and open the sns.js file this file exports a publish function that can be used to send a notification to the SNS topic let's get started completing this file by pasting the Arn for your topic to the value of topic Arn you may still have it in your clipboard but if not you can go back to your command line and copy it from the create topic output now let's supplement this publish function create a param's const object on the first line here just like the topic creation and subscription there aren't many properties required the first will be topic Arn which you can give the value of the topic Arn variable that you configured above the next property is message which you can set to be the MSG function argument now with our properties set let's publish out the message and the next line create a new const called command assigned to it a new Constructor call of publish command passing in params then after that return a call to client.send passing in command and that's all the changes we need to complete adding SNS functionality to our demo app now in the next clip we're going to configure a cloudwatch alarm to keep our application safe when you're working with many AWS Services cloudwatch alarms may be created for you a great example is the auto scaling group that we configured with Target tracking enabled AWS created two cloudwatch alarms one to scale up and one to scale down but sometimes there are specific things that we care about that we want to create custom cloudwatch alarms for in that case there are hundreds of different metrics for different services at cloudwatch lets us set alarms on in this clip we're going to set an alarm on our load balancer to send a notification to our SNS topic if the healthy hosts fall below one this basically means if something happens to our application instance and it stops responding we should get a text message all of these details will be wrapped up in a single cloudwatch alarm we'll create this alarm using the create cloudwatchalarm.js file and the Scripps 10 folder we'll start by filling in these to Do's for variables first we need to paste that SNS topic Arn again for the value of topic Arn I'm kind of thinking that by now you might not have access to it on your command line anymore so I'll show you where you can find it in the AWS Management console go to the SNS dashboard click on the topics menu option on the left and then you should see your topic there with the Arn in this right hand column if you copy that value then you can paste it in the string for the value of topic Arn next we need to get parts of the target group Arn and load balancer Arn from the resources that we created towards the beginning of this course so head back to the AWS Management console and go to the ec2 dashboard this time in the left hand menu at the very bottom select Target groups and then select the target group named hamster TG and then we're going to just grab a part of the Arn not the whole thing select from Target group slash hamster TG all the way to the end of the Arn this little section is all that cloudwatch needs to set up the alarm so switch back to your code and paste that as a string for the value of TG now we've got one more and that's for the load balancer so back in the ec2 dashboard select the load balanced service menu option on the left then select the entry named hamster lb and in the details for the Arn value we're going to select everything from App slash hamster lb to the very end and copy that little section then back in your code you can paste that value as a string for the value of the lb variable now that we have all those values completed we can get started on implementing the create Cloud watch alarm function first create the params const object we've got quite a few parameters here in no particular order so hold on to your hat first add the property alarm name which can have the value of the alarm name function argument this will be the alarms identifier in cloudwatch the next property is comparison operator and the value will be the string less than threshold this is the check that the alarm will do we want it to see when the healthy hosts for the load balancer are less than one that's why we chose less than threshold here as opposed to greater than threshold we'll Define the threshold below next add the property evaluation periods and give it the value of 1. this lets you define how many periods to evaluate if the value is less than the threshold before going into the alarm state in our case we want to know immediately so we'll set this to 1. the next property is metric name which we'll assign to the string healthy host count there are tons of these metrics but this one gives us the correct analytics for the load balancer hosts the next property namespace refers to the AWS service that the metric is a part of the value for this is a string with the value AWS slash application elb the next property is period which works with the above evaluation periods property this says how long a period should be defined as in your alarm the typical length is five minutes but I actually prefer one minute for this this value should be in seconds so enter the number 60. next is the threshold property in our case this is a number of healthy hosts that are okay this threshold is based on the metric that you selected we'll enter one here since we want the alarm to go into alarm state if it's less than one this property is directly tied to the comparison operator above you'll notice it says less than threshold and this is the threshold property the next property is alarm actions and it has the value of an array this will basically be the actions the alarm takes when it's triggered enter the topic Arn function argument here the fact that this is an SNS topic tells the alarm all it needs to know and it will publish an alarm to the notification topic the next property is Dimensions with the value of an array this is where you specify exactly which resources the alarm should gather metrics from for healthy hosts we need to enter both the target group and load balancer resources here create an object then add the property name with the value of Target group as a string the other property of this object is value with the value as the TG function argument now we'll make another object in this array the name property for this one will be load balancer and the value property will be the lb function argument now outside the dimensions array add the statistic property and give it the value of average as a string other possible options here are minimum maximum sum and Sample count minimum would probably also work for our purposes and the final property is treat missing data this property defines what the alarm should do if it is missing appropriate data we'll add the string breaching here as the value in all lower case this means that if there is missing data if it can't tell how many healthy hosts there are it should trigger the alarm other possible options for this are ignore not breaching or missing the reason that we are going to be putting breaching here is because if you reach zero healthy hosts on your load balancer instead of the metric showing up as zero it actually comes in as missing data so for us a missing value here basically equals zero thus we want the alarm to fire and that's it for all the properties and the next line create a new const called command and then assign to it a new Constructor call of put metric alarm command passing in the params variable then you can just return a call to send command passing in the command and that is all we'll need to create our alarm let's run the file by switching over to your command line and executing node create cloudwatch alarm dot Js and you don't really get any useful output when it finishes completing but once it does the cloud watch alarm is now configured to alert your hamster topic whenever your load balancer goes below one healthy host there aren't many limits when it comes to SNS or cloudwatch alarms this is mostly due to their simple operations cloudwatch does have a few more than SNS so let's take a look at the limits that do exist one restriction to be aware of with SNS topics is the allowable characters in a name only alphanumeric characters hyphens and underscores are allowed unfortunately emojis aren't supported Amazon really needs to get on it though cloudwatch has a limit of five actions that can take place per alarm this is a hard limit and can't be changed if you need more actions you could create a new cloudwatch alarm with the same metrics but that might end up being pretty messy in addition a cloud watch alarm period can only be a maximum of a day remember the period is used to decide whether the alarm should fire or not if you're looking for more lengthier time frames than a day then cloudwatch is probably not the best solution to make those evaluations the last restriction is that even though cloudwatch does have a mechanism for creating custom metrics you are restricted by the metrics that AWS makes available for instance if we wanted an alarm created whenever zepto the hamster came in sixth place three times in a row we'd have to look for another place to figure that out we all know zepto is a winner though and that would never happen so maybe this is just a moot point and that's it for limits and restrictions in the next clip I want to give you a guide for cleaning up everything we've created in this course throughout this course you've likely created a lot of resources I thought that making a list of what you need to clean up after might be helpful so let's look at what you need to clean up and how to do it we'll start with the ec2 dashboard where most of our resources are located you'll first want to delete the load balancer then the auto scaling group then the target group once those are gone you can kill any instances still around delete the Ami we created and then delete both key pairs since there aren't any more live instances using them delete the launch template used to make our instances delete any EBS snapshots that were created and any EBS volumes that are still around finally delete any security groups other than the default one next go to S3 and delete the hamster bucket then go into RDS and delete the MySQL instance don't bother taking a final snapshot of the database unless you're really nostalgic about the results of the Petco 2000. now go to dynamodb and delete both tables then go to elasticache and delete the redis cluster we created you can go into Route 53 and delete the record first and then the hosted Zone then go into API Gateway and delete the hamster API that we created now go to cloudfront and delete the hamster distribution you do have to disable it first and then once it undeploys then you can actually delete it we're almost done head into the sqs dashboard and delete the queue we created then go to the Kinesis streams dashboard and delete the stream we made we also have that Lambda consumer so go into the AWS Lambda console and delete that function finally go to SNS and delete the hamster topic and then go to cloudwatch and delete the healthy hosts alarm there are some resources that are still around in IAM but they have no cost so you can delete them at your discretion and that's it it's hard to believe we can delete so much work in such a short time while SNS is powerful and quite useful when coupled with Cloud watch alarms the true potential of real-time notification is realized there are an endless number of uses for both services so I hope this module has inspired you to try out things with each let's take a look at what we covered in this module we started by creating an SNS topic then we made an SNS subscription with that topic and your mobile number next we modified our app to send notifications to the topic and ultimately our mobile phone when race results were finished then we created a cloud watch alarm and hooked it up to notify us through the SNS topic finally we looked at some of the few limits on SNS and cloudwatch alarms and we've reached the end of this course AWS developer designing and developing I sincerely appreciate you taking this journey with me to learn more and more about AWS at this point you're in a good place to start doing some damage in AWS find a personal project or side project at work that allows you to try out some of the services discussed in this course and consider finishing the other courses in the AWS developer certification path to prepare to master the AWS developer associate certification exam again my name is Ryan Lewis and thank you for watching this course \n"
     ]
    }
   ],
   "source": [
    "# Specify the YouTube video URL\n",
    "youtube_url = \"https://www.youtube.com/watch?v=bhomsGI56Ok&t=20s\"\n",
    "\n",
    "# Extract the video ID from the URL using regular expressions\n",
    "match = re.search(r\"v=([A-Za-z0-9_-]+)\", youtube_url)\n",
    "if match:\n",
    "    video_id = match.group(1)\n",
    "else:\n",
    "    raise ValueError(\"Invalid YouTube URL\")\n",
    "\n",
    "# Get the transcript from YouTube\n",
    "transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "# Concatenate the transcript into a single string\n",
    "transcript_text = \"\"\n",
    "for segment in transcript:\n",
    "    transcript_text += segment[\"text\"] + \" \"\n",
    "print(transcript_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5bdfc62-1187-464c-a7d2-cf21594c5e44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Music] Foreign hello there my name is Ryan Lewis and I'm happy to welcome you to this course AWS development and development if you've come to learn the ins and outs of AWS then you're in the right place this course has a few options but the most important is that your understanding and development with development an AWS will have lived up by the end this course is intended as a fast-forward to my precious source AWS began to get started with many of the most effective users.AWS services so if you haven't watched that course yet then I definitely brought the time to work though it with that being said this course does cover many services already covered in the precious course is the application and treatments that we'll be covering the getting started course's goal was to get really you family with the multitreaties of aws's offers this course takes that family to make you realize an AWS even more dangerous- and information will be on the development side of AWS and not cover infrastructured services like VPC or iam for this course we'll take special care to include each service for AWS in the process we'll create and demonstrate how to destroy your AWS.We're sure for your own applications development an application in the street is not always quite the same as typic no-cloud development as company migrate to theclud more and more partners are getting their first measures of operations and infraction as they're fired to do their own clean treatments in your past experience you may have been used to just build your application and then spend it over to another team or engage to do the actual treatment of that application because that cause is the cause of the problem.Well, you may have thought of your application as only the code you wrote and other parts as outside of your domain but as with everything in Tech the world is changing and operations are concerned into developing programmes I believe this is a good thing and willI've imagined this personally over the past few years when I began study AWS development and it's only introduced the quality of my work I like to examine Cloud development like this family when you think about a new application you'll want to start with the infastratives you'll need to make such a contribution and AWS give you multi-options for each of you makeI'll be able to consider how much AWS Services might best be used because otherwise your application will be a dispensable and scalable product of code and service.Yes, the service is in AWS are grouped into domains in the AWS context you can see how they've grouped them by the application agreement each deal with some applications are complete and worse because there are quite a few services it helps to group them into each of this cap we're going to talk though AWS doesmain to help you understand what each of the most important domains is complete for the purpose of sex.I don't know what I'm talking about, but I'm not sure if I'm going to do it, but I'm going to do it, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm gonna do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you, and I'm going to do it for you.I'm sorry, but I don't know what you're talking about, but I don't know what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're doing, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about.I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.I'm sorry, but really are familiar on the development side of AWS using these are opportunist though so don't feel like you need to use any of these developments more services in AWS if you're developing on AWS service on AWS service on the basis of the service management side are in the management and the government domainize these services fous on the market and change of other services in AWS a lot of these services are used by more by devolves Engineerings but we will be using cover in this code of conduct and code of law.I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm here, but I'm sorry, I'm sorry, I'm here, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I's you, but I'm sorry, but I'm sorry, Mr.We'll talk about being applications for application which is for service that other services between other services in AWS for instance services like simplification services and simple queue service benefits ways for applications and those for all of the AWS dos have a few issues and they're more important to us.I'm leaving them out now that you understand the AWS doesmains keep them in mind when you're looking for a solution in your application there may be a cool new service and a domain that you're concerned with and it might just be the perfect solution for your problem Amazon benefits a few ways to interact with their futures with one of the first you're interested in the CIA and the CIA is correct as you negotiate your local case.You'll spend most of the time in your room this is because the AWS SDK proposal is because there's some other open source sds for support that aren't supported already by AWS these are very effective but it's important to know whenAn SDK is certainly supported so that you can understand the level of support it'll recover let's take a look at the official sds available from Amazon first of all AWS programs for the most popular version of development plans in no party order these are java.net Cplus plus pythonAnd unity with all of these offers it's very likely that your budget of choice has a certain SDB from Amazon in this course we're going to be working with an application build on none so we will be using the JavaScript SDK effectively if you want to get a head start with it's the URL for the document site for the JavaScript AWS SDK this course is dying to not only teach you how to be an amazing AWS tailor but also to help you develop the AWS model for the AWS development of the world.I can't make any assessments that this course will require everything you'll need for the exam whenever you take it I am covering each topic as if you'll be speaking on it.Ask about topics services or service limits there are also questions about debugging certain programs and giving assistance services because there's no real way to know which services will be on the exam but the core services of AWS like ec2 S3 and dynamod are almost obvious to be on the exam I've also seen alike like sqs Romania 53 and elastic Beanstal to name a few pieces with more you work with and learn about AWS the user the test will be for you recommended.Nothing in the AWS coded development path here on plainsight and then agreed building more programs if there's a service you aren't very family with it wouldn't be a bad idea to build something just to give it a shot in analysis I've created a course called destruction the AW coded development as appropriate executory executor as a great source of the analysis and what the day will look like I recommend worldwide to useI'm sorry, g it after you've completed the AWS certifier project that will move to make the most of the AWS here's the demo appalledYou can look at each user is ranked on their future hamsters if you log into the application you can get access to your own data and can pick and choose which hamsters you want to return to yours in the least.There are some sources in this communication panel that let you reset the race resources and start them again these are all made more than enough resources so each time you run the operation you'll get a different outside see if you can make it to first place this application is going to extend a multi-purpose of diversity to promote some worldwide benefits for usNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.I can say it has saved my bacon a few times Billing history through crudwatch use to be the main way that you could afford this Billing monoring but AWS inflated the volume to provide more Features and the monetary to fine-tune the monitoring binary number on an incredible number we'll use it in a very small way but the financiality is there to use it in a more widespread mannerIf this is a new account you might see equal like this here that you want to be able to last Explorer on your account to open up new types of options you can do this later it won't kill usYou can leave that as it is now for the future forever I always set it to a small object that lets me know if I did something wrong for the first time and I left some resources to run away with the current month so you can leave it as it is now for the future foreverNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.I'm going to admit 100 here so it'll let me know when the full amount will be exceded and the bigger drop down you can either choose which means it'll be true when you're capable of killing whatever that means you'll have paid for it or assigned it for the first time.You know, when you are forced to have complaints over that amount of money by the end of this means that you cantermine those resources before they actually hit you physically onto some money so clear for some money and we're not going to do that so just clear now we can review the book.If you're going to be more serious than I expect to look at this source and just give you more information about what's going on, what's going on, what's going on, what's going on, what's going on, what's going on, what's going on, what's going on, what's going on, what's going on, what's going on?If you want to use something different my only solution is to just make sure it has some sort of syntax high-leveling and sometimes they're pretty expensive for JavaScript development.I'm also making it available on GitHub if you prefer to get it from there would have said the match of using the extrasAnd after section of the code it can be really important to have that available just in case you make a typepo During a lesson and we will be writing a lot of code in this course and that's all you need to get started now let's digper into the world of Amazon web services back to AWS developer dying and developing in this mode we're going to learn Advanced methods for newing and manadvancing Claud compute or equator into the precious getting started source how you can realizeWe're going to interact mostly with AWS through the SDK and AWS CLI so we'll be intervening different ways to do some operations you may have learned in the past we'll start the movement by covering some of the financial concerns of ec2 Ami and the AWS Marketplace then we'll destroy the ec2 rect API and look at those details that go into every instance I'll make one of the most important of the most important parts of the world.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.I'm sorry, understood when killing your AWS Solutions and should help you from painting yourself into a company in the future the economic service so we're going to go into a bit more detail into order by AWS it undermany of the other services in AWS in AWS we've already covered most of the base of this service and the getting started source so we're going to go into a bit more detail into order in this caseI'll use an Ammi that comes from the AWS market place with Linus or Windows operation or none. js pre-installed you can create your own Amis with your custom application or launch from the AWS machine you can assign to the AWS.We'll cover this in more detail but basically enough gives you three options for how you can handle the problem and how you can handle it.I only agreed to the ecc2 instance and act differently is that EBS volumes can be agreed to other instances and also can live on even if the instability is fixed the third option is easier than ever to the EFS.Instance and being able to deal with what states are available to the new economy an EBS back can be stopped for an emergency purpose perod of time will be supplied on the EBS campaign or this to an instance returned opportunity which can only be fixed or 22 can't be stoppedtion but it can afford quite a bit of flexibility with using EBS volumes that the EBS is faster is that the Ami Data for an instacy-backed volume must be changed from S3 on which is different from EBS was inventively avoided by AWS there are many people to useI mean, you know, you know, you know, you know, you know, you know, you know, you know, you know, you're like, you know, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you know, you're like, you're like, you're like, you know, you're like, you're like, you're like, you know, you're like, you're like, you're like, you're like, you know, you know, you know, you're the AWS, you know, you know, you're like, you know, you know, you know, you know, you know, you're like, you know, you know, you know, you're like, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you're like, you know, you know, you know, you know, you're, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you, you, you, you, right, right, right, right, right, you, right, right, right, right, you, you, you, right,Now there are a few high-level interventions that you have to decide on before starting an economic opportunity of one of them is inventive type which I cover in the getting started source another occasion is another intervention is the inventiveI don't have anything to do with what the instance can do it really defiles the issues in which the instance lives and how much you'll pay for it on-demanance only for what you use no service instances that we spin up and use down like none these instance options are good for applications that need to be scalyed up and down it's also the best of instance when you're living AWS like we are because you can kissI the instability when you're playing with it correctly between one and three years but the studies can be made some up to 75 percent off if you're a capable company that will realize the need of supplyWell, ass of instances is called spots these are instances that you bit on and they add power of it like the unused hotel rooms or Airline rooms that are sold at the last minute just because it's better to get some money than no money Discounts can be even deeper than with preserved opportunities up to 90 per cent off but there's no real guesse on average a web applicationNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.I'm sorry, but I can make it different to work with many AWS understands this can be different which is why they develop and maintain the sds inside the code of the sds AWS is signaling each new Made You indisposed most of the SDK is just a player ofcode over the rest of the rest making a recall of this new API extraordinary development which I wouldn't wish on any one of them.And then actually making the request but let's go ahead and take a look at the process involved so we can apply what the SDK does for us first of all you need to decide which threat you want to realize you want to progress successfully by many of the original applications but not by many of the original services, but it's increasingly becoming involved in most of the place, 4 is the pre-federated threat itself.Yes, there exists a different process when signing advances 4 is the more secure and therefor more complicated process where the application 4 threat you'll start by dealing with a legal response this is a challenge of the HTTP methodology and the URI path alone and the fascination of the URI methodology and the fascination.I mean, really, the previously impossible question that has been done with sha-256 and we're not done yet once we have this string we need to actually sign it to do this we'll need to first sell a signure through making sure through an Hmac and then finally signaling our String with the result that process gives a signal which we will add as a query mark to the actual process when we make it this process is pretty difficult to get to right as my own own expert trying to make it happen.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.Is for this course in order to run your application in AWS we need to have one or more ec2 interventions to excute that code so the first step is to create an emergency opportunity there are many different ways to do this into getting started source we mostly use the AWS Management Console to create resources and another great scale is using the AWS CLI which can be used if you're going to use the AWS CLI instead we are going to use the AWS SDK to create an opportunity with code and before we can create the opportunity for us to find the AWS.We're going to create a security group for the AWS whenever we need to tell the code which region we want to use you to do this every time you perfect an operation in your code so I want to make that program where you don't need to use it for a reason.Your region over and over open that you're using then saving the file and your region is set to go for the rest of the course now a lot of this course will be usded to perfect operation through AWS so've created a screenwork in the course of the proceedings.I've created a file we'll make in this Clip called new object.js this file benefits a blueprint for the change we need to make each movement has a place to do so it's clear where you'll be making your movement I'll walk through you through the field to start with at least the top are willing Imports then we've got two to do's where we need to create a new enemy with our duty and send a company this is a policy group that we'll useSo, we don't have to write the same code multi-time keeping our code dry next is some local foods being destroyed then we've got the calls secured then we've got the calls secured to create a security group group new a key pan and then create an emergency below that are three ways ahead that we'll need to introduce the first one to create the first one is to create that security Group to create a newer and the last is to create a new one and a new one to create a new one.That we just created as a side note an economic opportunity can be created through the fact that the SDK without an exception group or Key Pair isn't much of an issue but the key peace is more serious because if you're referring an Ami that isn't changed to a different type of society such as you spend it on your own.Be able to make it three of the JavaScript AWS SDK which we will be using in this case is pretty different from the version two I want to cover those changesWell, you want to use these are broken into such as the economics as well as the dependency as present here in answer when we import things from a creditor when we'll de-struct the results and only import that we'll be using these will always be one firm and single companies into the next when you use it to make something happen in the AWS three ways how you make something happen in AW there's a newI mean, the client to send the company over and over with the new AWS SDK we'll be implementing a sendcommanfunction in most of our files to keep our code dry but just know that thisClimatic group sent company transaction is the way things happen in the new version of the JavaScript AWS SDK and this new version does not apply to other AWS files now get started with other AWS teams now get started with the changes in this file after we've made all files and we will be running a run-in.It's a lot of things firstYou know, after the AWS slK slash object ec2 package with our client we can now describe the end of the service as I mentioned this will help us keep from mentioning the fact that the operation will take a company as an important tool and then send the company as an object to an individual.This is how you'll recall the AWS SDK to use which will use the value that we assume in that EnV file in the next line we'll return a call to wrong.send pass in the companyWhich we will write many of the claims will be Pascal case starting with a capital matter so keep that in mind as you add one product call value will be the SG name parameter then add another proposal calling group name with the value as the same SG name now we can imagine our first company to do this we need to describe it from the AWS SWDKI mean, a company after the ec2 official import calls those serious concerns now into that the SG group active we created now the company will have all the right names and claims we need to use to send it which is the most important thing to do in the world.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.22 so that we can become the inventive once it's been created and then Port 3000 so we can access the demo app in our crowds so start by writing a new call for rules params and an effective application to we can get that from the data sustainability rolls that should sound family to you if you've set one up with the Manage contract before the world is getting the grooves to apply the fields to the fields to the fields to the fields.For the value here enter data dot group ID the potential value for this are likely like TCP or UDP access TCP here all lowercases the next two stages from Port and to Port Devine the port you're laying the ground for the first case we'll enter the 22 in both of the Ports or UDP entry to PortWe'll be able to access anyone so much the value of 0.0.0.0 now we need to also add the rule for Port 3000 so close to one more company.Now we can use the new code code of conduct as well as the last objection to it.Well, then we just need to send it and the next line add the return keyword and then call send company passin' in Auth company and with that code competitor we've identified the code to create and assign rules to a secure group in the next line we'll create the key fair in this Video we're going to finish a new piece of paperNostalgia, nostalgia, nostable name, nostalgia, nostalgia, nostalgia, nostalgia, nostalgia, nostalgia, nostalgia, nostalgia, nostalgia, nostable name.Now we just need to send that message and the next line at the end of the return keyword and then call send company passing in the company calling this support call here is important because the key price is because the price is great because the price is great in AWS the effect Returs the condition of that key price's price-key if you don't save the key air is easily available.If you'll see the Key Fairness is being called after the Key Fair is created this is where it will save the Key Fair job and the next tip will take the work that we've doneWe'll create an economy both we've created a security group and a key fair so now we're ready to create an economy that uses both of those sources we've only got one united operation left in our file record taking the arguments SG name and key name which were used to create the boundaries of the other sources inside this world by killing a params competitor and as you can develop an effective system.This will be used to create the Internet and to get this we'll need to make a little side trip to the AWS Management framework see there are a couple of things about Amis first there is a product in the economy in the AWS SDK called describe challengesYou know what you're looking for the second thing to know about Amis is that they are region special for an update of the image even if the images are the same for that reason I'm going to hoasonI mean, how you gonna find the Ami ID that you'll use in your code go to the ec2 dashboard in the AWS Management Console Click on the Launch Instance button and then Launch Instance and you'll be prepared with the Ami supply at the top there's the Amazon Linux 2 Ami at the end of that Title is the ID see the ID see probably won't match this one since you may be using a different development or they might have produced an updated ID that will also be two different IDs for x86 and one for each of the AmisWe'll be using a T2 micro intervention which is a type of type of type as T2 small or M4 extra range and defines the programs of the instance we've talked before about what these mean and we can use one of the two types of the smorst hereOne with the production key name and entry the value of the value name and give it an array as the value this is where we can add any security group to the supply source then we have the Min count program and you can add more than one Security Group here if you want to have there also a security group of people.But we will use it to run shell companies once again if you open the file in the O3 store called inside the field.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.The top of the file so we can attach to it a new Consortium call of runs demands and then pass in the last step is to send the company so retrenched a call to send company pass passable in the future and with that we've completed our crash now for the fun part to run it but before that last step is to send the company so returned a call to send company pass pass passable in the future.About a PSA this code is going to assume two things about the region that you're deploying to first it's assigning that you already have a default VPC for that region for some regions that AWS will consider this by consulting and for others it won't look at you at the ec2 dashboard and the Management Console AWS has the default VPC Listed if you don't have an ID here then the credit VPC isn't affected for this region on this page AWS has information and the pace for developing that destruction VPC manIf you don't have a default VPC already consulted for your recovery the other thing is that the subnets in your fault VPC are concerned to Auto-assent public impact but you might want to check by looking at this communication.We'll need them for our Script so go ahead and run npstrall at the demo project root once that computes navigate down to where these projects exist now you can run no new pieces out of the list.I'm going to go ahead and then I'm going to recall the ec2 instance here's where it was created and because it's running and because it's running at this point and because it's running at this point the demo application should have started working on the future.If you copy the public address which is found here in a new tab and ad coron 3000 you should see hamster ball running freely in AWS in the next version we're going to look at what we can do with this opportunity now that it's already been created once your opportunity has been created there a few different possibilities that you can perfect on the future as you manage to understand the SDKIf you have an opportunity to imagine some extrapoles of introduction so I thought we'd actually try out two different types to manage events in this world just wanted to mention this movement in order to make it happen.I'm sorry, I'm sorry, because it will give all the benefits you realize we have run in a gift region you can apply differences to supply the query and limit what types of instances are returned I wanted us to give this company a try and see the type of respence we get in your code operation open the file management ec2 opportunity.I mean, first entry into the market and then under that order, then add then an equals after the curly brace and require the AWS sdK slash object 2 package now let's introduce the list into operation until after we're not going to use a few but a few of those that I just want to know that.So we can go ahead and dispose of a new competitor call then call in the next line just return a call to the end combined action passing in the next version of the contract if you screw to the bottom of the box.I wanted us to go ahead and run it and see what kind of output we got on a company line make sure you're in the right front and then exited equalize the program.js on it's finished running it out of a bunch of Jason to your counsel this is a pattern of the Data object that is retoured on you've sent the programs into the program to bring the program that we care about from this Jason is called response to this problem.If you have more than one opportunity on your account there is a very high potential that is signed to the resources will be more than one object in fact.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.Well, I call and the next line will be many and many of the data available and so enter the return keywords then enter data dot reservations and then call for a call to renew this demand you'll create a new Arrow field with two arguments i and r is the added value and an application which is one of the most important things in the world.No, we'll use a piece of the information here with our information management operation so let's go ahead and go a grab and grab it to the market you're just launched from the community and you've got more than one opportunity you should see them all listed hereI'm sorry, but the easy way to identify that would be likely the key name for that should show differently key copy the instance ID proper which will be like a lowercase i hyphene and then a bunch of letters and numbers size over to your code eitor and paste into the list of the agents of the termage operation call at the end of the file where are lots around the ID in case you didn't copy them now let'sIf you want to the key difference between stopping and planning an inventory is that a stopped incompetent can't need a params object to pass into our Command so first delare a param's next will only take one process which is an opportunity you can imagine many different ways with a single community which is one of the best in the world.So we'll need the top of the file add acoma after delay and add the rest from the company at all in the next line just return a call to send company passing in the new Finance I've already spent a few years.I'm sorry, but you saw earlier it's clearly concluded out so company the line with the list of events calls and then until the line calls it illegal saves the file and the then switch over to your company line we're going to later our hamster opportunity by running node manage opportunity.D that was passed in and that's it for terminating opportunities if you don't trust your company line you can go check the AWS Management Console and see that the opportunity in question is present down in the next version we're going to open an opportunity with an Ami from the AWS Marketplace running your own code on a vanilla Linux or Windows opportunity is one of the most common use cases for ec2.I'm sorry, but you'll find that there is an existing ami with the software pre-installable available on the AWS Marketplace let's take a look at the marketplace by Navigating to the ec2 cashboard in the AWS Management Console sick on the AWS Exchange button and then Launch Instance and you will be on the Ami Selection Page the Software that are available from the AWS just AAW Marketplace are adrazing system and some custom system pre-empted that's important it on the AWSAnd menu click AWS Marketplace and here you'll be able to see what's on offer there are somefeatured and popular Amis and then a bunch of categories you can see there's quite a few categories available ways in Amis in practice for this course I wanted us to lay an opportunity with an Ammi from the market place specifically one that has none already in place on it in the search box type.js and hit access there should be more than a few resources but look for the original solution no one designed bybitnammy.And it should have a fair record raised by date this number here is the application of none that is included on the Instance there shouldn't be any lost information here if an Ami from the market places cost money it will have the per hour lost next to the image information you may see some other introduced with a cost now available with it none is free and open once and the bitnami Ami is free as well you'll still be involved for any instance cost but the Ami has no investigation has no impact nowWell, no and then crack on the next page has a lot more information about the image if you want to read it for our current caseI'm going to last and then once the subscription has gone through your page look like mine with a condition to negotiate but why don't you know why this takes time at all but it does a couple minutes at least now now I'm going to eat and then once the subscription has gone throughOnly get the Ami ID you can add some skills here as the institution of node and region if you remember I mentioned before each Ami is recall specific this is whyWell, you can see in the drop down that there are many different versions majors node that you can choose from now once you've sedated everything completely copyy the Ammi ID that is displayed right here and then switch back over to your code expert and open the new emergency.js file we're going to launch an emergency using this Ami instead of the vanilla Amazon Linux Ami that we did beforeI'm sorry, I'm sorry, but there are two more changes that we need to make the user Data we have here install node and then give it none the demo down and it runs it we don't need to install node anymore because this bit Nami Ami comes with it so let's change this user Data open up the filetilted markplace 2 startup.sh this is basically the same as before none of the noonInstallation steps and the Linux Package manager raised to at least date since the bitnami image is debian bought capy the base64 encoded text minus the starting hash at the bottom and paste that into the user data string value in the record area ec2 in the next area is to the key fair and secure group name on line 17 and 18 if we were to run this file again it would try to create new sources with those names and then exploit them because they already give them new names by nameYou should see the same type of message as when you ran the file prefaced this time you've launched the opportunity with a whole different Ami I'm going to ride the remote ahead in a bit and then deal correctly that the Internet created I'll copy the public IP from that newly created place into my heart to my future 300 and the next 300 and the next 300 and the next 300.We're going to create our own Amazon machine images are sort of like the primer for every paining that you want to create with an emergency it's the starting point in the base software that you build off of but they do another use which is to help you realize an application with your gift of yours and use one of the most important things for Amis.We're going to use the SDK to create an Ammi from the beginning we started on the last screen to do this move over to your code expert and open the door to the file community.We're calling our new operation passing in an Instacy ID and a name for the original image once it's done it just obviously forgets out before we'll go get the real entry point now on line 12 we're calling our new event action pass in an Instagram ID and a name for the actual occasion once it's done it's justWell, obviously first there are only a few examples of what should be done to a new image next is the name perfect to which you can assign the image name important this name is basically just an idea for the Ami and it's for the annexes now we can use to send a new image into the world.I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.After the image has been created or you can move it in the context or with the CLI the last step is to get the instance ID that we want to create our image from we can use the manage ec2 instance file to do this open the file and make sure the list is made available and not impossible and we're not needed to imagine then anything that file and then move over to your company line and run without management outside opportunity.We just created from the AWS Marketplace and the one that we had previciously fixed with the Amazon Linux on it if so look for this state of progress one of the instances should show as running while the other should show as terrible knowledge of the instance ID for the instance that shows that it's running now move back to your code home.From the company line and finally save the file now we're ready to create our Ami move back over to the market line and run node lie a.js it should process for a second or so and then print out mean that it's done you can go to yourrowser and then to Amis in the menu to see the image that you just created one thing that I want to mention before we finish thisIf you start a new opportunity with the image that we just created the demo project won't be started when the program is started when the code for the demo project will be present on the Instance it just won't run reproducable user Data would exist in a lanch templace and that we're going to be talking about in the next moment all servicees and resources in AWS have immatures it's important to be throughout them so they don't trip you upAnd that's the best thing about running numbers per recovery this actually gets a little complicated though so I'll show you where to view these limits to the ecc2 gap in the AWS context on the left hand hands that are a limity and clear on it's you'll see there is a lot of limits and they get very specific they're available.You can really do one thing to notice about all these things though that you can set any of these limits and use the rest limit increted button that these aren't meant that these aren't hard limits you can ask AWS to track them if you ever need to be using AWS for a few years.For your company then studying these limits is rarely enough to be a number to the number of Amis special that you can have but since each Amis is holding us an EBS snapshot and there is a limit of 10,000 EBS snapshots I would attach to the max numbers you can have is 10,000.Well, psot so hashing 10,000 Amis is not going to be done we've taken a good turn of what ec2 has so much to offer you should have a good answer of how company works in AWS let's recover what we covered in this mode we started by considering some special measures of what we're using after I walked you through how the different projects happen when making it happen.We don't need to do that much since we're using the SDK then we started writing code as we created a secure group a key sky and finally an economic opportunity once we had an opportunity that we looked at other SDK funnings like Listing Instances and Terminators then we took a trip to the AWS Marketplace to pick up an Ami preloaded with none and finally we started an Ami from an Instance that we can use when there are others living and we're still here.We're going to look at two of the major toos to normally scavenge your interventions are about meeting the demands on your resources without you having to stay awake all night in order to choose this magic we'll first take a look at what scale and economicity means with AW with this knowledge we'll take the first step by creating a launch schedule that exists we want to rub to rub upWell, then we'll create a huge group and the last number of steps we'll need to concubine a scaling policy on that auto scaling group with those four states in place before we'll close out the moves by looking at them at limitI'm aware of what I'm trying to do with AWS as well as I found how the numbers of these things aren't completely understood and can lead in some context in how easy it is for a friend of AW.I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about, what you're talking about.Well, then, they let you sleep soon at night with the argument that your applications are Healthy no matter what you'll fill your user Data it's true a latch technique is the most important thing for all of them.You know, he inventive type group and the beginning user data anyway they do stand alone and you can actually create new ideas just directly from a launch to sell you can still use a key contact for auto scaling group before one of you humans gives you and that's impossible to sell.If you need to update any of the negotiations through us on the launch tempole or you can easily speak a message in your Autumn group it is a lot clearer and more modern way to deny your introduction communication.Late for our demo project go to your code execution that we need to introduce once we've already gotten our feet wet with the AWS SDK I'm actually turning a lot more help here so you can write less troubles more on the future I've already charged with the AWS SDK.I've already done that and actual one thing that we need for you that we haven't really talked about yet an IAM network to give to the others to talk so much.I'm sorry, but I don't know what you're talking about, but I don't know what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about, or what you're talking about.Ignorance's sake for the demo when you're doing this thing for real make sure that you just don't give any opportunity access in a process since the missions and the routine to only the operations and resources on the Apple application on the Instance actually needs now we can introduce the new version of the treaty that exists so let's do that we'll start by declaring a params next subject and give the first name to give it the value LT name next is today.I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.Oh, I need to go get the ID for it so to do that trick over to the ec2 dashboard in the AWS Management policy and Click on the Amis menu option on the left find the image with the name master image and copyy the Ami ID bring back to your code andpaste it as a value to develop ID the next progress is effective type which is the ec2 instance type I'll just enter t2.micro hereWhich will be the name of the key price the group will be added with and give it the value of the key name group now at least that we created in the last mode which is already available in the SG name freely so hopefully that is available to the last program that we're using a medium.I'm sorry, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I'm talking about.Turn keywords and then call help send company passing in the company and that companies need for this file we can now run this file to create the launch templatate even though the other pieces of our record are not ready yet and that's in the next line make sure you're in theAs the next piece of our legal arsenal is an opening point for any single entry like legal web application I've already introduced some of the stuffed code so you can create that as we move this file into whatever I'm doing already.Kind of understand it at the high level at the top I've already identified two companies that I've already described below we will be using another company for reading the roadblock so let's go ahead and report that now developing the new world of the world.You can see there are two risks that we need to move the VPC is the first of the VPC ID for your loss VPC since the local baller will be created in a VPC we need to give it that information to use to move to the AWS Management Console and go to the VPC Dashboard we'll get the VPC ID first by clinking on your vpcs in the left hand menu you'll only have one VPC so copy for it and your codeYou can use this VPC code to make sure that you're doing the right ones I'm going to use the subnets Seter in Us East 2A and Us East 2B which I can identify us using this aggressive Zone consultation when we're setting up the auto caseI want to choose that now close each one of the subnet IDs and paste them into the subnet area as stretchers great just like the other screen files this operation calls the operation operations that we are implementing it by developing a new division group and a new group of people that is open on Port 80.I'll expand the network group in just a second of each of those calls are stored in variables so that we can then get the Arns for the Target group and load group then a lister is created with those values which consider the lot group and the target group I want to extend a solution of all these different resources quite quickly a lot;Now, ncer will send those requests this is going to be agreed to our Auto group and will basically end up being a group of ec2 projects with our project running on them what our project project is doing on them the two in our case the load breaker will be Listing on the Port 80 which is for HTTP and our Target group will be standing on Port 3000 since that's what our demo application is using the list will locate those things between the two of themYou can give the value lb name then add the security group property which will define these security groups in the local economy set the value as an Array with just one value the potential valueWell, it's actually important but I like to make it perfectly different so that you can clear it from other types of loose bailers then in the next line advert and then call help.I've already introduced the target operation with that company just has a different name and that's all we need to move below this group to create the network and no movement are needed for them so we can run this file move over to your company line and eclectic the company no new route router.js there should be some outside group specific I want to drive yoI mean, we're gonna need this in the next Clip so you can either leave yourcommend proposal out alone don't delete it or you can copy it or this Arn and save it somewhere like I'm going and with this we have a looseball ready to deal with and removing from a couple of other countries.I'll just like the other files you'll see that I've already mentioned some time for you from the next grouping client I've also created a special grouping group for the next few years.We are here to replace the value of the trade with the stargate group Arn that we copped from the company line in the last cup you do still have that saved to your criminal right well if not you can easily get it from the Management Console right right here now we can move on to the development of the market making group start by declaring our params next nameYou know, one as the value here next needs to be defined these clients zones should match the subnets that we used in the roadblock so that the auto case group doesn't exist an opportunity that can't attach just to the road base I'll enter 2A and USCs 2B next step is to give you an objet.If we don't identify a family group here then we'll use the dispute that's raised on the island.I'll enter one of the most important pieces of the world.I mean, the value of an earlier entry into the tgarn valiable which we concluded earlier this progress tells the group to the Target group now we can create the group on the next line line created a new claim and that's all that's needed to create a group that we're already done with.We're not actually going to run this file until we've completed the attack of the Auto scaling policy code and we're going to do that in the next Clip so see you there an Auto scaling policy defines when an auccured group should scavere up or down and there are a few different types of Auto scabling systems salivates salivities are the original way that these groups work with a central corporation and a non-performs.If you wanted true economicity you needed a policy up and a policy for scaling down scaling policies were adopted as a way to define multiactions when a single array of fields this morning to move the risk of a disease of the illness of the illness of the victim that's being raised forever after it's been introduced into the system.We're going to be using Targate tracking systems allow you to just Devine a guess for a mercic approach and the policy the single policy will take care of either down to stay pretty much right at that level that throws it's the best policy it's kind of like a set it and forget it type operation and what it is now for us to use for auto scaling clusters in our codeYou're going to be putting all of our policy claims in here the first phase is effective type and this definition of what the policy should do with the price you can set the price to the price.If it's all about what metal to use the first procedure for this object is Target value we're going to be watching CPU so if you enter the number of numbers active then we're going to be able to get the number of ours.I don't know, or it'll sell down and move inside if the value of a stocking with the number five because the CPU extension is a predefined technology and it's well underwood by Auto project group this how we can use it and it's going to apply the CPU to all of the States.So we have the group being created and the policy being attached so let's run this file and watch it work in your company node emerging.I'm going to skip this video and time a bit as I usually do and we can see that the auto scaling group was newly created and an instance has also been created for the group if you go to theloaders menu op and copy the DNS name into your webs from the road baller that was created you'll see the demo project put up this is a result of each one of those things that we put across the last few over the last few years.I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry, I'm so sorry.Those are the ones that you can imagine like the soft limits but let's take a look at some of the most important limits in my operation with auto scaing the main limits are on the number of groups and launch consultations that you can think these are so many hours enough that you wouldn't hit them if you had an entire company working on one account and again it's something that you can raise as far as I can tell there are no limits on how many subjects on how many subjects you can go wild withIf you do do a lot of international assistance the applicationload Balincer limit may not be enough so just keep an eye on that other lot; it's the number that you can afford to place a lot to a lot Balancer which is set at one of this is important when you're creating a web application because you're gonna need dividating you're public interested in the future.Well, I've never actually had a problem with this because I've never actually needed more than one SSL certificate on a road baller but if you do that's how you would solve it by having multi-loaded bales in analysis you can only have one road baller.I'm sorry, ad Balancer and that's about it for the majole walkers for auto scaling and load bailing scaling is the lifefood of the clean and scaling commercial resources is one of the most common operations done in AWS this moment through everything you need to be able to negotiate equitablely we stared at each other.You'll work with when using Auto scaling or land bailers in the next version of the program we're going to look at different types of stuff from all over the world.I don't know, but see you next time there and welcome back to AWS developer dying and developing file surveillance is one of the most common issues and something has to deal with there are always different options in AWS for stopping your files and each Target a special case in this situation we're going to start by discussing operations then we'll try to get two jobs at the end of the EBS.Well, then we'll swing over to S3 to create an S3 machine and then upload objects into that bucket finally we'll talk about some important tasks with EBS volunteers and S3 without Furt Ado let's get to stop 2 benefits you three different systems EBS store or EBS Elestic File system or EFS and instant stores each were intuited at different times showing a sort of event and progress for AWS intervention in the first place for the first of the first two years of the year.But there are a few reasons why they're not best for General use anymore ec2 data on an inventive store that could move you off the volume but it's perfectly up to the user and it's created on an inventive version of EBS.It's the preserved way to store data with an options of options to the types of things you can agree this can show you the options you can choose when developing a new EBS volume EBS volumes are also available within the same system of data data EFS is an EBS but it's not EBS.You know, it's just that I'm not ready for this, but it's good for big data type operation and is more of a dedicated file system that isn't operating for a road area for an EFS project project project, but I haven't been able to test that EFS is good for big data type operation and is more of a dedicated file system that's available for the next round we'll play.We'll give you just some of these EBS operations in this context.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.We want to move the EBS campaign from this will be the third opportunity now one that we created many not one of the Auto scaled ones let's go to the ec2 drashboard to find this Click on the Instances Menu option on the left you should probably have two to three opportunities now that you created many and one or two that were Auto scaled one of themAmi name of a long string that starts with bitnami node when you find it noe which availability zone that it's in this is going to be important so that we can lay our new opportunity in the same ageful action we have a place where the incompetent ID and then move to your code Editor andpaste it as a string for the value of instance ID next in the params available in the new universe where it exists where it exists to be present.I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I just...I'm in a different Data Center and a different audience Zone even though it's sublow because it's staying with the AWS Network it could be justified to actually do anything with anything with CPU it needs a lot closer to go back to the AWS delegation now at the top of the top of this params object we need to add the image ID to use to make use of our new opportunity doesn't need a special Ami just Amazon Linux do so go back to the AWS management committee and you can clean up thisCome on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on, come on.No, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.Ids we will need the volume ID of the EBS campaign that will be deteaching and the inventive ID of the economy that we just created for the second value you can get that from the market line again in the left hand sideI mean, I mean, I don't know what you're talking about.I mean, we passed in then on the line after the end came forward now once the voluminous is determined it'll be available to be available to another source so let's apply individuallyNow, once I'm aware of the problem to assign the name that will be used to the market on your opportunity these values are usally things like slash Dev slash sda1 or Dev slash xvdas these are all kind of like Linux doing things we'll call some more interesting way than those others.Well, I'm sorry, SDF alright, so now on the line after the params came new when this it'll swap the bill in question to a new inquiry there's just one more thing to do after that after the return keyword and call send company pass in the company now when we run this it'll swap the bill to do that to the next instance there's just one more thing to do though we need to stop the new new new screen before we can stop it more quickly to do it more.Well, any campaign called from an Ami with a thing called a project code cannot be attacked to a running opportunity I believe this recitrant is in place because an instance starttates up any project code on attached charges are done to the instance and then they're used to make sure the product subscriptions and magazines are applied to any way to find out if a product is produced.If you get an officer so're we'll get an employee so're just going to go go ahead and do this but recently it's effective a better idea to stop an accident if you get an employee so're going to go ahead and do the job.If it's possible so let's go shut down the Instance move to the ec2 cashboard in the AWS Management Console Click on the Instances menu option and find the Instance we just created you can tell by the Ami ID that's present on the Instance it should say Amazon Linux 2. Set it and then Nick the Instative State drop down and then spot opportunities we could have done with this file as well but you know this is just a little bitier I'm sure you don't want to write any more code that is unnecessaryI'm going to skip in time a little bit here so you don't have to wait with me in your company line run the market since EBS returned because EBS vote.If you did that then have full access to all data that's on that volume there's price more that you can do with EBS but I hope that this gave you just an idea of the service in the next tip we're going to get started with S3I'm sorry, but in AWS of 10 years, but there are some extraordinary applications of that kind that make it even more useful for applications two of those things are affecting and if they exist, it's important that you be doing what you want to do or what you need to do.If you can always get the old ones of that file as a simple bit that can beI'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry.You can say that once a file has been made in S3 for five days moves it to either S3 infect or Glacier access both much higher price forfields that don't need to be accessible only you can use this option for precious files as well this means you can move them to Glacier and most of the time is having experience on your S3.With moving objects you can apply rules to only enforce versions permanency out older versions once they've been retrained for a certain amount of time both of these fives can add to the rootness and usefulness of S3 as the multi-created structure in the next version of the universe let's get started by developing our first S3It's the name that we'll start the name for our sucket now S3 Bucket names do have to be global unie which means that you'll need to add something to the end of this sucket name to make yours different from mine and everyone else's so close to the company after you don't know what you want to knowSo make sure your sucket name doesn't exist in URLs for some reason now let's introduce the clean game start by cutting a trust oldThe next problem is ACL all uppercase and it stands for Access Control List this basically defines the effects of the bucket and its effects here is one of the most dangerous ACLS that AWS offers which are just predefined sets of access to the current public Dash read as the value here and with our params availableYou'll see some outside until after it's complete and your baget will be sent if you really want to you can do something like this.No, the Management Console as well if you want to explain that the bagcrelation was subjectful in the next Clip we're going to upload some of our demo project status charges to this game S3 projects are great for stopping all types of objecets videos text files you name it's partly nice for state interests and what's we'll be using it for us'll be using the file uploads of S3 objects.To help you out specifically that's going to be laying files from your file system to upload the movements to this stage will only start theput object to S3 operation so start by replacing the string value in the bottom nameUpload S3 object operation is called on each file and then the e-tag contact is logged to that context that's so let's introduce the upper S3 object start by declaring the params competitorship that we used when we fucked up but this ACL file is perfect to the next file.At's being raised and since all the files should be exposed to end user who is the target public Dash read here that should be used in S3 we can use the file name that set the value to file.Whatever type is being covered from that file name and that's it for our params object on the next line line.We have done that yet for this demo project run this company will run wepack to complete all the charges and it can take a while it's not having any Output it's just going to sit here on this black screen but once it's complete it's back to the program where we can raise the risk that we're going to the next.Yeah, well, I'm sorry, but once it's complicated you'll see the outsider for each file that was raised our files are now an S3 yay but we aren't using them yet oh we need update our ass references in the motion project code sock over to the AWS Management case and go to the S3On the file name I'm going to use application.min.js this object URL value is what we want but we really only need part of the URL for our applications which is everything in the Link up until the file name so select and coppy that and then we'll use that with the asset applications and our project so move over to your code estate we only have two places in our application where we need to move to path for the satellites and the first is in the hands of the handlers5 we're describing a style sheet and right now it's recalling that style sheet from the public branch so move the public part of that path and paste in what you got from the S3 object make sure there's not a double slash there before a style sheet your pass should look something like this money difference is going to be your lucky name on line 7 do the same thing to the faffecton then on line 12 we're bringing again file again again in the JavaScriptYeah, bring the other place that we need to move the path in the util fronter and the file is assets.js at the very top of the file there is a base next being created and it should have public right right now you can move that completly and past the S3 URL that you have just made sure that there is not a slash at the end so that the final path for those whose eyes are created correctly in the next version we're going to look at some limit that you may run into to the EBS and S3 both EBS and S3 have rights that you might spendWell, actually hit a little bit more than some of the other services that you've seen with EBS I've already mentioned at any time the opportunity has to be stopped an EBS campaign must be stopped in order to detach a root volume if the volume isn't a root volume then it can be detarched at no time to be stopped at any time at any time if it exists at the end of the list.I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.Well, it's not just unique in your account and that's most of the money you may hit in your daily use with either service there's no limit to either use for file monitoring and we were only able to cover a small amount of styles here this moment has given you a good overview of EBS and S3 and hopefully spent your impact for what you might want to use for them for we started by talking about the trade-off between styles for EBS and economics and economics for EBS.EBS was the best for us through then we went through all of yesterday and then onto an EBS campaign between two different subjects next we got into S3 traffic and created our first sucket in the next picture we're going to look at another type of service in AWS as we look into our state assets from the demo program and RDS and us.And I'll see you in the next moment there and welcome back to AWS developer dying and developing along with Community and transition Dataes and power are the third tall upon which stands the balance of AWS there are different approaches in AWS and they continue to raise and add the new ones every year the key Solutions are dying for nosql and Quick Document Data data systems and the new ones.We're going to start by looking at how successful throughput and subconsciously works in dynamodb then we'll sell then we'll work with the SDK and then we'll work with the SDK by counting a clock and then using that to Cache next week and we'll look at each other.I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.I want to start by going over how to talk through some trouble though and then go over each type of capacy move for dynamoteb tables we'll start with a few things like these over the past few years so may be some surprises here for you so let's talk through some troubles we'll raise in this dayI'm sorry, but I'm sorry, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about, but I don't know what you're talking about.Well, since one thing will have one of two things happens and you won't be able to really guarantee which will either right will turn a claim through the process or bought product will be affected and the right will be through AWS documents say that we can't use it but there's no argument that we're going to be able to do this.So you'll like only see that Error unless it's an extraordinary case now looks at the records on them've been introduced let's say you want to all get six things at once to do a scene where it's going to happen with one of the most important things.If you're doing an eventful task you're doing before you get ahead of AWS realizing your plans so each unit will give you two reads instead of one read if you were doing a strong concern reed you would only get one read out of that unattached the consultations hereI continue to get you the report but it may not have very recent changes to it since which is quite relevant now we'll just run through another quick application let's assume we've got a list of six items with some binary items they're about 20kilobytes and many of them will return to the United States of America.We're going to take three real units each unit in this context will hold up to rightkilobytes a 20kilobyte it will technically use 2.5 units but AWS runs up so we've got three about if we're going to read it with three different subjects now it would take exactly five because each unit would only account for 20 years for fourkilobytes let's look at how many right pieces would be involved in the next 20 years.It's a lot more than looking for sure now let's talk about the capacitity modes in dynamodb there are two moves progress capacitation which is the essential mode and on-board move the new mode that is more flexible let's look at progress capacitation mode with prognosis or become more involved with AW countries.I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.But it may be more expansive depending on actual use on-demeaning capacy also needs you don't need to need to concubine anything it's a true set it and forget it on its capacitation mode and that's about it hopefully those examples have given you an idea of how throughput is calcula in AWS and the capacy move review has given you some idea for negotiating your tables in the next area we're going to talk aboutDynamo keys and interesteds in order for dynamodb also certain it's called the hash attack this key is passed to an international threat and the outside operation is used to assign the item to a party where this branch must be located in yourtable kind of like a strategic key there's another type of youI mean, that leaves your party key be no-unique and that's by asking a sort key also sometimes called arche attack in the case that you define both of those keys Dynamo will use the party key as input to the hash fill to find which part to use and then it'll use the sort key to use all of the items found to be located both of the ones that make it possible to be very flexible when you can deal with how many hours you can sell but some of the kids are meant to be able to work for us.Your data in this case you can use a second periodic analysis with a global policy index you remove a new key child to orient your items and query on you can have a party key or alternative key in order you'll understand where you want to be in the world if you're interested.Well, I'm sorry, but I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I...It's not true that you don't create problems or deal with an extraordinary new set of things and with both of them.You don't have to do anything in the next Clip we're going to put some of this information into practice as we create a dynamodb table the first step we need to acquire a table in dynamodb that we can use to store data in our case we'll be creating two tables to store the data for hamsters and races in the scripts06You know, we're both gonna use the same key schema which is how we're able to do both operations so simply like this so let's go ahead and introduce the new version of that classable trade start by creating a perfect object to create a Dynamite table that will take into consideration the schema and keys of the table as well as the table as the perfect basis for the first version of the program to call table name and give it the table of the table name name name action argument not here to present the table of the table.If you use different resources you can use a table name as our key inside the near future an object is available and the first proposal here is worth one name which will give the value a price with the value of the next individual.We've got three options for holding n for number or B for binary in our case our IDs will be numbers so entry a string with the value of an uppercase in that that's the only attempt we need to defend ourselves against the first problem here will be given name which will also be given the value of an opportunity here is where we Devine ourtable Keys has been on the average to match the needs of the victim.We'll add the last product through which is an object adjective add up with the value of the value of the benefit right coverage also with the value 5.When it's ready you should get some outside on the second tableWe'll be able to use them in the next Clip we're going to populate those tables with some data that our demo project needs to use an Empty Dynamo table is worse than an Empty S3Tell you what I want to bring another thing to your attention if you look at the company making it work with dynadites way by handing the serious ideas and dying from mine if you've used the dynamodb document from two of the AWS SDS.We'll use a dynamodb document instead of the rest of the community to send the company now let's introduce the populate version of the paper to populate the table of one's own family and then use the battery message to write the items at once we'll start by declaring our boundaries.We'll build the value for this beautifully so give it the value of a call to Data. Map and pass in an arrow action with one argument called I inside will turn an object into an object with the perfect putback as you can add another type of i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-I-iNo, this object such as treatment is because we're using the Dynamodb document record it'll take care of the items to a new Constance committee and give them with this item for the latest will be built out worldwide one time at the next time.You call of battling writer passin' on the line after that return keyword and call end Dynamo item passed passin' in company and that's it for this screen so let's go ahead and try it out in our company line that we record in the last video should be finished developing by now so go ahead and run the company freely moving over to the AWS team.We're going to contract our project up to this table that we just put there in this cap we're going to look at two of the main ways to get data out of aMes a very few and Powerful data there are some others for retrieving data that we'll look at in this crip and those areYou'll see with the Imports that we are using things from both the dynamodb company and the library we're describing the regular dynamodb company and then the dynamodb document and some company will be working with Dynamothem because we'll be working with Dynamite teams.This means that if your table has more items than that you'll need to do much more to get all the data a single series of numbers that can fill into that number of items that you can add to the list.Well, bye and then it'll return a start key to use with your next skan for our purposes then we will only have to perfect one scan since our Data set is small start by creating a barams competitor call for the skan only one more time after then give it the value of the table name fun action argument then in the new line line demand from the next source of the new Constructor call for a new complete pass in the params after that timeI mean, you know, you know, you know, you know, you know, you know, you know, you know, you know, you're like, you know, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're like, you're gonna-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------That we're passing in the table name and an ID it's true with these arguments we could just do a call to get and immediately recall the recvant mine but this is also a great place to use the Query in place so let's do that by the ID start by declaring a params compjet add the table name perfect first and give it the value of the table name name action obligation the next effective disposal of the query we're using a proper claim to make a statement the value will be a question on the Queen.Key so emerging with the value ID equals coron H key this query will find any records or the ID will be owed whatever we set as Hekey before H key indexes that it is a debtable with the next product value value this will have the value of an object and the value of the ID.I mean, it's important to work with the key beyond us return it so on the next line beyond a new request after an equal sign at night and then call sent pass in the next line on the next line and the next day.Well, when you use a querry recall is an argument for this get fun we just want to return one thing which would be the first case in the course so then return the first item withbracket notation and the index of zero great action our just movement are complete so let's use them to move up on One Directory and open up hamsters.js we want to change the sources of our hamsters andI mean, moving the right hand side of the administration and removing it with the require party calls passing in the string dot slash Dynamo now save that file and open up races.js and make that same change to line one to import that DynamoYou know, there's a lot of work to do, but I suppose it could really live in either Dynamo or RDS -- or RDS -- but RDS -- but RDS -- but RDS -- we'll use it to stop the user and user Frances data from the crashes 06 solder open.js file in this file as soon as possible.It's not like we're going to do this, but we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it, and we're going to do it.It's kind of funny because they already between calling this value in an instance class or an instance type but it's just the same as an instance type like we can use the last type of thing that you want to use to use you to know I really want to use our own Aurora Dadabase for the future but for the future of the DB name and i i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-ii-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-i-I mean, it's not easy for the free Tier and it can only be credencerts not individious instances so it's a little more complicated instep let's just set this to the string MySQL next after a proper name DB name and give it the value DB name is the next product is vp's Security Group IDs there are actually two types of security group to be used hereOh, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God, my God.I mean, we are setting them out because that's what's already written in our code now in the next line line created a new call demand and then assignment to it a new compruct call of new DB intervention passed in the programme then in the next line return a call to send RDS completed pass in the company's future.I mean, you know, you know, you're just supposed to take some time but we do need to assume some code in our demo application to extend this date so let's just do that back in your code expert open the file slash Data slashers.js place of concern to the database literally the demo appuses or even to do all the database interest it progressing that you treat like yoYou're accessing plans on line one and two of us consider the user and user copies from sqlitze these are mostly pointed to a mock so let's update the record for user change it to re-rere in the parenthes do do doth slashOnly change necessary here is to change the value of host to the actual hostname of the RDS because it takes a while it's possible that this opportunity will still be in the process but as usual I've applied my video in time a few minutes here and you can see that it's already finished you may have to use it because it's already taken a while.You know, he video and come back when it's finished writing Scroll down to the endpoint value and coppy it now back in your code Editor paste this value inside the string next to host and that's it for the changeover and using for cutting and using an RDS database in the next tip we're going to negotiate a redis standard in Elastic cache adaptation services that are partly used to web applications there are two sources available and memcaches redis and memcache that can be used with economic useAche, but over time it really seems that redis has come out as the clean better solution here we're going to create a redis cache cluster and then use it to Cache the user session in the demo application that our application can afford to scavenge to a whole lot of us without introducing application into the screens 06 standard.js file on demand we need to develop here is the new record record record record record operation the only arguments take its name for the list and an ID.Well, I'm sorry, f a secret group for the cluster we'll start here for the next product node type this is like the ec2 introsity type we'll use the smallest available for our customers add the string cape.You want to give the cluster for redis the value must be one so just admit that then then then add the advances Security Group IDs and set the value to an end with one of its name sgid and the next line created a new call passed in the next few years.Now we finish the story so we can run it by wanting to get the most of the redis cache to be able to add but there's some things that we can go ahead and do I've already written most of the code to use the redis cache in the application so really you just need to make a differenceI'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.We need to find the host of the redis cash charge that we created so that we grow on the left hand because the cache record can take a while I'm going to skeet my plan ahead a few minutes here and the cache is now available so in your AWS management contract to go to the Elastic Dashboard and Click on the redis monus on the left hand sideNow you can save that file and the demo app to use it now it would be really great to be able to try out all of this stuff literally in fact the dynamoteb and RDS stuff can be run very quickly but unfortunately we can't access a redis standard running in larstic territory or a Bast could afford a jump box or a lot of things to enforce it but it's just a little overkill for this course in the future.We're just going to upload the code deploy what we've got local to our insurance in AWS and see it work remotably in your company line navigate to the root of the project I've created a deploy compt that will push your code out to your instance and get it running all you need to provide is an IP so go to the ec2 dashboard and the auto groupIf you make any progress you may need to raise some of the values and deploy.js now on running this company willzip up your entire code upload it to the present.tion you should be able to try it out then in your browser using the looseball DNS name is a lot more touchy subjects and requires both directly involved in working with any of these persent services extraneous RDS and elasticaches have some more symbolic measures than the other ones and take a look at the least of the others and lately.You can only grow up to five Global seconds don't have this right since reading a table with seven different ideas must be created safely more than one cannot be created at the same time RDS has lots of data and data of the population.I mean, which is in the middle of the world but nothing you wouldn't expect there are some responses about database and using but they change with each database engine so you'd need to research those your own legitimate case side to RDS has a limit around the numbers and none of these are soft limits and can be drawn with a recall to AWS the freedom to remember is that you can't directly identify from the VPC it's created in between there no other StatesI'm sorry, d mention with so many different options for application in AWS worldwide has been accepted and will help you then make the best choice for your use of case let's recover what we cover in this movement we start out by going over some fundamental campaign through throughput and second-views then we created a solution with the RDS.I don't know what I'm talking about, but I'm just saying, you know, I don't know what I'm talking about, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you know, you're kind of, you know, you know, you're kind of, you know, you're kind of, you know, you're kind of, you know, you're kind of kind of, you know, you're kind of kind of, you know, you're kind of like, you know, you're kind of like, you know, you're kind of, you know, you're kind of looking at how we've done doing things with AW, but you're kind of doing things with AWS, but it's kind of AW.We're going to use both of our projects in different ways so let's look at some of the moves we'll cover in this movement we'll start by disassembly the fields of the road.'ll see how to negotiate a host Zone and recall set in Romania 53 to support a domain name next I'll expect what API Gateway is and how it works then we'll go about creaking consulting and deplaced an AWS resource there's more extensive access to some of the many lives with Lotte 53 and API Gateway let's get started while Route 53 is used as small placesI don't know what I'm talking about, but I don't know what I'm talking about, but I don't know what I'm talking about, but I'm telling you, I'm telling you, I'm telling you, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.I'm sorry, but I don't know what I'm talking about, but I don't know what I'm talking about, but I don't know what I'm talking about.I don't know, for each you'll deal with the region and the record set to match the region of the resources then when a user requests a domain Route 53 will determine which will have the lowest latency base on the available data that you agree and the IP of the restore so it's a pretty objection way of dramanalizing latency a global rule is small to latency laying treaties where latency exists Automatically lies on lowlencyWhich resources will go to which jurisdiction belongs on board or satellite which is a state or a programme where you treat over who goes what resources and is like very useful for legal or generalizations that will be settled to the next level for the future.I mean, I don't know if you can be able to find one for whatever you need in the next Clip we're going to contact Romania 53 for a domain name 53 has two key functions to know when setting it up to a domain name the first is the most famous Zone this is basically the Domain name without subdomains.As a result, there will be many records set in it in our case we're going to create a host zone for our demo project and then complete a list set to send us to do in this file so if you do want to burn and treat a name and use the name of the future.This file is simple with just one method that we need to describe for this area that we'll be using the Route 53 Center and as already I've already impressed the end medium in the list of your own names which you're looking for.Can give the value of the HZ name argument Above we're passing this in as Hbfl.online if you wanted to use your own domain name just update the string for the HZ name claim the next challenge is caller recall this is a unique value that changes claims to create zones it's requited must be a string and the economic thing to do is to just set it as a timestamp for the value hereIt's just a quick way to get a time stop as a stocking and again this is to Avoid duplicate missed zones being created and that's all we'll need for progress and the next line created a new claim against it a new competitor call of created zone passed in the pastNow we have a host identified we can add set and your code attend to the file created 53 record.It's important that this value is valid since the root set has to be applied to the right host zone for it to work now we can introduce the new record set creation a new contract name and order to an object make sure to destroy this company because we're going to need that.We'll be using a change company which can be used to create or deletrecord sets for a host country where it's a different choice than we've used before and Route 53 is pretty much given in this way to give the most important of you.I mean, the value of an obligation with a single request is either we will assign the value of a price with record in all cases the next product is available now which has the value of an object now finally this is where we will get to the next-rate development of the current system.I'm just going to enter the string hbfl.online, so I can record this record for the root dotdomain the next product is type and this is the type of DNS record to negotiate we're going to do an answer so you can access the upper case letter A here in QuoteYou just point to an AWS resource and Route 53 will figure out all the IP and building stuff add the product Alias Target and give it the value of an object the first product is DNS name and this is the internal AWS DNS name in our case we're going to send it to the road baller that we've created so's going to the ec2 range as a stone to the DNSSo we can just set that up to last year's progress and we can use that shot ID even though they are called the same thing don't get this condition with the shot Zone ID at the root of our rooms.I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.I think that might be the longest company name we've used so far on the line after that you can just return a call to send company pass over to your company nowYou should get some Jason output like this showing the status as sending a quick note here because these are actual DNS changes they can take some time to project it's not into some of the other changes we've done if we switch over to the AWS Management Console and go to the Ruthe 53 dashboard you'll see that we have our shot Zone here and clicking into it now we just created is also here even though it shows here it doesn't mean that the change has produced so even ifYou're using a real Domain name this a record may have not implemented out into the public interest yet and that's all for Route 53 and the rest of this movement we're going to look at a different way to use your application with API Gateway API Gateway is one of the newer AWS services and its records an evolution of what exactly is so close to the futures of the world.API Gateway it's available either part of a URII'm using this example because you could see how you could find another path on that same family name to other applications now let's get started with developing our API in the next versionI'm going to walk you through it through our screen files let's get started by working the new API and API Gateway are stupid like you have your API has changed the API's rules so powerful.Well, then each resources can have one or many methods a method has two main parts to it the integration request and integration where you can manage the Data going either way keep all these pieces in mind because those are what we'll be considering with this file so let's look back at our code here we're creating the root rest API save the recovery to a core so we can use the data from the root ALI later in the codeYou want to create a resource and an API you have to give it a priority resources to Branch from in this case we're using the top level resources since the root API to Branch from then we create a solution called Hbfl this will be what our demotivated Hbnds on we have to create the methods and integration for that then one more source is actually a problem it's basically a catch-all for any recent Beyond Hbbl's will be used to use such things like API calls to do the API application once in a few years.Well, we're going to need to introduce all the operations called in this exaction so we'll be working in this file over the course of the next few days let's start by introducing the new API first removes a params priority area there's only one perfect need here with the name thing to note with all of these API Gateway companies that are in camel case not every case likeAnother AWS service I'm not sure why they made an exception for this service but in a different application 2 of the API Gateway company they've returned to Pascal case so it seems like this might just be given a Whoopsie gift of the value of the API name argument after the panels became a new concern and that's for creation.API the next funnel we'll demand which will get the root source ID from the API we just created the new rest API brings back the ID of the API but not the ID of the root resources which is needed to find other resources to that's why we need this funning start by creating a params next subject this is another simple object with only one progress with the key next API which will be the API dot dot quality from the API foundation in the line after the line.We want many more before return from this operation so many other concerns and then associate calls to send goods in the next line of duty and we're going to go through the resources from our source to find the resources that we need to find.I mean, the path this will identify as a source of one argument named rr on the right side of the archer r dot pass trickle equals singleI see we can create a new source off of it by appreciating the new source support start by collating the current phase and you can give it the value of the resource course support and this is the path that the source will be consulted to continue on the final API URLI'll be given API ID which you can give the value of API dot ID and that's it for the params necessity on the next line add a new cons name after an equal sign adjecting an entire claim in the next line and then return to the next line of the world.I'm sorry, but there's still more to do in the next crip we're going to clean up a methods an integration for a resource methods in API Gateway are where you decide how a user will actually engage with your endpoint HTTP methods with a single HT mitthods like getting or different Integrads are assigned to the transferor.We're getting started with API Gateway Methods you're able to create sources by using IAM or even your own action we won't select our endpoint we want it open to the public soaring none in all next editions are HTTP mediums to give the message to the people of the world.You can give the value of the resource ID fund support assessment this is how to give this method to a source the last progress is due to API ID which you can give the value of the API dot dot ID and that's for the params object in the next line to create a new claim from the market but there's a new Constructor calling for no temporary pass pass.Well, it's not much to build a methhod either stuff with API gasteway coming into play with the Integrations let's go ahead and move into action the new methhod infottle start by creating a params condition object this one will have the most likely problems in this fileWhich you can give the value of the value ID support both HTTP method and resources ID together tell this together which resources method to at least each of the next progress to the next progress is which you give the value API dot dots then add it like a La La AWSAnd we're going to use the HTTP program when sending applications so easily over to our local bankers so add that as a couple of years ago and get the DNS name from our home bankers in the market for the URI.I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I've just-------------and------------------------and------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------Well, remember coming on the HtTP endstreams away all of the original headers and only ends specific ones from the integration sometimes are this might be what you want but other time you want to present the inditive response to API Gateway and just pass it completly over to your HTTP source on the other side of this application is called a program in o-o.We have valid endpoints on our app as API type endpoints like getting the easy source so if we call something like hblWell, I'm sorry, but it's not what it is that we use the same methods and influences that we used to use but we made an attempt to advance progress as the last argument you'll also notice the HTTP methhod we're doing none of us.So because we're using the same plans that we've already introduced we'll just add some truth to the new code and the new code of conduct that we want to deal with as a real problem.Well, I'm sorry, but on our parents' books, I'm sorry, but I'm sorry, I'm sorry, but we're going to be fine, but it's going to be fine so important, and it's going to be true, what is doing is telling our source method that we're going to have a balance on the URI path of the United States of America.It's going to be proxy this is basically how you definine parmeters on your path for a resource method and this is simillar to how you would add query parameters if you were going to use them now we can add the new Behavior to the new methhod integration plans consort review if the path is truth and the truth is true a black just like we did in new sourceThe URI intends on params so enterer params.uri then add itself to add and access to it then back picks to new astring topplace inside the back kicks open open open open doors open open open open doors on our URI on the next line.I don't know what you're talking about.We're going to be able to run it now which will be beyond everything like this we can remove the new API stateway.I mean, we can actually try it out in our breaker phase and API Gateway is kind of unique there are two key Concepts stages and deployments API Gateway lets you create days for your interventions think like development intervention stays and progress you can remove your API to different dates to have a healthy development life life to a phase to a phase a deployment is new these are saved in their own history of each stage so you can move between them and everyone else in the world.Now we need to depoy it so open up your code Editor and open the file deploy API Gateway.js first we need to put the ID for your API in this API ID claim to get this to go to the AWS Management Console and then the API Gateway Dashboard you should see your API Titled hamster API in this table there's an ID code with the API so just copy that and then back in your code place it inside the price for the API development of our API asasWe'll add the first program which you can give the value of the API ID fighting immediately next to you in the next line which you can give the value of the stone name actionAlled company assigned to it a new Constructor called for new cooperation passing in params then in the line after that call to send combined passing in the company now even though this is a new demand you can actually call it every time you make a change to your API every time you need to remove it each time the action is called API Gateway will take the current state of your API service and deploy it to that stage now let's move over to our company line and depolyAPI by running the company node deploy API gameway.js after it finishes it'll output some recsponse data to get the URL for the exploitation let's go back to our webbrowser and select that hamster API op in the API Gateway able under underI mean, the get under the Hbfl source you'll see a perfectly diverse link here which is where the API Gateway was deploited we need to literally imagine this link just a bit so close the link and thenpaste it in a new breaker window window in the next window we'll look at the end of the URL and then you can hitent of the URL and you right now we're seeing the demo project building from ALI?Now, when working with Rote 53 and API Gateway 53 and API Gateway are both concerned in AWS let's look at a few of both in Romania you 53'll notice barely that is a problem on how last set changes are done to the internet and this is buried on DNSIf you make an error and need to change that same record set again the change will be deplayed by around five minutes I've acquired this single time so make sure to be cared for when changing record sets as from there are the most important resources available but they can all be laid by AWS APS Api Gateway has around the numbers you can make to recover or delete some of these things into your memory.You can only send two new API plans here and it's a good idea to be aware of some of the issues it openly faces I ran into some way in the past away the ALI Gateway resources recall and spend a few months of a new month with the AWS fund.Well, there are the resources available, and those can be raised in this movement we looked at two different ways that you can get your web application to your users 53I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.We're going to look at two different ways that AWS executives you to deal with your clients the first is clarion every Network that we'll start by disassembly.We'll look at the limits and struggles with cloudfronts and S3 state subjects next from crudfront work in S3 and then we'll negotiate one base on our illegal S3 sucket and finally we'll look at the limits and struggles with cloudfronts and S3.You'll set a Max time to live before the cache retrieves a new version of a new version of the case from the orgin set the value too high and when you release new code then users are still getting the cached version set it to low and you're not taking allowance of what cloudfront can offer there are two real solutions to make the most out of a high time to live value the first is to invariate anymore whenever you've got a new version available this can be done by applying an involonization to eliminate the diseaseNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.Well, maybe anything about that has a price granted with it and to top all this off indexations can take some time they aren't instant while I've taken it only a few minutes to process an involuntarily that was with only a few files I can imagine it would take longer with more files the second solution which would work better for many applications using usThis is to acquire a name for the object any time it changes this might be something as simple as having a cash to the file name or a date time stop then when you push a new version of the news itself the new time they're request crudrons from the orgin and the same time they for the maxim time to live this has been my Chosen solution when it works with crudfrons since the first time they'reWe're going to actually create a clear understanding for our state assets we've already changed our code to directly access Assets in S3 so let's create a complete separation file just because we can get even better perfect from it regularly to the screens.I moved the two biggest sections into this file so that the actual Create file will be much easier to read we've got two sections here the orgin and default captors produce let's get started with the orgin projects this ormany program can hold one or man so many of the number of the items that we're interested in.And it has the value of an argument where the original defenses will go clean one after after that contour with DOT S3 dot Amazon aws. com and with that we'll build the domain nameWell, that will be true in the other params since after after an award in the word this will make an ID base on your baget name so it should be unique enough for our purposes the next product is S3 orig confuge which has the value of an application that is not available by us but it's recovered bycolludfront for an S3 orifice to work one of its children.I said it's unused now that's everything we need to continue for our first progress let's consult the original cache Behavior below to use that orgin these services do exactly as they say they relate to the truth of the truth between that our new career is meant to be true.Well, we don't want any of this Behavior add the value of the value now after the value values object add a new product call for Min TTL and set to zero this is the mine time to keep an item in cashforwarding the return to the next edition of the world to make it happen.This will let you create new URLs and only allow users to access that way we won't be doing that but this is another requited field adveld the most important thing with the number zero the most effective source of all.We'll Devine what HTTOs can use with this device we'll have two sowed a complete name quality with the value 2. then added a complete number of items with the value we'll add to all caps and head after all of it.Well, then add the perfect items with the value of an item here will copy what's above with get and head introduced here and with that our long projects are complete now we can open wide proliferation.Look, I've crossed our boundaries even though I've crossed many aspects to that other file we've still got Queve a few to ad here so start by asking a list of problems and giving an assessment of what's really going on.I'm going to put the next program this is what we're sending us to help you change our plans so name it something that will help you deal with the disaster.I like to set to the string HTTP 2 to make sure that it's true that it's true that it's certain that it's important the priceI'm sorry, I'm sorry, but we don't really need it right now but it's good to have and fine the last production is the most important it's available and you want it to be set to true if you want to be able to use this device at all if it's set to die it just won't work and that's it for the next line of the new codeNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.We can go and we can move our demo project first we need to get the new Domain name for our secret occupation which is available even though the disease is still depositioning so go ahead and open the AWS Management Console and go to the market Dashboard source by either the service or the service or the then copying name from thetable now just like S3 there are only a few places we need updateAnd then on line 5 Update the URL and take out any shit that's on the path just leave the file name at the end do the same on line 7 with the same line and then also on line 12 with the JavaScript now toI'm not going to be walking through you through them you'll just need to follow them on your own so you'll need to refuel the project local code out to your ec2 opportunity upload the files to S3 again the Dynamocy table and finally wait for the clean front to finish set you upWe did in the last mode, a lot of steps but our application is becoming real core with S3 is one of as's oldst optionsIt's just it's just like getting state assets a website is different because you can recall a path and some HTML will be returned to you don't have to return the file where the HTML is effectively in many ways that's what S3 is happening today.I mean, it's letting you negotiate some state Behavior when you can place these files in any location in your S3 business and they will be returned when that path is considered to be connected to a unit URL to use to get to the site there isn't very much universal.We're going to get a statesite set up us using S3 bag in order for us to get to try to get out a state S3 package and then get laid in your codeThe first change is to raise the name of your S3 baget for the value of the Bucket name claim now we can introduce the information supply by first creating the first object which is where we're going to tell S3 how we want the value of our business there are only two options for the future.But I didn't bother making one so we'll just leave that out for now go ahead and give the proper index document and give it an opinion as a value this tells what file to use as the index in the objectI just wanted to make it illegal how this was working you know kind of something for you so that's for the programmes object and the next line created a new constundant call and access to it a new Constructor called for putbite product pass in the books.Well, I'm sorry, mamand and that's it's pretty simple that it doesn't actually give us any useful output such as the Domain name that we would want to see the site so suit over to the AWS Management Console and go to the S3 Dashboard clean and then go to the site at the beginning of the story for a few years.If you visit it you'll be taken with S3 statesite killing again this isn't the full Hbfl article it's just a statefile to declare these S3 state cases in the next version we're going to look at some limits and response to global S3 jurisdiction and S3 list of how much AWS service has been givenA few credits to be aware of most of them just soft limits on the number of resources but there are some limits on how you relate and only a few dollars on a list of issues that you'll have to address if you require multicredits there is no limit on the number of questions that the number of users can sell and only a soft limit on the number of claims there are included in the list.But other than those few risks can be a little more of a special use of these days let's recall what we covered in this movement we started by discussing funds and why you might not want to use them.With all of those weapons next we moved on to S3 state systems and set one up on our dealings S3 Bucket and finally we looked at the few limits conceivably closed in the next few minutes we're going to look at inter-service company inside and developing and developing in this mode we're going to look at two different ways.We'll be looking at both in this movement and consulting them to work with what we'll be working in this place.- of the specifics of popping and sqs and how few works then we'll sell an sqsq to send race results and look at how to see some of the problems and problems next I'll introduce you to Kinesis breaks and we'll set one up for our application to send race results to the races table and finally we'll cap it off by looking at some of the problems and scales on a list of problems on a list of problems to the queen and the non-consumersWell, I'm sorry, f like the life zone of a couple of minutes with a few issues the process is simple enough enough the issues are just sent and the Clit points on, but polling is a different story when there are multiconsumers pulling a single Queue what happens for a reason for issues but then can't process them sqs has circumstances for these seconds which are what we're going to talk about here when a message comes into the Queue it may or may not become irrelevant to the Councils today on these issues.I mean, he plays seconds attribute this play seconds attribute will play the visitability of once a message is read in the context of a gift number of seconds when the message is still in the Queue but it's not visible to Consumers until the visitation doesn't run its course will stay in the memory and be i-i-i-i-i-i-i-i-i-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-I-INo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry.I'm sorry, y're polling for minutes this can be set up to 20 seconds if any pieces are available or come available while the consumer is waiting to return with the new minutes this is called long polling and it can be used because you are started for sqs by each recall made changes that make it equally important to read another message or to write a message so by using a barrage of space and how freely you realize that you want to make it easy for a good break that makes it almost impossible for me to remember a message that I'm going to write a message for you.I've actually done the work and will just be working in many parts of the world.So go ahead and start by creating the first object which we can give the value of the quartet which is the value which has the value of an object most of the benefits we'll add to the value of the value.I'm sorry, an incoming message can overrule this if they want to the next day if you ever need that number next add the number forever sighting time which is how long minutes will be invisible after after we're able to set it to the value 30 for 30 minutes after 30 minutes of the last anniversary.I know will set the long polling time of a consumer to the value 0 and let consumers decide whether they want to Long Pole or not now the new url of the Que which is very nice but we don't actually need to keep it to now for now in the next sphere we're going to deal with me.It's sort of a real-time operation I thought it was a good use of sqs naturally you'd want to have another service on the other end considering issues one to send one to answer but for our demo we'll have the same application end and recall issues let's start with the ascension and the libata mine there is an answer.I've had access to the full Q URL and we'll name a new con con and we'll call it params not regular params you can assign an object to that it'll only need one specific name which you can give the value of the QURL.Well, you know, you know, you know, you know, you know, you know, you know, you know, it's like, you know, you know, you know, it's like, you know, it's like, you know, it's like, you know, it's like, you know, it's like, you know, it's like, you know, it's like, you know, it's like, you know, it's like, it's like, you know, it's like, it's like, you know, it's like, it's like, it's like, it's like, you know, it's like, it's like, you know, it's like, you know, it's like, it's like, you know, it's like, it's like, after, it's like, it's like, it's like, it's like, like, it's like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, like, it, like, like, like, like, like, like, like, like, like, like, like, like, after after after after after after after, after, after, after after after, after, after after after after, after, after after after, after, after after after, after, after, after, after, after, after, after, after, after, after, after, after, after, after after after after after after after after after, that, after after after after after, that, that, after, after, after, after, after, after, after, after, after, after, after, after, after, after after after after, after, then, then, then, then, then, after after after, after after after after after after, after after after, after, after, after after after after, then, then, then, then, then, then, then, then, after, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then, then,And the first challenge is going to be most personal which is that we want to actually send and put on the queue we will need to turn the message into a new source company and attachment to it a new source call of supply request.If you want it to be different then we need that on the cube and with that we're done making our changes for sending messages and the next help will change stuff for the demo project now that we're putting resources away to those things that we need to pull for those things.And then complete the data with those records we'll make our first changes in the lib data lib folder with the sqs.lister.js file this file will pull for sqs messages every five seconds if there's something there that will make the master records in dying 'round 5 years if you remember the extension of long spending of five years of experience00,000 dollars a month so correctly do use a better value if working with real world applications this init fact is what starts the interval timer for polling and it calls the poll funnel the cube let's add the benefits by collating all the things to do in this file the first step is to get the Q URL so let's add the single progress for cuumbs will be valid name and you can give it the value of the goldI'm going to admit this is just one of the most important things in the world.I just wanted to give you an example of that now that's all the changes anymore for the poll operation once we return and process a process then we need it to destroy it so close down to the deletees fun action we need to add to the end to the end of the deletation service brought the first product is Q URL which you can give the value of the Q URL supply of the next version of the new state of operation is ready to begin this year.I'm sorry, I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry.We just need to admit all the truth in the end.js file at our project root at the bottom of the file on line 87 there is a service line that calls the Q Init operation this is going to turn the case on and have it start pulling the Queue so go ahead and just save this line and save the file now our changes are done and you can run them now forward them up to ec2 but it'll probably be easier to wait until we finish the rest of the work on our KinesYou're gonna do it with the rest of this movement, not good enough for your application because case you could want to Avoid the polling process or you find that you want a more real-time solution there might be many concerns why sqs won't work but Kinesis currently might be perfect for you Kinesis stocks as a service for real-data transfers it records easily and can have multi-producers and questions it doesn't have fine-gramined control over how many issues are faced with Siglicits but that Simplicit is not the only one of the most important things in the world.Well, you might make it better in some situations let's talk about one of the key Concepts in KinesisWe're going to use a Lambda as one of the consumes is that because a stone is real time it requires a dedicated consumer and our Web application is not really the right fit are a great fat as Kines consumes so this will give you a step of how can work now in the next cup we're going to create our first Kines now that we understand what a Kinestis lie is going to go ahead and how it works let's go ahead and sell oursI'm open the scenes 09 filer.js file in the screens 09 file this is going to be another one of those very simple files which are the number of hours that will be created in the Street you can move this value on if the world needs to record its needs but we'll just enter the first page 1 hereWell, it's true that we can give that value of the value of the story claim and that's it for the stamp object on the next line you can declare a new claim combined and assigned to it a new Contractor call of creation demand and pass in the params later that just return a call to send company and pass in that combined claim and that's for this standard you're defunct getting the hang of this now let'sIt's gonna prove it's worth and it has no use for whatever purpose in the next counter we're going to start racitations to this Kinesis field now that we've created a Kinesis field we need to start sending records to it let's start by just looking at a file to the lib family that isn't here before and open the solution.This is what we have a constant feeling when pushing data to Kinesis these values will be used when we call the target action so let's apply that action by operating the keyes.I'm doing more and more code for you I'm sure you're getting Tired of me reading the same things over and over so the first production we're going to put here will be Data which is what will actually be sent to the Kinesis stage this need to be a Buffer Dot and then need to call Buffer Dot from those sources for that we need to keep the message.So called json.stringify passing in the message fund protection which will have the value stone name it's pretty simple just point the data at a stage and put it up there we use theput record to actually send that message and that's all there is now in the next pageWe're going to use a Lambda operation as a convivial kingsis store what's important about this is that you can relate a Lambda operation to be truer meant that you don't have to create some long running application to all of this Kinesis field Data operations are involved in the story so much so the game will start coming into the Stream so we'll start fighting our Lambda operation in the field.So 9 years open up the market room and go to the Kinesis dashboard crack on the number in the data series section and the then crack on the hamster rsources option and then you can copy the Arn which is found here nowNo, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no.This is what we needed to know that Arn from the beginnings dashboard for the next program is funning name which is the name of the Lambda function giving it the value of the Lambda name action emphasis the next product is staring position which tells the Lambda action at which it should start progressing from the beginningSo, as a stringer since this operation will be set up before there are even any records in the Kinesis Stream so late is finely added that I do want to show you the code that will be running on the Lambda operation so look at the Lambda Kinesis consumer filer and open the index.js file the file.You can see how to get the actual data from the Kinesis record that you don't have to go to Kinesis in any other way it will be a base64 encoded builder so does need to be done correctly before you can really do anything with it and then the LambdaThis will take some time to run because the rale culture isn't immediate so I'm standing up on the video here a bit and then you'll see the output for the trip record in the console once new you don't need to do anything else with the LambdaYou'll just have to recover your application to the economic use of the same alternative use of the user name Ryan and the password pass and then you'll pick the remote next to the username and the Run satellite if you've already run the satellite.We want to start fresh and that's it for sqs and Kinesis streams in the next Video we're going to look at some of theties that exist with both of those sqs and Kinesis treaties are high-levelly protected at special levels at the level of niches and conferences and of both of the systems have worked and the resources around them are available for a worldwide application in the first few years.I wanted to mention here the first limit with Kinesis scale is the limit to the size to the size of the data blobs you can send through a Kinesis street they are state at one mechabyte this is basically controlled by the size of the sands making up your street if you have Data stretcher that you'd either need to break it into places or send records of S3You can perfect each second and that is one q and Kinesis existing you should be able to find a solution for any massing you need what sqs progresses a more multi-strategized and decoupled for all of the squiding times and Kinesis projects real-time and high-risk projects we can't find a solution for what we need.We started by discussing some of the losses of sqs like delay Seconds and vision without on issues then we created a Cue and changed our app to send race results through a queue to update the hamster's able to talk I introduced Kines streams and then we again sent rsources into the street while the Lambda action concerned them on the other end and updaed theWe're going out with a bunch of services that will within the real-time nature of your applications sizeless service or SNS is used to send notes to users Via SMS e-mail and HTTP and many other ways a user tracker of SNS is on file.We'll move our project to publish results to the Topic and give it a world next we'll create a secret system that we'll have all known.Hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey, hey!I mean, I'm not sure that this will be the case, but it will be the case that we only need to introduce the new technology in this file so let's start by creating a new name params and assign to it an object the first and only perfect for this object is going to be named which you can give the value of the title file as the complete object is actually present as this is actually the original product for the new state of the country.You should get out with this technology something that we're going to need later so don't get rid of this right now.There's been trouble and now that we've got a topic we need someone to subscript to it which we'll do in the next Clip one of my favorite things about SNS is the different types of silence options that are available you're not only able to send emails but also SMS hit HTTP endpoints trigger Lambda operations or even send the messages to an Sqsq we'll keep it low-key for this course and just set up an SMS notification we do want to know what happens with the hamsterOne of you're welcome then on the next line we need to enter the area of the world we just learned you can get that from the company line outside from where you grew the topicy count right here and hereYou can give that value of the technology Arn operation the last product is available and you can give it the value of the value of the end support that you need to deliver to the end of the world.I don't know, ribe to a point on the next line created by a new constance called signed to it a new Constrector called in params and then after that you can return a call sent company passed in this will make our file available so let's swing over to the next line to run it in your company line excute node late SNS subscription.I've prepared some of what you need to do this we just need to introduce a public action in your code expert opinion to the library fund and open the suns.js file to the end of your life.You may still have it in your first line but if you can go back to your company line and copy it from the new technology out now let's present this public operation lie a param's next object on the first line here just like the first line and presentation that isn't many offers the first will be given to the MSG operation nowWith our clients let's publish out the message and the next line needed to create a new consultation call added to it now in the new Consul call we're going to negotiate after that call to keep our application safe when you're working with many AWSGreat surprise is the greatest group that we consider for that case there are demands of different technologies for different sets of subjects at least us set values on this point we're going to set an entire base on our land base to send a message to our SNS to the United States of America.Below one of this basically means if something happens to our application 10 years we'll start by killing in these times to do what we need to spend that SNS time again for the value of the world I'm kind of thinking that file and the Scripts 10 years we'll start by killing in these timesOre so I'll show you where you can find it in the AWS Management Console go to the SNS Dashboard Click on the tops menu on the left and then you should see your technology there with the Arn in this right hand code if you coppy that value then you canpaste it in the string of value of topic Arn next we need to get part of the stargate group Arn and ad group Arrn from the sources that we created the bigger of this class so he back to the AWS Management Console and go to the ec2And then we're going to just grab a part of the Arrn not the whole thing from Target group slashWe're going to sell everything from App slasher lb to the very end and copy that little section then back in your code that you can save as a price for the value of the lbvariable now that we have all those value values we can get started on implementing the Cloud Watchwork firstI'm sorry, I'm sorry, I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, but I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry, I'm sorry.This lets you deny how many years perhaps to exist if the value is less than the threat before going into the entire state in our case we want to know immediately so we'll set this to 1.I know that the mercic is a part of the value for this is a string with the value AWS slash application elb the next product is perfect which works with this value evaluation which works with the above-mentioned aspects of progress this says how long a perod should be defined as in your history that is valid is five minutes but I actually prepare one minute for this value should be in seconds so enter the number 60.That you secured we'll enter one here since we want the Alarm to go into Alarm state if it's less than one of this will basically be the actions the entire takings when it's serious enough to enter the field into the field of the field operation here that's the fact that an SNS intends to know all its needs and it needs to knowWe need to enter both the tower group and prepare resources here and an object then add the property name with the name of the Target group as at least one of the names in this name.Well, I'm not sure you're going to be able to do this right now and give it the value of the value as a stocking other possible options here are minimum maximum som and Samble count as the progress also works for our parents and the financial benefits are worth dying data for this purpose.Oh, this means that if there is missing data if it can't tell how many Healthies there are it should triple the entire other possible options for this are important not breaking or missing the reason that we are going to be putblaching here is because if you reach ahead and that's why it is happening on your side of the roadThen you can just return a call to send company passin' in the company and that is all we'll need to create our entire team let's run the file by swinging over to your company line and exiting none of the new crew almost done and you don't really get any useout when it finishedI don't think it's worth it when it comes to SNS or Claudwatch anyway this is mostly due to their small operations practice does have a few more than SNS so let's take a look at that do Exist one struggle to be large to get on with SNSIf you need more action, you could create a new cludwatch Allam with the same metrics but that might end up being pretty messy in analysis a crud watch alrightperiod can only be a maxum of a day remember the person is used to listen to the alright should fire or not if you're looking for more time ahead than day after day thenYou've got a lot of time to look for another place to figure that out we all know zepto is a winner though and that would never happen so maybe this is just a moot point and that's just a moot point and that's for thousands of times in the next cap I want to give youI mean, you know, a guy who's got a lot of resources that makes a list of what you need to clean up after maybe be helpful so let's look at what you need to clean up and how to do it we'll start with the ec2Amy, we created and then delete both of the keys since there aren't any more live choices using them to delet the lanch template used to make our opportunities delete any EBS snapshots that were created and any EBS volumes that are still around everywhere before you're really about the restaults of the Petco 2000.Now go to dynamodb and delete both then go to elisticache and delete the redis recorder we created you can go into Route 53 and delete the record first and then then then go into ALI Gateway and destroy the hamster API that we created now to cloudront and delete the hamster demand you do to the Kinesis field and then once it exists then you can actually die it're almost done and we're almost done into the sqsIAM but they have no luck so you can delet them at your trial and that's it's hard to believe we can deal so much work in such a short time while SNS is powerful and quite spent with CloudwatchI'm sorry, the true state of reality is realized there are an endless number of uses for both kinds of services so I hope this moment has touched you to try out things with each let's take a look at what we cover in this movement we started by beginning an SNS to make then we made an SNS proposal and then we made a team together and looked into the world.Well, it up to noify us through the SNS experiment finally we looked at some of the few limits on SNS andcludwatch arms and we've reached the end of this course AWS development and development application you took thisJourney with me to learn more and more about AWS at this point you're in a good place to start doing some damage in AWS find a personal project or side project at workI'm sorry, is in the AWS development committee path to prepare to make the AWS development agreement extra in my name is Ryan Lewis and thank you for watching this course\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Replace this with your own checkpoint\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-zh-en\"\n",
    "translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_length = 512\n",
    "\n",
    "# Split the input text into smaller segments\n",
    "segments = [transcript_text[i:i+max_length] for i in range(0, len(transcript_text), max_length)]\n",
    "\n",
    "# Translate each segment and concatenate the results\n",
    "translated_text = \"\"\n",
    "for segment in segments:\n",
    "    result = translator(segment)\n",
    "    translated_text += result[0]['translation_text']\n",
    "\n",
    "print(translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50374a98-eab9-488b-8c2e-b8ec32d2d1b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65cb92eaaec14894861946515b445f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()okenizer_config.json:   0%|          | 0.00/2.35k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7833929c1f28476781e3603458cad52c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()/main/tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3a4d9c6ab04b6c95584c1930022112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()cial_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a90e1fe07e7d4cb2a522359ed685f36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading ()lve/main/config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad16ff6f648844c78d165622a31c2d6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 100, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this course is intended as a follow-up to my previous course AWS developer getting started I will expect familiarity with many of the most common AWS services so if you haven't watched that course yet then I definitely suggest taking the time to work through it with that being said this course does cover many services already covered in the previous course the main difference is the approach and details that we'll be covering the getting started course's goal was to really get you familiar services like VPC and iam for this course we're going to focus on taking a web application and integrating it into AWS in the process we'll create and modify services using multiple different methods such as the CLI and SDK I'll take special care to include specific details about each service we've interacting with that should help you understand how to design your AWS architecture for your own applications developing an application in the cloud is not always quite the same I've experienced this personally over the past few years when I began studying AWS development and it's only improved the quality of my work I like to describe Cloud development like this basically when you begin thinking about a new application you'll want to start with the infrastructure there are certain decisions you've made fundamentally shapes how your application is structured and this approach is what I'd like to call Cloud development with each service that we cover in this course and when you approach  a better application the service is in AWS are grouped into domains in the AWS console you can see how they've grouped them by the application concern each deals with some examples are compute storage and database because there are quite a few Services it helps to group them conceptually and provide some context to each one in this clip we're going to talk through each AWS domain to help you understand what each one contains the first and sort of the most important domain is this domain includes managed SQL databases with relational database service and nosql data stores with dynamodb elasticash is also included in this domain even though it may be debatable whether it can be classified as a true Database Network and content delivery is a hugely important domain that is often hidden under the covers the networking concerns of this domain manage how external users access and interact with your application as well as how internal AWS resources interact with each other cloudfront rounds out the the security identity and compliance domain contains services that deal with managing secure access to your AWS account and Individual Services IAM is the main service here as it manages and provides the policies that affect nearly every service in AWS the analytics domain focuses on services that allow you to consume and process data in different ways from services like elastic mapreduce which lets you process big data to the Kinesis service which provides streams that facilitate data transfer any service focusing on dealing with internal data can the AWS SDK provides the simplest interface between your code and the cloud AWS distributes and maintains these sdks in multiple languages so you can develop in the cloud in whatever language your application is built with there's some other open source sks for languages that aren't supported already by AWS these are typically very usable but it's important to know when an SDK is officially supported so that you can understand the level of support AWS provides sdks for the current most popular server-side development languages in no particular order these are java.net C plus plus python Ruby go and node on the client side you can use the same JavaScript SDK that is used with node in mobile you have some interesting options . the course is designed to help you complete the AWS developer associate exam which tests your knowledge of AWS and the many different Services options and problems that the current AWS developer associate exam contains all multiple choice questions typically each question will focus on a single service and ask about topics service specifics or service limits there are also questions about debugging certain problems and giving architecture recommendations based on use cases there's no real way to know which services will be on the exam but the core Services of AWS like ec2 S3 and dynamodb are almost guaranteed to be on . certified developer associate exam which serves as a great intro to the exam the course covers the exam format how to study which resources to use and what the day of the exam will look like I recommend watching it after you've completed the AWS certified developer path to put you on the best footing for the exam in order for us to utilize a bunch of AWS Services together we need an application to work with for this course I provided a demo web application project that will modify to this application is going to utilize a multitude of different services to provide some robust capabilities for users starting in the next module we'll get this application running in ec2 and see how it feels when it's served from the cloud as we begin this course I'll assume that you already have an AWS account set up if you don't pause the video and go sign up for one now new accounts receive access to the free tier for one year so you a billing alarm or an AWS budget this lets you know once you've accrued a certain amount of charges on your account it's a nice Peace of Mind in case you spin up some resources without being aware of the costs I can say it has saved my bacon a few times billing alarms through cloudwatch used to be the main way that you could achieve this billing monitoring but the budget service to provide more features and the ability to fine-tune the we're going to set up a cost budget which is already selected here so click next now we'll have this budget be recurring so it just continues to check every single month the start month drop down should already be populated with the current month so you can leave that as it is now for the budget amount I always set this to a small enough amount that lets me know if I did something wrong for instance I left some resource running also I set it low enough that when and how you'll be notified if the budget is exceeded click the add an alert threshold button first we'll Define the when of the notification the threshold amount refers to the dollar amount we specified in the previous screen I'm going to enter 100 here so it'll let me know when the full amount will be exceeded and the trigger drop down you can either choose actual which means it will be triggered when you actually have billable charges over that amount which means there are some prerequisites you'll need to ensure that you have completed the first is to have the AWS CLI downloaded installed and configured with a user the user should have permissions to do almost anything in AWS if you have to be more restrictive than that I recommend taking a look at the services in this course and just giving yourself read write permissions only for those we will be working a lot with the SDK and it requires you to have your CLI the demo project code you can get in the exercise files section of this course on pluralsight but I'm also making it available on GitHub if you prefer to get it from there I would say the benefit of using the exercises files from pluralsy is that any changes in the course are present in that code from each module there's a before and after section of the code it can be really beneficial to have that available just in case you make a typo during  some operations you may have learned in the past we'll start the module by covering some of the fundamental concepts of ec2 Ami and the AWS Marketplace then we're going to discuss the eC2 rest API and look at the details that go into each request I'll spare you the pain of manually making one of these requests because it is really inconvenient next we've created an ece2 instance with the SDK and install the demo basic unit in ec2 is the instance this is a virtual not physical self-contained Computing unit each instance needs software installed on it to run and this is available in AWS as an Amazon machine image or Ami Amis can be a basic Linux or Windows operating system installation or include other software like a vendors application or some open source application to give an example later in this course we'll use an Ami that comes from the AWS marketplace with Linux and EBS volumes can be connected to other instances and also can live on even if the instance is terminated the second option is elastic block storage shortened to EBS the main difference between EBS and instant store volumes is that EBS volume can be able to connect to different instances . the first option is an instance volume backed Ami which can only be terminated or restarted can't be stopped for an arbitrary period of time the instance data will be per EVS backed instance created from an Ami is significantly faster to boot than an instance-backed volume . the main reason that EBS is faster is that the Ami data for an instance backed volume must be transferred from S3 on boot up which is different from EBS EBS was introduced by AWS after instant store volumes so there are many good reasons to use EBS one more important detail of Amis is the visibility of the image basically who has permissions to use ec2 instance type which I covered in the getting started course is the instance class these classes are spot instances on-demand instances or reserved instances you may have seen these terms but based on the descriptions consider giving the other classes a try instance class doesn't have anything to do with what the instance can do it really defines the conditions in which the instance lives and how much you'll pay for it . these instant instance types are good for applications that need to be scale if you're a stable company that will reliably need certain instances running for that period of time reserved instances might be a great way to save some money the final class of instances is called spot instances these are instances that you bid on and they utilize spare ec2 computing power think of it like the unused hotel rooms or Airline seats that are sold at rock bottom prices at the last minute just because it's better to get some money than no money discounts can API is a process that requires multiple levels of hashing creating hmax and then actually making the request but let's go ahead and take a look at the process involved so we can appreciate what the SDK does for us first of all you need to decide which version of signature you want to produce is the older version supported by many of the original services but it's largely being deprecated in most places version 4 . is the preferred signature version certain newer AWS regions actually only support version 4 as do many newer Services each signature version has a different process when signing requests version 4 is the more secure and therefore more complicated process where the version 4 signature you'll start by creating a canonical request this is a combination of the HTTP method the URI path query parameters any headers and finally the hashed contents of the request payload the next step is to create a the SDK uses your local credentials to make the request and then return the response to you due to the complexities involved . we are going to use the AWS SDK to create an instance with code and before we can create the instance we need to create a few other resources in order to run your application in AWS. this clip we're going to create a security group for the instance then we'll use both and create the instance the first change I want to make will have impacts across your entire project whenever we do things with the AWS SDK we need to tell the code which region we want to use you need to do this every time you perform an operation in your code . there's a string here which will be saved as an environment variable when you run your code modify the created a file we'll modify in this clip called create ec2 instance.js this file provides a blueprint for the changes we need to make each modification has a to do comment so it's clear where you'll be making your modifications I'll walk you through the file to start with at the top are dependency Imports then we've got two to Do's where we're going to create a new client with our region and the SDK without an explicit security group or key pair if neither are given the instance is assigned the default Security Group in your account and no key pair is assigned after the fact you can reassign security groups so getting the default one is more serious because if you're using an Ami that isn't pre-configured to allow a different type of access method such as a password then you won't be able to SSH into your instance this pattern happens over and over with the new version of the JavaScript AWS SDK this new version doesn't apply to other AWS sdks with other languages now let's get started with the changes in this file after we've made all modifications and run this file we'll have a running ec2 instance with a security group allowing us access and a key pair that has been saved locally that we can use to access that instance node installed the demo project downloaded dependencies installed and the demo running on Port 3000 it's a lot of things first scroll to the top of the file at line two we need to import the ec2 client Constructor to start with replace the comment with a const declaration and curly braces entered as pascalcased then add an equals after the curlybrace and require the at AWS SDK slash client . the we'll return a call to client.send passing in the command variable this is the pattern that I mentioned a bit ago all right now we can implement the create Security Group function the first step is to create the input for our Command create a new const variable called SG params assigned to an object literal in these param objects which we will write many of the property keys will be Pascal case starting with a capital letter so keep that in mind as you object we created above now the command will have all the right instructions and details we want now we just need to use the client to send it which is what our helper function above will do in the next line add a new const called Data then after an equal sign add the await keyword and then call send command passing in the create command variable the send command function needs await here because it is an async function that returns a promise now once the command is sent it' the next property is ippermissions which is an array this array enables us to configure multiple rules in one request create an object in this array and add the property IP protocol the possible values for this are protocols like TCP or UDP enter TCP here all lowercase the next two properties from Port and to Port Define the port range you're enabling the rule for in the first case we'll enter the number 22 in both ports and to port since we' the only differences will be the front port and two Port Properties and you can set both to three thousand awesome now we're ready to send one more command first let's import it at the top of our file after ec2 client enter authorize Security Group Ingress command it's a long one and then a comma I like to organize these Imports as the client on top and then the commands that are in alphabetical order below it now we can create key pair function is what we'll need to implement so let's get started first create a new const variable named params and assigned to it an empty object literal a key pair only really needs one property to be created which makes it really easy for us the property identifier is keyname and you can give it the value of the keyname argument that was passed into the function now we need to create the command to do that we have to import a don't save this locally the key pair is essentially useless I provided a helper function that will actually take this key pair data and save it locally as a file and the next clip will take the work that we've done creating a security group and a key pair so now we're ready to create an ec2 instance that uses both of those resources . this function takes the arguments SG name and keyname which were used to create the other Amis is region specific for example an Amazon Linux 2 Ami will have different IDs depending on which region they're in and you can't just reference the ID from one region to create an instance in a different region the image IDs are unique for each region even if the image contents are the same so for that reason I'm going to show you how to find the Ami ID that will be used to create the instance and to get this we'll need ID that you'll use in your code go to the ec2 dashboard in the AWS Management console click on the launch instance button and then launch instance and you will be presented with the Ami selection screen at the top there's the Amazon Linux 2 Ami at the end of that title is the ID the ID you see probably won't match this one since you may be using a different region or they might have published an update there will also be two different ID enter the value of keyname from the function arguments next is the max count property which defines how many instances to create enter one then we have the Min count property and you can enter one again here add the property security groups and give it an array as the value this is where we can add any security groups to the instance enter the SG name argument into the array if you want to there's also a security groups ID property that you could use instead and reference these security groups by you can't just put these shell scripts directly into the user data field it needs to be base64 encoded so copy that line without the preceding hash symbol then go back to the create ec2 instance file and paste it into a string as a user data value and with that we have all the params in place next we need to build our Command scroll to the top of the file so we can import The Constructor after the create Security Group if you don't have an ID here then the default VPC isn't configured for this region in your account on this page AWS has information and the steps for creating that default vPC manually so make sure that those steps before running the script are followed . the subnets are configured to Auto assign public ipv4 addresses this means that when you create a new instance in this subnet they will give the instance a public IP address now node create ec2 instance.js to execute that script no other arguments after a few seconds of processing it should print out the location of the key that was written locally and the details of the instance that was just created or maybe an error if something went wrong the actual boot up of that script can take a couple minutes and it's done completely on the instance you don't see any output here I'm going to go ahead and fast forward the video the SDK command that you'll want to use to modify the configuration of a running instance is modify instance attribute command some examples of instance attributes this command allows you to modify are granular ones like kernel and ramdisk and higher level attributes like instance types and block device mapping I really didn't think there was enough for us to dig into with this command so I thought we'd actually try out two different commands to manage existing instances in this video just wanted to mention instances command at the top of the file replace the to do with a const variable with curly braces inside the curlybraces first enter ec2 client and then under that add the describe instances command then add an equals after that and require the at AWS SDK slash client . now let's implement the list instances function the describe cases command only has a few possible parameters in the command input the main one is the filters to apply you function is called and the result logged to the console We'll add a little more to the list instances function but I wanted us to go ahead and run it and see what kind of output we got on a command line make sure you're in the right folder and then execute node manage ec2 instance.js once it's finished running it should print out a bunch of Json to your console this is a structure of the data object that this is the information that we actually want the actual instance info is hidden here just because the Json printout doesn't go that deep so let's go back to the code to reduce the JSON that's coming back we're going to use a little javascripting to join together all the instance arrays from each reservation object so just stick with me here first replace the return statement at the end of list instances with a new const variable named data then if you've got more than one instance you should see them all listed here we'll use a piece of the information here with our next management function so let's go ahead and grab it find the instance you just launched from the create ec2 instance script the easiest way to identify that would likely be the key name that should show hamster underscore key copy the instance ID property which will be like a lowercase i hyphen and then a a stopped instance can be restarted but a terminated instance can't we will need a param's const object to pass into our Command so first declare an instance IDs which is an array you can terminate multiple ec2 instances with a single command and then back in our function create a new const called command . in the next line just return a call to send command passing in the command variable and then uncomment the instances save the file and then switch over to your command line we're going to terminate our hamster instance by running node manage ec2 instance.js when the script is complete it'll output the response from the terminate command which should contain that same instance ID that was passed in and that's it for terminating instances if you don't trust the command line you can go check the AWS Management console and see that the instance in question is currently there are just Amazon machine images with an operating system and some custom software pre-installed that's basically it on the left hand menu click AWS Marketplace and here you'll be able to see what's on offer there are some featured and popular Amis and then a bunch of categories you can see there's quite a few categories available thousands of Amis in them in fact for this course I wanted us to launch an instance with an Ami from the be charged for any instance cost but the Ami has no additional charges now click more info and then click on the link in ending in product detail page on AWS Marketplace see we're not going to use the console to launch this image but instead grab the ami ID and then use our scripts again this next page has a lot more information about the image if this image had a cost associated with it we would also be agreeing to pay whatever amount the image costs once you can adjust some parameters here such as the architecture version of node and region if you remember I mentioned before each Ami is region specific this is why picking the correct region here matters it's technically the same code on the Ami for each region the same image but the IDS are different based on the region in your console just make sure that your region is matching where you want to deploy it to and then also make sure the version of the node matches what we' we need to make the user data we have here installs node and git and then it clones the demo down and it runs it we don't need to install node anymore because this bit Nami Ami comes with it so let's change this user data open up the file titled Marketplace ec2 startup.sh this is basically the same as before minus the node installation steps and the Linux package manager updated to apt-get since to the end of each string save the file then switch over to your command line and run the file with node create ec2 instance.js you should see the same type of message as when you ran the file previously this time however you've launched the instance with a whole different Ami I'm going to skip the video ahead in time a bit and then confirm that the instance creation worked correctly I'll copy the public IP from that newly created instance into this module we're going to use the SDK to create an Ami from the instance we started up in the last clip to do this move over to your code editor and open the file createami.js you can probably tell just from looking at the file but there will be significantly less code than the last few files we worked on we can start by importing the ec2 client and the create image command at the top of the file replace the to do with a the function argument seed instance ID every Ami needs an instance to be based off of this instance ID is what should be copied to a new image next is the name property to which you can assign the image name argument this name is basically just an identifier for the Ami and that's it for the parameters now we can use this params object to send a create image command and assign to it the return keyword and call send command passing in the command variable and from we can use the manage ec2 instance file to do this open the file and make sure the list instances operation is uncommented and not terminate instances we don't need to terminate anything then save that file and then move over to your command line and run node create ami.js it should process for a second or so and then print out complete instances output in case that you are just barreling through this course you may see both the instance we just created ec2 has a limit on the max number of running instances per region and that's the worst thing about limits you end up hitting them at the most inopportune times. if you start a new instance with the image that we just created the demo project it just won't be started when the instance is created the code for a demo project will be present on the instance it just wouldn't run reproducible user data would exist in a I'll show you where to view these limits go to the ec2 dashboard in the AWS console on the left hand menu there is a limits entry and click on it you'll see there are a lot of limits here and they get very specific they're also instance type specifics so you can really do a great deal of damage if you get creative with the image types you're using scrolling down . there doesn't appear to be each Ami is stored using an EBS snapshot and there is a limit of 10 000 eBS snapshots and the max number of Amis you can have is 10 000. ec2 instances instant storage and a security group created a key pair and finally an es2 instance once we had that instance we looked at other SDK functions like listing instances and terminating instances. we're going to look at two of the major tools to automatically scale your instances scaling is about meeting the demands on your resources without you having to stay awake all night in order to achieve this magic we'll first take a look at what scalability and elasticity mean within AWS with this knowledge . the last coding step is to configure a scaling policy on that auto scaling group with those four pieces in place our scaling configuration will be complete . we' elasticity is the ability for your resources to scale in response to stated criteria often cloudwatch rules this is what happens when a load balancer adds a loading balancer to adds to the load balancing. scalability and elasticity are the most common things in the world of the cloud and AWS terms are often used interchangeably something which I'm guilty of as well I found however that the nuances of these terms often aren't completely understood the services in AWS that start with elastic often support elasticity scalability gives you the ability to increase or decrease your resources and elasticity lets those operations happen automatically according to configured rules together they let you sleep soundly at night with the assurance that your applications are healthy no matter the situation now in the next clip we're going to start our path towards elasticity by creating a launch configuration in the last module I mentioned launch templates and how they are the place that you' AWS created launch templates Auto scaling groups used launch configurations to launch instances a completely new launch configuration had to be created with launch templates you can create a new version if you need to update any of the configuration detail and then you can just update the version through using tags on the launch template or you can manually specify a version in your auto scaling group . the script is pretty simple with only one function that we need to implement also since we've already gotten our there is no ec2 client Constructor being imported here that's because I've already done that and implemented a send command function in the helpers.js file in the execute function you can see that we're also using a create IAM role function from that same helpers file another one that I have already implemented for you that're actually one thing that we need for the launch template that we haven't really talked about yet an I a params const object will be in this launch template data object the first property here will be IAM instance profile with the value of an object inside this object there's only one property needed which is Arn and the value for this will be the profile Arn variable then that's it for that IAM example profile object the next property is image ID which is the Ami that we want to launch from the last module but we do need to use the ami click on the Amis menu option on the left find the image with the name hamster image and copy the Ami ID bring that back to your code and paste it as a string as the value to image ID the next property is instance type which is the ec2 instance type I'll just enter t2.micro here since it's cheap and what we've been using so far any instance type could be entered here next add the property keynain create the command to create the launch template and the next line create a new const called command assigned to it a Constructor call of create launch template command passing in the params variable as an argument then we can send the command add the return keyword and then call helpers dot send command passing into the command and that completes the modifications needed for this file we can now run this file to create this launch template even though the other pieces of our scaling architecture aren modifying the file create load balancer.js in our scripts folder just like before I've already implemented some of the support code so that you can kind of understand it at a high level at the top I'm going to explain what I have already done to help you focus on the actual loading balancer creation as we modify this file . the first is the VPC ID for your default VPC since the load is the load balancing V2 client both balancer will be created in a VPC we need to give it that information to use move to the AWS Management console we'll get the VPC ID first by clicking on your vpcs in the left hand menu you'll likely only have one VPC so copy the ID for it and your code editor paste that value in the string after the variable named VPCID now go back to the Management console and click on the subnets menu entry we need this execute function calls the operation functions that we are implementing it starts by creating a new security group for the load balancer that is open on Port 80 . this is so that users can access the loading balancer over HTTP from their web browsers then a Target group is created and a load Balancer I'll explain the target group in just a second the responses from each of those calls are stored in variables so that we can then get the Arns for be listing on Port 80 which is for HTTP and our Target group will be listening on Port 3000 since that's what our demo application is using the listener will facilitate sending those requests between the two ports now let's implement the create load balancer function start by creating a params const object the first property will be name which you can give the value lb name then add the security groups property which will Define these security groups in the loadbalar itself call helpers.send elb command passing in command this send function is different from the one that we used before because it is going to be using a different client to send the command the elastic load balancing V2 client I've already implemented the send function with that client it this function just has a distinct name and that's all we need to modify below this function are functions to create the target group and The Listener and no modifications are needed for scaling groups are responsible for creating and removing ec2 instances from a group according to configured rules they use a launch template when creating the instance and they can be attached to a Target group so that each instance can be used with the load balancer let's create our Auto scaling group open the file create autoscaling.js just like the other files you'll see that I've already imported some commands for you this time from the auto scaling client availability zones and set the value as an array these are the availability zones where the auto scaling group will launch the instances at least two need to be defined these availability zones should match the subnets that we used in the load balancer . the next property is launch template give it the value of an object that object just has one property launch template name which can be assigned a value of the ltname argument this is how the launch template that we created is going to be connected that's the only version that exists next adds the property Max size which defines the maximum number of instances to create I'll enter two then add the property Min size that defines the minimum number of cases to have in the group I'd enter one here the next property will be Target group Arns with the value of an array enter the tgarn variable which we configured earlier this property ties the auto scaling group to the Target group now we can create the command on auto scaling policies are the original way that auto scaling groups worked these used a cloudwatch alarm to monitor certain attributes and then perform an action once the alarm actually occurs only one action could be configured per alarm so if you wanted true elasticity you needed a policy for scaling up or down step scaling policies were added as a way to define multiple actions when a single alarm fires the policy will keep scaling even after it's been initiated which differs from simple scaling policies the create ASG policy function to start with we need a params const object so go ahead and create that we're going to be putting all of our policy details in here the first property is adjustment type and this defines what the policy should do enter the string change in capacity here the next property is auto scaling group name and you can set the value to the ASG name function argument next add the property policy name which is just the unique identifier for the policy you the auto scaling group will scale up and add instances if it's above five percent CPU utilization or it'll scale down and remove instances and if the average Falls below so enter the number five here the next property is predefined metric type with the value of a string with the text ASG average CPU utilization . given an object as a value this object has one property predefined type with a . value with the . text average CPU usage because CPU this is a result of each one of those pieces that we put together over the last few videos the load balancer is routing our request to an instance that was created by an auto scaling group using a launch template pretty awesome in the next clip let's look at limits that you may run into when using Auto scaling and loadbalancing largely Define how many of each resource that you can have the ec2 limits page that we looked at earlier also contains limits for both auto scaling the main limits are on the number of groups and launch configurations that you can have I think these are sufficiently large enough that you wouldn't hit them unless you have an entire company working on one account and again it's something that you could raise as far as I can tell there are no limits on how many launch templates that you have so go wild with that load balancing limits are also defined around the numbers of resources if you do a lot of internal balancer so that limits the number of certificates for https I've never actually had a problem with this because I have never actually needed more than one SSL certificate on a load balancer but if you do that's how you would solve it by having multiple loads balancers configured for each Target group but you can only have multiple Target groups for one loadbalancing balancer . this module walks you through everything you needed to be able to configure scaling efficiently we started we're going to look at different types of storage in AWS specifically elastic Block store for ec2 instances and S3 storage for all-purpose object storage we'll work from the resources that we created in this module so don't tear anything down yet see you next time hi there and welcome back to AWS developer designing and developing file storage is one of computing's oldest concerns and something every developer has to deal with there are several different solutions for storing your files instant store was the first storage type for use with ec2 instances these volumes are physically connected to the server where your instance is running they used to be the default storage option but there are a few reasons why they're not best for General use anymore if the connected instance stops or terminates data stored on an instant store volume is basically stuck you could move the data off the volume but it's completely up to the user . there are also lots of options in regards EFS is built to connect to multiple instances at once and scale the file system capacity as needed the product documentation advertises capacity growing to petabyte scale and isn't optimized for a root disk for an ec2 instance EFS provides data durability by replicating volumes across multiple availability zones in the next clip we're going to detach an EBS volume from one instance and then attach that volume to a different instance in the scripts o5 directory be removing the EBS volume from but then it will launch a new ec2 instance so that we can attach that volume to that instance that's going to all happen in this one file after this we'll modify a different file that will manipulate the volume but let's go ahead and run this file first we need to add in the instance ID for the instance that we want to remove the volume from this will be the example that we created manually not one of this is going to be important so that we can launch our new instance in the same availability Zone copy the instance ID and then move to your code editor and paste it as a string for the value of instance ID next in the params object in the create instance function we have a placement object which describes which availability Zone the incident should be launched in this should be the same available Zone as the instance that we are removing the EBS instance from the one we just stopped so enter this is a great time for a quick note why are we actually stopping the instance here well to detach a root volume from an instance that must be Stopped the instance in question only has one attached EBS volume and that is its root volume to attach it we need to stop it first if it had multiple volumes and we were detaching a non-root volume and we're going to create a fresh brand new instance to attach the volume to this this script does two things it detaches a volume from one instance and then attaches it to another pretty simple so let's make this script functional the first step is to find some IDs we will need the volume ID of the EBS volume that will be detaching and the instance ID of an ec2 instance that we just created for the second value you can get that from the command line in the output to the command we just executed copy the case ID a params const object the only property that AWS needs to know when detaching a volume is the volume ID . add a command const variable and then assign to it a new Constructor call of detach volume command passing in the parrams variable then on the line after that you can add the return keyword and then call send command pass in command now once the volume is detached it'll be available to be re-attach be used to mount the volume on your instance these values are usually things like slash Dev slashes sda1 or Dev Slash xvda these are all kind of like Linux mounting things we'll call ours something way more creative than those enter . now on the line after the params object create a new const called command assigned to it a Constructor call of attach volume command passing in the Params if a volume has a product code they've removed it from the Management console there's no CLI operation that'll show it to you for volumes that you've launched from a custom Ami that you own in your account but otherwise you just have to try to attach it and see if you get an error so let's go shut down the instance move to the ec2 dashboard in the AWS Management console click on the instances menu option I'm going to skip in time a little bit here so you don't have to wait with me in your command line run the command node reuse EBS volume.js once you get some output it should have detached and reattached the volume to the new instance now we can check our work by going to the ec2 instances in the AWS Management console refresh it and select the target instance that was attached to in the instance details click on the there are some automated aspects of the service that make it even more useful for applications two of those features are versioning and life cycle events versioning allows you to store multiple iterations of the same object in case you need to access a previous revision this can be configured at the Bucket Level and applies to creating deleting or modifying when a file is created it becomes the current and only version of that file if you upload a new file with that same key the a lifecycle event allows you to set certain rules for what to do with an object in your bucket one says certain number of days has passed one life cycle event you can trigger is to move an object to a lower cost type of storage for example you can say that once a file has been in S3 for five days move it to either S3 infrequent access storage or Glacier storage both offer much cheaper costs for files that don't need to be accessible quickly you can you can apply rules to only affect previous versions permanently cleaning out older versions once they've been retained for a certain amount of time using both of these features can add to the robustness and usefulness of S3 as the ultimate storage utility in the next clip let's get started by creating a bucket so that we can store the assets for our demo project in the scripps05 folder open the create s3bucket.js file this script is bucket name does not contain an underscore so maybe not whatever you want as long as it's not an underscore we will be using the bucket name in a domain name later and underscores aren't allowed in URLs for some reason now let's implement the create bucket function start by creating a trusty old params const object creating this bucket doesn't require a lot of properties the first property is bucket and you can give it the value of the bucket is a command sending function that has been imported from the helpers file up above that I wrote previously it uses an S3 client to send the command and with that our script is done you can go ahead and switch over to your command line and execute the command node create s3bucket.js you'll see some output once it's complete and your bucket will be created if you really want to ensure that the bucket creation was successful in the next clip object to S3 operation is where the code actually runs a helper function is called that gets all the files in the public folder of the demo project each one of those files is iterated through with a for Loop the upload S3 object functions is called on each file and then the e-tag confirmation is logged to the console that's it so let's implement the execute function by declaring the params const object the first property is bucket which value of the property to file dot contents is key which is the object key identifier that should be used in S3 we can use the file name for that so set the value to file.name the next property will be different for each file and the property name is content type this should match whatever type of file is being accessed so I've created a helper function called helpers.getcontent type call that here and as the value and then pass in file. will run webpack to compile all the assets and it can take a while it doesn't have any output but once it's complete it'll send you back to the prompt then we can upload the files execute the command node upload S3 objects.js this operation may take oh we need to update our asset references in the demo project source code so switch over to the AWS Management console and go to the S3 dashboard click into the bucket that we the file is index.html on line 5 we're importing a style sheet and right now it's referencing that style sheet from the public directory so remove the public part of that path and paste over that path the other place that we need to modify the path is in the util folder and it should have public right now you can remove that completely and paste the S3 URL that you have just make sure that there is not a double slash there before a slash at the end so that the final path for those images is created correctly in the next clip we're going to look at some limits that you might run into with EBS and S3 both have limits that may actually hit a little bit more than some of the other services that you've seen with S3 I've mentioned the limits around attaching and detaching but I'll reiterate them here an instance must be stopped in order to detach  there's no limit to the number of objects aside from the size of your wallet as mentioned before bucket names must be globally unique not just unique in your account and that's most of the limits you may hit in your daily usage with either service there'd be a limit to your number of uses for file storage and we were only able to cover a small amount here regardless this module has given you a good overview of EBS and S3 and hopefully sparked we're going to look at another type of persistence in AWS as we look into dynamodb and RDS and using redis caches with elasticache make sure you stick around and I'll see you in the next module hello there and welcome back to AWS developer designing and developing along with Computing and storage databases and persistence are the third pillar upon which stands the halls of AWS . the all important limits we may hit when using dynamodb RDS or elastic cache throughput capacity is the key measure for how your tables will perform there aren't really any other attributes in the table that you need to configure . throughput isnt exactly that simple at its most basic it can be defined as the number of Records you're allowed to read or write per second up to four kilobytes for reading and one kileo we have six hamster records locally that we're going to populate the table with if we do a batch write call it'll essentially try and write all six records at once there are a couple of considerations here how large are the records in our case they are only a few properties with not much text and come in at under a hundred bytes each Dynamo will round up each right to one kilobyte if it' if you want to get all six records at once doing a scan there will be six reads that will happen within a second and even though the records are well below the 4 kilobytes reserved for each unit each will consume a full unit it'll round up except they won't because by default the SDK does an eventually consistent read but by default you get double the reads per unit so each unit will give you two reads instead of one we've got a table with some binary items they're about 20 kilobytes each how many units would it consume to read one item well if we're doing eventually consistent reads it would take three read units each unit in this consistency will hold up to eight kileobyte, a 20 kilosbyte item will technically use 2.5 units but AWS rounds up so we'll just run through another quick example let's assume we AWS has released Auto scaling for provision capacity mode so you could configure rules that would scale the provision capacity up or down based on an increase in requests with the configured reads and writes for your table you do pay regardless of whether you actually are using that capacity or not and then there's on demand capacity mode this was released in the last couple of years and is a model that charges you per read and write requests unlike provision capacity you don't pay for anything that you don the partition key is passed to an internal hash function and the output is used to assign the item to a partition this partition key must be unique in your table kind of like a traditional primary key there's another type of key schema you can use instead that lets your partition key be non-unique and that's by adding a sort key also sometimes called arrange attribute in the case that you define both of these key schema types allow you to be very flexible when defining Global secondary indexes are a global secondary index you define a new key schema to organize your items and query on you could have a partition key schema or partition key plus sort key schema in addition you'll Define which attribute you want to include in the index if you query on the index only selected attributes will be accessible in some ways Dynamo is just replicating the data and organizing it in a different way another nice thing is that when you create a all of the base tables properties are carried over in many ways a local secondary index just enhances the original data where a global secondary index creates an exclusive new set and with both indexes Dynamo maintains them completely when adding or deleting items updating them . the first step we need to accomplish is to create a table in dynamodb that we can use to store the data for hamsters and races in the scripps06 table names do need to be unique in each region at the account level if you use different regions you can reuse a table name the next property is attribute definitions with the value of an array create an object the first property here is attribute name which will give the value a string with a value ID the next properties are attribute type here we've got three options s for string n for number or B for binary in our case our IDs will be numbers so enter the attributes we defined inside the array add an object the first property here will be attribute name which will be the string ID this needs to match the attribute name defined in our attribute definitions the next property is key type which will enter as hash in all uppercase now after the key schema property We'll add the last property provisioned throughput which is an object add the property read capacity units with the value of 5. and those are all the properties needed to create our table now on we'll be able to populate the tables with some data that our demo project needs to use an empty Dynamo table is worse than an empty S3 bucket okay maybe not but still we want to populates our Dynamometer table with some Data in your scripts folder open the file populate dynamotable.js let's go over what's happening in this script I've created some helper functions to get the initial hamster and race this Library will use a dynamodb document client instead of the regular client to send the command now let's implement the populate table function the input to populate the table will be dynamically created from the data function argument and then we'll use the batch write command to write all the items at once so we start by declaring our params const object this one's a little weird because it has a single root property request items give that property the items for the put request will be built out dynamically one item at a time on the next line create a new Constructor call of batch write command passing in the params variable then on the line after that add the return keyword and call send Dynamo item command pass in command and that's it for this script so let's go ahead and try it out in our command line the tables that we created in the last video should be finished creating by now we're going to connect our demo project up to this table to consume the data that we just put there in this clip . the easiest way to get data out of a dynamodb table is to just use the primary key of the item that you want to get by utilizing this method combined with secondary indexes Dynamo becomes a very flexible and Powerful tool but there are some other methods for retrieving data which we'll look at in the clip the dynamodb document client and some commands this send command function is like the one I previously hid in a helper's file you can see a Bare Bones client is created then that is used to create a document client we're using this document client because we'll be working with Dynamo items and the document client greatly simplifies these operations let's start by implementing the get all function this function will use one of the first name give it the value of the table name function argument then in the next line create a new const called command assigned to it a New Constructor call of scan command passing in the params then after that create an new constation called response we need to modify the result from the scan to only return the items from that result after an equal sign add the await keyword and call send command passing into command then add the return keyword and then response dot items with an upper the next property defines the query we're using Create a property called key condition expression the value will be a query on the key so enter a string with the value ID equals colon H key this query will find any records or the ID of the record equals whatever we set as H key the colon before H key indicates that it is a variable We'll add a plus in front of it so that it will be coerced into a number in modify the command response before we return it so on the next line create a new const called response after an equal sign add a weight and then call send command passing in the command then on the Next line add return and then response dot items when you use a query response is an array for this get function we just want to return one item which would be the first item in the array so then reference the second item with bracket notation and the index of zero great our Dyna the relational database service in AWS is great because it takes care of the maintenance tasks that all databases need performed including automated replicas and backups the data we're working with in the demo project is simple so it could really live in either Dynamo or RDS but I split it up so that you can have an example of working with each in the scripts 06 folder open the create rdsdb.js file in this file we'll do exactly the next property is DB instance identifier you can set the value to the DB name argument then we have the property engine which is the database engine that you want to use you know I really wanted to use aws's own Aurora database engine for this course but it's not eligible for the free tier and it can only be created in clusters not individual instances . there are actually two types of security groups applicable to an RDS instance DB security we're going to be using them here DB security groups are old and they're actually only applicable when you have an ec2 classic configuration which you probably don't want to figure out how to set one of those up it's kind of an old way of doing things for the value of this property create an array and add one item the sgid argument the last two properties are the database credentials add the property Master username and set it to the string we do need to modify some code in our demo application to utilize this database so let's just do that back in your code editor open the file lib slash users.js instead of writing queries to the database manually the demo app uses the sqlize orm library to do all the database interaction it provides an abstraction that lets you treat querying the database like you're accessing functions on lines one and two we import the user and user favorites objects from sql the AWS Management console and the RDS dashboard on the left hand menu select databases and then select the database and since it takes a while it's possible that this instance will still be in the process of creating but as usual I've skipped my video in time a few minutes here and you can see that the video's already finished creating scroll down to the endpoint value and copy it now back in your code editor paste this value inside the string next to host and to Cache the user sessions in the demo app ensuring that our application can scale to a whole lot of users without impacting application performance in the scripts 06 folder open the create rediscluster.js file the only function we need to implement here is the create redis cluster function the only arguments it takes is the name for the cluster and an ID of a security group for the Cluster we'll start by creating a params const object the first set the value to an array with one item named sgid and the next line create a new const called command assigned to it use the return keyword and call send elasticache command passing in the command now we finish the script so we can run it by switching to your command line and then executing the command node create rediscluster.js once it's complete your redis cluster will be booting up and just like RDS this can take some a redis plugin and make a connection to it to use as a cache then scroll down to where we Define this cache const and uncomment the line with the property cache this will set the redis cache as the default cache to use for user sessions before it was just using memory now go back to that top section of code that we uncommented and here the cache is now available so in your AWS Management console go to the elasticash dashboard and click on the dynamodb and RDS stuff can be run locally but unfortunately we can't easily access a redis cluster running in elastic cache outside of the VPC we could configure a jump box or a Bastion host to access it but it's just a little overkill for this course instead we're just going to update the code deploy what we've got locally to our instance in AWS and see it work remotely in your command line navigate to the if you made any modifications you may need to update some of the values and deploy.js now on running this command it will zip up all your code upload it to the instance extract the zip file install the dependencies terminate any node applications that are running and then restart the application you should be able to try it out then in your browser using the load balancer DNS name in the next clip we're going to look at limits you may run into in working with any of be created consecutively more than one cannot be created at the same time RDS has limits around numbers of databases and size of the databases which is in the terabytes but they differ with each database engine so you'd need to research those yourself elastic cache similar to RDS have a limit around the number of clusters and nodes these are soft limits and can be raised with a request to AWS the one restriction to remember is that you can't directly access the next module we'll look at how to configure different types of external routing using AWS services like Route 53 and API Gateway hi there and welcome back to AWS developer designing and developing we've been doing all sorts of cool things with AWS in this course but what good would any of it be if users couldn't access our hard work that's where Route 53 is direct DNS routing whereas API Gateway provides an abstraction over routing and is based around rest API design Route 53 is often used as simple DNS rules to route to AWS resources there's more advanced functionality available in regards to how the routing takes place routing policies allow you to define the behavior a routing rule follows once a request has come in there are five types of routing policies that we'll discuss here simple weighted latency geolocation and failover a simple rounding policy is when you want to map a single DNS record to an AWS resource for instance sending a geolocation routing policy allows you to configure multiple resources and Define regions for each you'll configure the region and the record set to match the region of the resource then when a user requests a domain Route 53 will determine which request will have the lowest latency based on the available regions that you configured and the IP of the request this is done automatically so it gives you tremendous control over who goes to what resource and is likely very useful for legal or compliance reasons finally there if the health of the primary resource fails then requests will be routed to the secondary resource this provides some stability for domain records when a load balancer won't work between all of these routing policies you should be able to find one for whatever need you have in the next clip we're going to configure Route 53 for a domain name the first is the hosted Zone this is basically the domain name itself without subdomains for example.com or hbfl we'll do in this video so if you do want to follow and test it purchase a domain name and use that instead of the placeholder here now switch over to your code editor and open the file in the scripts 07 directory named create Route 53 hostedzone.js this file is simple with just one method that we need to implement for this script we're using the Route 53 client and as usual I've already implemented the send command method in the helpers file let create hosted Zone command passing in the params variable then after that add the return keyword and call send command pass in command now save the file and switch over to your command line and navigate to these scripts 07 directory execute the command node create Route 53 hosted Zone dot Js once it's complete you should see the Json output for the requests we need to copy the ID field under hosted Zone so go ahead and select that and copy it now that we have a hosted a command to send but the properties for the record set itself will be quite numerous so let's start by replacing the string contents of the hzid variable with the hosted Zone ID that you copied from the command line output in the previous video it's important that this value is correct since the route set has to be applied to the right hosted zone for it to work now we can implement the create record set function create a new const named params and assign to give the value of an array with a single request you could create or modify multiple record sets by adding different objects to this array for our case we're only going to need one object so add that this is the object where we will get to the nitty-gritty details of this change the first property is name and this is a DNS name for the record you could enter a subdomain here something like demo.hbfl.online or www.h AWS resource and Route 53 will figure out all the IP and routing stuff add the property Alias Target and give it the value of an object the first property is DNS name and this is the internal AWS DNS name in our case we're going to send it to the load balancer that we've created so let's go to the ec2 dashboard select the loading balancers menu option and then the loadbalar that you created copy the DNS name from this page has all the hosted zones for any region in AWS for load balancers look for the region that you're deploying to and then look in the column that has application loadbalars in it they have created multiple hosted zones now so just make sure you are picking the one with application loadbalancers then copy the ID that's in that cell back in your code editor paste that value as a string for hosted Zone ID and that is going to be API Gateway is one of the newer AWS services and it reflects an evolution of routing with Cloud resources while there is API directly in the name it is by no means only for use with apis it actually works well for all types of routing so what exactly is API Gateway it's essentially a routing layer that allows all kinds of routing to work well for any type of routing. if we switch over to the AWS Management console and go to the Route 53 dashboard you to configure many rules on the same domain and connect it to many different resources the language of API Gateway is couched in classical rest terms each part of a URI path is called a resource and on those resources you configure methods each method can be configured to have a certain behavior and send the request to some AWS service you can transform the request before sending it on and also transform the response as well each method on a source is configured separately so you could have different our script files let's get started by opening the createapigateway.js file you may notice right off the bat that the execute function in the script is really large this is where the API is created and configured . we'll explain each step in just a second an API and API Gateway is structured like this you have your API the API has different resources which are organized sort of in a tree structure with parents and children then each resource can have one or many methods create a resource called hbfl this will be what our demo app responds on we have to then create the method and integration for that resource then we create one more resource which is actually a proxy resource it'll be used for things like API calls to our demo project once that proxy resource is created . we're going to need to implement all the functions called in this execute function so we've been working in this file over the course of the next few value of the API name argument then after the params object create a new const called command assigned to it and then on the next line return a call to send command passing in command and that's it for create rest API the next function we'll Implement is get root resource which will get the root resource ID from the API we just created the created rest API command does return the ID of the . API but not the ID that is needed to add other resources to awaiting call to send command passing in the command in the next line create a new const called root resource we will go through the resources returned from our Command call to find the resource that only has a slash as the path this will identify that Resource as the root resource after an equal sign call the function response.items.find which will look for an item matching some criteria from the function . within the parentheses add an arrow function with one argument create resource function argument this is the path that the resource will be configured to respond to on the final API URL scheme the last property will be rest API ID which you can give the value of API dot ID and that's it for the params object on the next line add an equal sign add an awaiting call to send command passing in command and then return and then response dotID we just need to return that ID property from the response . there's still more create resource method function in createapigateway.js file start by declaring a params const object on the first line the first property is authorization type with API Gateway methods you're able to create authorizations by using IAM or even your own function we won't be securing our endpoint we want it open to the public so enter the string none in all caps the next property is HTTP method which will give the value of the method function argument create method integration function starts by creating a params const object this one will have the most properties in this file add the first property named HTTP method to which you can give the value of the method function argument the next property is resource ID which is the method used to call the back end AWS resource we will use the property integration HTTP method which is used to send it to a Lambda function or using it as a mock or sending it to an AWS Resource HTTP proxy type when sending requests over to our load balancer so add that as the value in all caps with an underscore between the words and the final property is URI which we will give the URL to the load balanced demo application so switch over to the ec2 dashboard and go to the loading balancers menu option and get the URL in the value for URI . the URL does need to include a protocol so in a string add HTTP colon slash  a normal API Gateway integration to an HTTP endpoint strips away all of the original headers and only sends special ones from the integration sometimes this might be what you want but other times you want to preserve the initial request to API Gateway and just pass it completely over to your HTTP resource on the other side this operation is called a proxy in our case we have several endpoints on our app that serve as API type endpoint. if we call something like hbf Gateway to execute the resource with the rest of the path whether it's nested or not whatever it is then we use the same method and integration functions that we've already implemented but we add an additional proxy string as the last argument you'll notice the HTTP method we're using is any which isn't actually an HTTP method but it tells API Gateway to handle all possible HTTP methods so we'll just add some functionality to the create resource method and create method this property add true now what this is doing is telling our resource method that we're going to have a parameter on the path of the URI the value of which is whatever is passed into the function in the path argument for example in our case it's going to be proxy this is basically how you define parameters on your path for a resource method and this is similar to how you would add query parameters if you were going to use them now we can add the new Behavior end of our URI on the next line enter params.request parameters assigning to it an object this is where we tie our integration request parameter which will be going to the load balancer endpoint to the method request parameter similar to the resource method change we made we'll have a Dynamic Property key here add square brackets and then give that property the value of a string template and inside it add method dot request dot path dot dollar sign open this we can deploy the rest API in the next clip and then we can actually try it out in our browser deployment and API Gateway is kind of unique there are two key Concepts stages and deployments API Gateway lets you create stages for your apis think of stages like development integration staging and production you can deploy your API to different stages to have a healthy development life cycle each time you deploy to a stage a deployment is created these are saved in the history of each stage so API ID deploying our API is as easy as just creating a deployment so let's implement the create deployment function create a new params const object We'll add the first property which is rest API ID which you can give the value of the stage name function argument . this is where you could differentiate between stages like prod staging or Dev . the command node deploy API gateway.js after it finishes it'll output some response data to get the URL for the deployment let's go back to our web browser and select that hamster API option in the API Gateway table under the hbfl resource now click on stages and you'll see the resources and methods that are deployed you can see the any method for our proxy was actually turned into all the different HTTP methods if you click on the get under API Gateway Route 53 and API Gateway are both constrained by restrictions and limits but they come from different concerns Route 53 has many restrictions due to the nature of DNS changes . there is a restriction on how fast record set changes are propagated to the internet and this is based on DNS rules usually I found that when you change your record set the change can be observed almost instantly if you make an error for some reason and need to change that same record set again the change will be we looked at two different ways that you can get your web application to your users Route 53 follows the classic DNS model but has some tricks up its sleeves an API Gateway represents a new way of abstracting routing behind a URI unless you get creative with how a domain routes requests Route 2 based on waiting lowest latency geolocation of requests and more then we created a Route 53 hosting Zone and record set to route to our users Route 2 and more. we'll look at two different ways that AWS empowers you to deliver your content to users by looking at caching static assets and cloud formation and serving up static sites with S3 hello and welcome back to AWS developer designing and developing in this module we're going to look at other ways to get content to our users . the first is cloudfront a global content delivery Network that is essential for providing fast delivery of static files the second is a feature in S3 cloudfront and S3 static websites content coming from cloudfront is typically best when being served from the cloudfront cache and not from your origin getting this right will dramatically reduce latency to your user but when configuring cache you'll set a Max time to live before the cache retrieves a new version of the content from the origin set the value too high and when you release new code then users are still getting the cached version set it to low and you're not taking invalidations can take some time they aren't instant while I've often experienced it would take only a few minutes to process an invalidation that was with only few files I can imagine it could take longer with more files . the second solution which would work better for many applications using cloudfront is to treat your assets as immutable and never change them this way they can be cached the maximum time and never need a manual invalidation the easiest way to achieve this we've already changed our code to directly access Assets in S3 so let's create a cloudfront distribution based off that bucket so that we can get even better performance from it cloudfront SDK commands might hold the award for most properties in a params object I seriously made us a completely separate file just because there were so many navigate to the scripts 08 folder and open the cloudfront parameters.js file I moved the two biggest sections of par origin details will go create one object here the first property for the object is domain name this will be the domain for your S3 bucket we'll build it dynamically so create a string template then add a variable and add the bucket name function argument after that continue with DOT S3 dot Amazon aws.com and with that we build the domain name dynamically based on whatever your bucket name is the next property is ID this is going to be a origin lets's configure the default cache Behavior properties do exactly as they say they define the default behavior that our created distribution will follow the first property is forwarded values which has the value of an object this will tell our distribution to forward on cookies or query parameters to the origin in our case we don't want any of this Behavior add the property cookies with the value . an object inside give it a property named query string and give it the . value of false now after the an object this will let you create signed URLs for cloudfront content and only allow users to access content that way we won't be doing that but this is another required field add the property quantity with the number zero then the property enabled with the value false after trusted signers add a property named viewer protocol policy this is where you can Define https only redirect HTTP to https or both I like to redirect HTTP calls to https so add the string redirect Dash 2 Dash https next we're setting up the cloudfront distribution in front of our S3 bucket so start by adding a distribution config property and giving it the value of an object the first property inside this will be caller reference which you may remember from the Route 53 hosted Zone this is a unique identifier for the SDK command that we make add a string template with a variable and the expression date dot now to get a timestamp for this value the next property this is what we set up in the other file so set the value to a call to Origins and pass in the bucket name function argument the next property is HTTP version which I like to set to the string HTTP 2 to make sure that it's future facing the next Property is price class basically the pricing is set according to how many Edge locations that you choose add the value price class underscore 100 which is the cheapest and it only uses Edge locations in the US Canada and Europe the if it's set to disabled it just won't work and that's it for the params object on the next line create a new const called command and assigned to it a Constructor call of create distribution command passing in command and then return a call to send command pass in command . now we can create this script to create our distribution so switch over to your command line and navigate to the scripts 08 folder execute the command node create the first is in util slash assets.js and then on line 5 update the URL and take out any bucket that's on the path just leave the file name at the end do the same on line 7 with the favicon and then also on line 12 with the JavaScript import now to fully test this out you need to follow all of these steps because this video is already long enough I'm not going to be walking you through them you'll just need S3 static website hosting with S3 is one of aws' oldest Solutions launching shortly after S3 itself and while AWS has since released other services that provide similar functionality we're going to take a look at how it works first let's look at the difference between a website and an S3 bucket with a bucket you use keys to get back objects every object must be referenced directly and only that object is returned it's just like getting static letting you configure some static Behavior to occur when a directory is referenced in the URL with S3 static hosting you configure what the name of your index file will be the defaults are index.html and error HTML then you can place these files in any directory in your S3 bucket and they will be returned when that path is navigated to in addition to the static site route there isn't too much complexity involved but it provides a simple solution when you need a the first change is to update the name of your S3 bucket for the value of the bucket name variable now we can implement the configure S3 site function by first creating a params const object and then the next property is website configuration which will need the property of an object this is where we're going to tell S3 how we want the site to operate there are really only two options for properties that you could use here index document or error document we could add error document is actually the default for this property so we didn't necessarily have to add the property so that's it for the prams object and the next line create a new const called command and assign to it a Constructor call of put bucket website command passing in the params variable then on the line after that just return a call to send command pass in command and that' is it it's pretty simple to configure an S3 site now switch over cloudfront and S3 static website hosting are a static file to demonstrate these static sites in the next clip we're going to look at some limits and restrictions with cloudfront . cloudfront has a few limits to be concerned with the number of resources but there are some limits on how you configure your distributions you can only have a single SSL certificate on a distribution so you'll have to expect multiple distributions in your design if you require multiple certs there we started by discussing cloudfront invalidations and why you might not want to use them then we created a cloudfront distribution with all of those parameters next we moved on to S3 static websites and set one up based on our existing S3 bucket and finally we looked at the few limits concerning cloudfront in the next module we're going to look at inter-service communication inside AWS both simple queue service and Kinesis streams provide ways for services to decouple and communicate we'll start by looking at some of the specifics of polling and sqs and how messages work and how to consume and process those messages to the races table and then we're going to set up an sqsq to send race results and look at how to use them to send races results to the race table . the module will be based on a process of clients sending messages and then consumers polling for messages on the Queue this is sort of like when a message comes into the queue it may or may not be visible to Consumers immediately depending on the delay seconds attribute . when the message initially arrives in the queue once the message is visible and a consumer reads it another attribute comes into play the visibility timeout once a text is read it is considered in flight this means the message will stay in the waiting and be invisible if the visibilitytimeout runs out then the message becomes visible to consumers again and they can then in this next diagram the message is deleted In Time by its consumer this is the ideal life cycle for a message in sqs both the visibility timeout and delay seconds can have default values set on the cube but each message added to the queue can override those values and provide specific delay seconds when sending an message . the consumer will immediately return with the new messages this is called long polling and it can be useful because you are charged for sqs by each request made requests the first step in using a Q and sqs for communication between services and AWS is to actually create the queue let's start by opening the file in the scripts 09 directory named create sqsq.js since there's quite a bit of code required to work with cues I've actually done most of the work and will just be filling in certain parts of the code mostly properties in this file you'll only need to create the param if they're not read and deleted we'll set this to the number 345 600 which is the second equivalent to four days if you ever need that number next add the property visibility timeout which is how long messages will be invisible after being read we can set it to the value 30 for 30 seconds then the last property is receive message wait time seconds which will set the long polling time of a consumer . the create script is ready so we can save that file sqs.js is a file that will send the results of each race simulation pushing a message to a queue requires you to have the full Q URL and then it will go and retrieve the queue URL before it starts pushing to it so the first step in this function is to get the Q URL so inside the push function create a new const and we'll name it Q params not regular Params you can assign an object to that it'll in the next line create a new const called Q response and assigned to it an awaiting call of client.send passing in the queue command this is going to store the properties for our send message command the first property will be message body which is the message that we want to actually send and put on the queue we will need to turn the message function argument into a string so call json.stringify and then pass in MSG message the next property is Q URL the next clip will Implement cue polling for the demo project now that we're pushing race results messages up to sqs we need to pull for those messages and then update the data sources with those records we'll make our first changes in the lib data lib folder with the sqs.listener.js file this file will pull for sqs messages every five seconds if messages are received if there's something there then it will modify the master records in file the first step is to get the Q URL so let's add the single property for cube params the key will be queue name and you can give it the value of the global variable race underscore Q all in caps that's done so look further down for the receive Params object and this is used to pull and receive messages from the queue the first argument is Q URL which you can set to the queue data dot Q URL value the next argument is Max number the delete messages function is called in a for Loop so each retrieved message will have this delete message command called on it there's also a delete message batch command in the SDK that could also have worked well here since it deletes many messages so you can take your pick with that change our listener is ready and we just need to enable all the functionality in the index.js file at our project root at the bottom of the file on line 87 there is  Kinesis streams is a service for real-time data transfer it scales easily and can have multiple producers and consumers it doesn't have fine-grained control over how messages are handled like with sqs but that Simplicity might make it better in some situations let's talk about one of the key Concepts in Kinesi streams and that's sharding . if you need more input or output we're going to try running them now the consumer of the Kinesis stream will update the races instead of the hamsters in our case we're going to use a Lambda as the consumer one of the main reasons is that because a stream is real time it requires a dedicated consumer and our web application is not really the right fit lambdas are actually a great fit as Kineis consumers so this will give you a taste of how that can work now in the next clip we' if the stream needs to increase its capacity but for now we'll just enter the value 1 here the next property is stream name and we can give that the value of the stream name function argument and that's it for the prams object on the next line you can declare a new const called command and assigned to it a New Constructor call of create stream command and pass in the params variable then after that just return a call to send command the partition key is a value that Kinesis hashes and uses to segment the data record to a specific chart even with A Single Shard which is what we have this is still a required field when pushing data to Kinesi this is another very simple file that only performs one operation the only thing we need to do is to add properties to the params object in the send function as you can tell as time goes on I'm doing more and more we need to stringify the message so called json.stringify passing in the message function argument that will provide the Kinesis stream with the message in the format that it wants the next property is partition key which we can give the value of our partition function argument and then the final property is stream name which will have the value stream name it's pretty simple just point the data at a stream and put it up there we use the put record command to actually send that message now to configure a Lambda function to connect to a Kinesis stream you will need the Arn for the Stream So to get this we're going to move over to the AWS Management console and go to the Kinesi dashboard click on the number in the data streams section and then click on hamster race results option and then you can copy the arn which is found here now the rest of this file creates a role for your Lambd Source Arn which will give the value of the Kinesis Arn function argument the next property is starting position which tells the Lambda function at which record it should start processing from the stream you can just enter latest in all caps as a string and give it the value 100. this defines how many records will be retrieved at one time and 100 is the max value here and that's it for code that we need to modify I do want to show you the code event.records property on line 12 you can see how to get the actual data from the Kinesis record that's there you don't have to go to kinesis in any other way it will be a base64 encoded buffer so it does need to be processed correctly before you can really do anything with it now we can switch over to our command line and run the command node create the dynamodb races table this will take some time to sqs and Kinesis streams are highly Performance Services targeted at specific use cases understanding how the systems work and the restrictions around them are essential for designing a robust web application in the previous discussions we've covered most of the limits that exist with both of those streams . there are the soft resource limits that can be raised but there are a few limits that I wanted to mention here the first limit with Kinesi streams is the limit to the size of the data blob if you have data larger than that you'd need to break it into pieces or send records of S3 keys where the data is actually stored in relation to sharding there is a limit around the number of put requests operations you can perform each second and that's one thousand per Shard . sqs provides a more distributed and decoupled form of sending messages and Kinesis provides real-time and high throughput streams let's recap what we this is the last module for this course and we're going out with a bang by covering two services that will increase the real-time nature of your applications simple notification service or SNS is used to send notifications to users Via SMS email HTTP and many other ways a usual trigger of SNS are cloudwatch alarms and those can be configured to make sure that your application and services stay up and healthy let's take a look at what we'll be covering this is essentially how SNS simple notification service works in this module we're going to set up a topic and subscription so that we can send notifications whenever races are complete we'll start in this clip by creating the topic that will have messages published to it open up the file in the scripts 10 folder named create SNS topic.js we only need to implement the create topic function in this file so let's start by creating a const named params create topic command passing in command this file is now complete pretty easy so we can switch over to our command line to run it navigate to the scripts 10 folder and execute the command node create SNS topic.js you should get an output with this topic Arn something that we need someone to subscribe to it which we'll do in the next clip one of my favorite things about SNS is the different types of notification options that are available you're not only able to if you've got an American phone number your country code is one you're welcome then on the next line we need to enter the Arn of the topic we just created and then paste it as the value for topic Arn now we're ready to implement the create subscription function go ahead and start by creating a params const object at the top of the function the first property is protocol which we are passing in as the function argument type the next property is topic this file exports a publish function that can be used to send a notification to the SNS topic to the value of topic Arn you may still have it in your clipboard but if not you can go back to your command line and copy it from the create topic output now let's supplement this publish function create topic.js it'll output the subscription details and you should be up and running in the next clip we're going to implement a library and a param's const object on the first line here just like the topic creation and subscription there aren't many properties required the first will be topic Arn which you can set to be the MSG function argument now with our properties set let's publish out the message and the next line create a new Constructor call of publish command passing in params then after that return a call to client.send passing in command and that's all this clip lets us set an alarm on our load balancer to send a notification to our SNS topic if the healthy hosts fall below one this basically means if something happens to our application instance and it stops responding we should get a text message all of these details will be wrapped up in a single cloudwatch alarm we're going to create this alarm using the create cloudwatchalarm.js file and the Scripps 10 folder we'll start by fill need to get parts of the target group Arn and load balancer Arn from the resources that we created towards the beginning of this course so head back to the AWS Management console and go to the ec2 dashboard this time in the left hand menu at the very bottom select Target groups and then select the target groups named hamster TG and then we're going to just grab a part of the Arn not the whole thing select from Target group slash value of the lb variable now that we have all those values completed we can get started on implementing the create Cloud watch alarm function first create the params const object we've got quite a few parameters here in no particular order so hold on to your hat first add the property alarm name which can have the value of an alarm name function argument this will be the alarms identifier in cloudwatch the next property is metric name which we'll assign a string with the value AWS slash application elb the next property is period which works with the above evaluation periods property this says how long a period should be defined as in your alarm the typical length is five minutes but I actually prefer one minute for this this value should be in seconds so enter the number 60. next is the threshold property in our case this is a number of healthy hosts that are okay this threshold is based on the metric that you this is an SNS topic and it will publish an alarm to the notification topic the next property is Dimensions with the value of an array this is where you specify exactly which resources the alarm should gather metrics from for healthy hosts we need to enter both the target group and load balancer resources here create an object then add the property name with the lb function argument now we'll make another object in this array the name property for this one will be loading balancer and the other property this means that if there is missing data if it can't tell how many healthy hosts there are it should trigger the alarm other possible options for this are ignore not breaching or missing the reason that we are going to be putting breaching here is because for us a missing value here basically equals zero thus we want the alarm to fire and that's it for all the properties and the next line create a new const called command and then assign to it  cloudwatch has a limit of five actions that can take place per alarm this is a hard limit and can't be changed if you need more actions you could create a new cloudwatch alarm with the same metrics but that might end up being pretty messy in addition a cloud watch alarm period can only be a maximum of a day remember the period is used to decide whether the alarm should fire or not if the alarm is looking for more lengthier time frames than if we wanted an alarm created whenever zepto the hamster came in sixth place three times in a row we'd have to look for another place to figure out that out we all know zpto is a winner though and that would never happen so maybe this is just a moot point and that's it for limits and restrictions in the next clip I want to give you a guide for cleaning up everything we've created in this all EBS snapshots that were created and any EBS volumes that are still around finally delete any security groups other than the default one next go to S3 and delete the hamster bucket then go into RDS and delete a MySQL instance don't bother taking a final snapshot of the database unless you're really nostalgic about the results of the Petco 2000 . now go to dynamodb and delete both tables then go to elasticache and remove the red SNS is powerful and quite useful when coupled with Cloud watch alarms the true potential of real-time notification is realized there are an endless number of uses for both services so I hope this module has inspired you to try out things with each let's take a look at what we covered in this module we started by creating a SNS topic then we made an SNS subscription with that topic and your mobile number next we modified our app to send notifications to the topic and ultimately our you're in a good place to start doing some damage in AWS find a personal project or side project at work that allows you to try out some of the services discussed in this course and consider finishing the other courses in the AWS developer associate certification path to prepare to master the development associate certification exam again my name is Ryan Lewis and thank you for watching this course.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# Instantiate the tokenizer and the summarization pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained('stevhliu/my_awesome_billsum_model')\n",
    "summarizer = pipeline(\"summarization\", model='stevhliu/my_awesome_billsum_model', tokenizer=tokenizer)\n",
    "\n",
    "# Define chunk size in number of words\n",
    "chunk_size = 200 # you may need to adjust this value depending on the average length of your words\n",
    "\n",
    "# Split the text into chunks\n",
    "words = transcript_text.split()\n",
    "chunks = [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "# Summarize each chunk\n",
    "summaries = []\n",
    "for chunk in chunks:\n",
    "    # Summarize the chunk\n",
    "    summary = summarizer(chunk, max_length=100, min_length=30, do_sample=False)\n",
    "\n",
    "    # Extract the summary text\n",
    "    summary_text = summary[0]['summary_text']\n",
    "\n",
    "    # Add the summary to our list of summaries\n",
    "    summaries.append(summary_text)\n",
    "\n",
    "# Join the summaries back together into a single summary\n",
    "final_summary = ' '.join(summaries)\n",
    "\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "408aef07-a191-401e-96a6-0f04fa683abb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_393/3145115536.py\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranscript_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo-16k\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         messages=[\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    151\u001b[0m         )\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         )\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m             return (\n\u001b[0;32m--> 700\u001b[0;31m                 self._interpret_response_line(\n\u001b[0m\u001b[1;32m    701\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"error\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             raise self.handle_error_response(\n\u001b[0m\u001b[1;32m    766\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "def split_text_into_chunks(text, max_chunk_size):\n",
    "    return textwrap.wrap(text, max_chunk_size)\n",
    "\n",
    "openai.api_key = \"\"\n",
    "max_chunk_size = 4000\n",
    "\n",
    "transcript_chunks = split_text_into_chunks(transcript_text, max_chunk_size)\n",
    "summaries = \"\"\n",
    "\n",
    "for chunk in transcript_chunks:\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{chunk}\\n\\nCreate short concise summary\"}\n",
    "        ],\n",
    "        max_tokens=250,\n",
    "        temperature=0.5\n",
    "    )\n",
    "\n",
    "    summaries += response['choices'][0]['message']['content'].strip() + \" \"\n",
    "\n",
    "print(\"Summary:\")\n",
    "print(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336f4267-3a5a-4bf0-ae57-48c1b79e3c01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo-16k\",\n",
    "messages=[\n",
    "{\"role\": \"system\", \"content\": \"You are a technical instructor.\"},\n",
    "{\"role\": \"user\", \"content\": transcript_text},\n",
    "{\"role\": \"user\", \"content\": \"Generate steps to follow from text.\"},\n",
    "]\n",
    ")\n",
    "\n",
    "# The assistant's reply\n",
    "guide= response['choices'][0]['message']['content']\n",
    "\n",
    "print(\"Steps:\")\n",
    "print(guide)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baf718-84cf-4720-9660-02882201c262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "model=\"gpt-3.5-turbo-16k\",\n",
    "messages=[\n",
    "{\"role\": \"system\", \"content\": \"You are a helpful assistant that generates questions.\"},\n",
    "{\"role\": \"user\", \"content\": transcript_text},\n",
    "{\"role\": \"user\", \"content\": \"Generate 65 quiz questions based on the text with multiple choices.\"},\n",
    "]\n",
    ")\n",
    "\n",
    "# The assistant's reply\n",
    "quiz_questions = response['choices'][0]['message']['content']\n",
    "\n",
    "print(\"Quiz Questions:\")\n",
    "print(quiz_questions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
